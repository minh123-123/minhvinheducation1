[
{
	"uri": "//localhost:1313/vi/1-introduce/",
	"title": "Giới thiệu",
	"tags": [],
	"description": "",
	"content": "Trong bối cảnh AI phát triển nhanh chóng hiện nay, việc cập nhật các công cụ và phương pháp tốt nhất để xây dựng ứng dụng AI Tạo Sinh (GenAI) an toàn, đáng tin cậy và hiệu suất cao trở nên thách thức hơn bao giờ hết. Sự tiến bộ liên tục của các mô hình AI, cùng với độ phức tạp ngày càng tăng của môi trường triển khai, đòi hỏi các nhà phát triển phải không ngừng thích ứng với công nghệ, framework và yêu cầu mở rộng mới. Ngoài ra, đảm bảo bảo mật dữ liệu, tính toàn vẹn và tuân thủ các tiêu chuẩn ngành là điều bắt buộc, đặc biệt khi doanh nghiệp cần cân bằng giữa đổi mới và quản lý rủi ro.\nĐối với các doanh nghiệp, khả năng triển khai các giải pháp AI một cách nhanh chóng, mở rộng và an toàn mà không ảnh hưởng đến chất lượng hay thời gian ra thị trường là yếu tố then chốt để duy trì lợi thế cạnh tranh. Điều này đòi hỏi một phương pháp tiếp cận hiệu quả và được tối ưu hóa – đây chính là vai trò của OPEA (Open Platform for Enterprise AI). Bằng cách đơn giản hóa quá trình triển khai AI và tích hợp các phương pháp tốt nhất trong ngành, OPEA giúp các nhà phát triển vượt qua những thách thức cốt lõi và xây dựng các ứng dụng GenAI sẵn sàng cho doanh nghiệp một cách dễ dàng.\nGiới Thiệu OPEA OPEA là một dự án mã nguồn mở thuộc LF AI \u0026amp; Data Foundation, được thiết kế để hỗ trợ phát triển và đánh giá các giải pháp GenAI mở, đa nhà cung cấp, mạnh mẽ và có thể kết hợp. Framework này được tối ưu hóa cho doanh nghiệp, giúp việc tích hợp các quy trình AI an toàn, hiệu suất cao và tiết kiệm chi phí vào hệ thống kinh doanh trở nên dễ dàng hơn. Ban đầu, OPEA tập trung vào Retrieval Augmented Generation (RAG), cho phép doanh nghiệp khai thác những đổi mới tốt nhất trong hệ sinh thái AI đồng thời đảm bảo tính hiệu quả và khả năng mở rộng.\nRetrieval Augmented Generation (RAG) là gì? RAG là một kỹ thuật AI tiên tiến kết hợp giữa khả năng truy xuất thông tin và mô hình ngôn ngữ lớn (LLM). Không giống như các LLM truyền thống chỉ dựa trên kiến thức đã được huấn luyện, RAG có thể truy xuất thông tin theo thời gian thực từ các nguồn dữ liệu bên ngoài (chẳng hạn như cơ sở dữ liệu hoặc tài liệu doanh nghiệp) trước khi tạo ra phản hồi. Cách tiếp cận này giúp nâng cao độ chính xác, tính phù hợp và giảm thiểu sai lệch thông tin (hallucinations), đặc biệt hữu ích cho các ứng dụng doanh nghiệp cần dữ liệu cập nhật và chính xác.\nNội Dung Workshop: Xây Dựng Ứng Dụng RAG với OPEA Trong workshop này, bạn sẽ sử dụng OPEA để xây dựng một ứng dụng GenAI dựa trên RAG có khả năng truy xuất và xử lý dữ liệu bên ngoài, đảm bảo phản hồi chính xác và phù hợp với ngữ cảnh. Nhờ vào kiến trúc module linh hoạt của OPEA, bạn sẽ khám phá cách tích hợp các cơ chế bảo mật, tối ưu hóa hiệu suất và triển khai ứng dụng trên AWS một cách dễ dàng. Trải nghiệm này sẽ giúp bạn hiểu rõ cách mà các giải pháp GenAI dựa trên RAG có thể nâng cao quy trình AI doanh nghiệp, đảm bảo tính bảo mật, khả năng mở rộng và hiệu suất cao trong thực tế.\n"
},
{
	"uri": "//localhost:1313/vi/1-introduce/2.1-createec2-copy/",
	"title": "Sử dụng Workshop Studio",
	"tags": [],
	"description": "",
	"content": "Nếu bạn tham gia các bài lab này trong một sự kiện do AWS tổ chức trên Workshop Studio, bạn không cần tự cấp phát tài nguyên—tất cả các tài nguyên cần thiết cho các mô-đun sẽ được thiết lập sẵn.\nNhững gì sẽ được thiết lập trong tài khoản AWS của bạn? Stack sẽ tự động cấu hình các thành phần sau:\nCụm EKS: Tạo mới cụm EKS có tên opea-eks-cluster. Triển khai một node trong cụm sử dụng M7i.24xlarge instance để đảm bảo hiệu suất cao. CloudFormation Templates: Stack cũng tạo các mẫu CloudFormation cho từng mô-đun:\nMô-đun 1: ChatQnA Default\nMô-đun 2: ChatQnA với Guardrails\nMô-đun 3: ChatQnA với OpenSearch (mã nguồn mở) làm cơ sở dữ liệu vector\nMô-đun 4: ChatQnA với Bedrock làm mô hình LLM\nMô-đun 5: ChatQnA với Remote Inference (Denvr) làm mô hình LLM\nVới các tài nguyên được cấu hình sẵn, bạn có thể tập trung hoàn toàn vào việc khám phá và phát triển ứng dụng GenAI mà không cần lo lắng về việc thiết lập hạ tầng.\nBước 1: Cấu hình quyền truy cập vào cụm EKS Để tương tác với cụm EKS bằng kubectl, bạn cần cấu hình môi trường của mình để nhận diện cụm. Điều này được thực hiện bằng cách cập nhật tệp kubeconfig, tệp này lưu trữ thông tin xác thực và cấu hình truy cập cho Kubernetes.\nĐăng nhập vào AWS Management Console: Bắt đầu bằng cách đăng nhập vào AWS Management Console. Mở Cloud Shell hoặc thiết lập môi trường cục bộ: Trong giao diện console, nhấp vào biểu tượng Cloud Shell để mở terminal được cấu hình sẵn. Nếu sử dụng môi trường cục bộ, hãy đảm bảo bạn đã cài đặt AWS CLI Client và kubectl trên máy cá nhân. Cập nhật kubeconfig: Chạy lệnh sau để cập nhật kubeconfig với thông tin của cụm EKS: Bạn sẽ nhận được một đầu ra xác nhận rằng tệp cấu hình của bạn đã được cập nhật. Giờ đây, bạn có thể sử dụng kubectl để quản lý cụm Kubernetes. Bước 2: Kiểm tra kết nối với cụm EKS Sau khi cập nhật kubeconfig, hãy kiểm tra xem bạn có thể kết nối với cụm hay không bằng cách liệt kê các node: Nếu lệnh thực thi thành công, bạn sẽ thấy đầu ra hiển thị các nút được liên kết với cụm của bạn. Bây giờ bạn đã sẵn sàng khám phá mô-đun bạn chọn và bắt đầu triển khai khối lượng công việc trên cụm EKS của mình!\n"
},
{
	"uri": "//localhost:1313/vi/1-introduce/2.1-createec2/",
	"title": "Sử dụng Workshop Studio",
	"tags": [],
	"description": "",
	"content": "Nếu bạn tham gia các bài lab này trong một sự kiện do AWS tổ chức trên Workshop Studio, bạn không cần tự cấp phát tài nguyên—tất cả các tài nguyên cần thiết cho các mô-đun sẽ được thiết lập sẵn.\nNhững gì sẽ được thiết lập trong tài khoản AWS của bạn? Stack sẽ tự động cấu hình các thành phần sau:\nCụm EKS: Tạo mới cụm EKS có tên opea-eks-cluster. Triển khai một node trong cụm sử dụng M7i.24xlarge instance để đảm bảo hiệu suất cao. CloudFormation Templates: Stack cũng tạo các mẫu CloudFormation cho từng mô-đun:\nMô-đun 1: ChatQnA Default\nMô-đun 2: ChatQnA với Guardrails\nMô-đun 3: ChatQnA với OpenSearch (mã nguồn mở) làm cơ sở dữ liệu vector\nMô-đun 4: ChatQnA với Bedrock làm mô hình LLM\nMô-đun 5: ChatQnA với Remote Inference (Denvr) làm mô hình LLM\nVới các tài nguyên được cấu hình sẵn, bạn có thể tập trung hoàn toàn vào việc khám phá và phát triển ứng dụng GenAI mà không cần lo lắng về việc thiết lập hạ tầng.\nBước 1: Cấu hình quyền truy cập vào cụm EKS Để tương tác với cụm EKS bằng kubectl, bạn cần cấu hình môi trường của mình để nhận diện cụm. Điều này được thực hiện bằng cách cập nhật tệp kubeconfig, tệp này lưu trữ thông tin xác thực và cấu hình truy cập cho Kubernetes.\nĐăng nhập vào AWS Management Console: Bắt đầu bằng cách đăng nhập vào AWS Management Console. Mở Cloud Shell hoặc thiết lập môi trường cục bộ: Trong giao diện console, nhấp vào biểu tượng Cloud Shell để mở terminal được cấu hình sẵn. Nếu sử dụng môi trường cục bộ, hãy đảm bảo bạn đã cài đặt AWS CLI Client và kubectl trên máy cá nhân. Cập nhật kubeconfig: Chạy lệnh sau để cập nhật kubeconfig với thông tin của cụm EKS: Bạn sẽ nhận được một đầu ra xác nhận rằng tệp cấu hình của bạn đã được cập nhật. Giờ đây, bạn có thể sử dụng kubectl để quản lý cụm Kubernetes. Bước 2: Kiểm tra kết nối với cụm EKS Sau khi cập nhật kubeconfig, hãy kiểm tra xem bạn có thể kết nối với cụm hay không bằng cách liệt kê các node: Nếu lệnh thực thi thành công, bạn sẽ thấy đầu ra hiển thị các nút được liên kết với cụm của bạn. Bây giờ bạn đã sẵn sàng khám phá mô-đun bạn chọn và bắt đầu triển khai khối lượng công việc trên cụm EKS của mình!\n"
},
{
	"uri": "//localhost:1313/vi/2-prerequiste/2.1-createec2/",
	"title": "Sử dụng Workshop Studio",
	"tags": [],
	"description": "",
	"content": "Nếu bạn tham gia các bài lab này trong một sự kiện do AWS tổ chức trên Workshop Studio, bạn không cần tự cấp phát tài nguyên—tất cả các tài nguyên cần thiết cho các mô-đun sẽ được thiết lập sẵn.\nNhững gì sẽ được thiết lập trong tài khoản AWS của bạn? Stack sẽ tự động cấu hình các thành phần sau:\nCụm EKS: Tạo mới cụm EKS có tên opea-eks-cluster. Triển khai một node trong cụm sử dụng M7i.24xlarge instance để đảm bảo hiệu suất cao. CloudFormation Templates: Stack cũng tạo các mẫu CloudFormation cho từng mô-đun:\nMô-đun 1: ChatQnA Default\nMô-đun 2: ChatQnA với Guardrails\nMô-đun 3: ChatQnA với OpenSearch (mã nguồn mở) làm cơ sở dữ liệu vector\nMô-đun 4: ChatQnA với Bedrock làm mô hình LLM\nMô-đun 5: ChatQnA với Remote Inference (Denvr) làm mô hình LLM\nVới các tài nguyên được cấu hình sẵn, bạn có thể tập trung hoàn toàn vào việc khám phá và phát triển ứng dụng GenAI mà không cần lo lắng về việc thiết lập hạ tầng.\nBước 1: Cấu hình quyền truy cập vào cụm EKS Để tương tác với cụm EKS bằng kubectl, bạn cần cấu hình môi trường của mình để nhận diện cụm. Điều này được thực hiện bằng cách cập nhật tệp kubeconfig, tệp này lưu trữ thông tin xác thực và cấu hình truy cập cho Kubernetes.\nĐăng nhập vào AWS Management Console: Bắt đầu bằng cách đăng nhập vào AWS Management Console. Mở Cloud Shell hoặc thiết lập môi trường cục bộ: Trong giao diện console, nhấp vào biểu tượng Cloud Shell để mở terminal được cấu hình sẵn. Nếu sử dụng môi trường cục bộ, hãy đảm bảo bạn đã cài đặt AWS CLI Client và kubectl trên máy cá nhân. Cập nhật kubeconfig: Chạy lệnh sau để cập nhật kubeconfig với thông tin của cụm EKS: Bạn sẽ nhận được một đầu ra xác nhận rằng tệp cấu hình của bạn đã được cập nhật. Giờ đây, bạn có thể sử dụng kubectl để quản lý cụm Kubernetes. Bước 2: Kiểm tra kết nối với cụm EKS Sau khi cập nhật kubeconfig, hãy kiểm tra xem bạn có thể kết nối với cụm hay không bằng cách liệt kê các node: Nếu lệnh thực thi thành công, bạn sẽ thấy đầu ra hiển thị các nút được liên kết với cụm của bạn. Bây giờ bạn đã sẵn sàng khám phá mô-đun bạn chọn và bắt đầu triển khai khối lượng công việc trên cụm EKS của mình!\n"
},
{
	"uri": "//localhost:1313/vi/4-s3log/4.1-updateiamrole/",
	"title": "Thiết lập tích hợp Bedrock",
	"tags": [],
	"description": "",
	"content": "Amazon Bedrock là gì? Amazon Bedrock là dịch vụ được quản lý hoàn toàn, cung cấp quyền truy cập vào nhiều mô hình nền tảng hiệu suất cao (FM) từ các công ty hàng đầu trong ngành như AI21 Labs, Anthropic, Cohere, Meta, Mistral AI, Stability AI và chính Amazon. Thông qua một API duy nhất, Bedrock cho phép các nhà phát triển xây dựng các ứng dụng AI tạo sinh đồng thời đảm bảo tính bảo mật, quyền riêng tư và các hoạt động AI có trách nhiệm.\nCác nhà phát triển có thể tương tác với Amazon Bedrock thông qua AWS Software Development Kit (SDK) hoặc AWS Command Line Interface (CLI). Bedrock cũng cung cấp các tính năng gốc cho phép người dùng tạo cơ sở kiến ​​thức RAG (Retrieval-Augmented Generation), quy trình làm việc của agentic và lan can. Việc tích hợp Bedrock với OPEA mở rộng quyền truy cập vào nhiều mô hình nền tảng hơn đồng thời tận dụng các khả năng tiên tiến của Bedrock cùng với OPEA.\nKiến trúc thay đổi như thế nào?\nCác tích hợp trong tương lai với OPEA sẽ mở khóa toàn bộ tiềm năng của các khả năng của Amazon Bedrock, bao gồm các mô hình Titan Embedding. Tuy nhiên, đối với mô-đun này, trọng tâm chỉ tập trung vào LLM.\nNhờ kiến ​​trúc mô-đun và có thể hoán đổi của OPEA, hầu hết các thành phần từ thiết lập ChatQnA mặc định (Mô-đun 1) vẫn không thay đổi. Dịch vụ TGI (Hugging Face) hiện được thay thế bằng một vùng chứa Bedrock, tích hợp liền mạch vào triển khai hiện có.\nThành phần kiến ​​trúc được cập nhật:\nchatqna-bedrock Trong triển khai này, khi người dùng gửi tin nhắn qua Giao diện người dùng ChatQnA, tin nhắn đó sẽ được định tuyến đến vùng chứa Bedrock ở phía sau, nơi giao tiếp với Amazon Bedrock để truy xuất và trả về phản hồi. Tích hợp này duy trì kiến ​​trúc ChatQnA đồng thời cải tiến kiến ​​trúc này bằng các khả năng LLM mạnh mẽ của Amazon Bedrock, đảm bảo khả năng mở rộng, hiệu quả và triển khai liền mạch.\nChúng tôi đã sử dụng không gian tên Kubernetes của bedrock để tách các nhóm và dịch vụ liên quan đến triển khai Bedrock. Khi bạn sử dụng kubectl và các lệnh Kubernetes khác trong các ví dụ bên dưới, hãy đảm bảo đủ điều kiện cho lệnh bằng -n bedrock.\nTriển khai ChatQnA bằng Amazon Bedrock LLM\nĐối với phòng thí nghiệm này, chúng tôi đã tạo một tập lệnh thay đổi với triển khai song song đầy đủ của ví dụ ChatQnA trong cùng một cụm Kubernetes mà bạn đã sử dụng. Lệnh sau sẽ triển khai các pod vào cụm trong không gian tên \u0026ldquo;bedrock\u0026rdquo; giống hệt với các pod ChatQnA ban đầu, ngoại trừ các mô hình Bedrock thay vì TGI.\naws cloudformation execute-change-set \u0026ndash;change-set-name bedrock-change-set \u0026ndash;stack-name OpeaBedrockStack\nKích hoạt Mô hình\nMô-đun này hoạt động với hầu hết mọi LLM tạo văn bản được Bedrock hỗ trợ, nhưng vì mục đích của phòng thí nghiệm này, chúng tôi đã sử dụng mô hình Anthropic Claude Haiku 3. Vì vậy, trong khi bạn đang chờ bộ thay đổi triển khai, hãy kích hoạt mô hình của chúng ta trong bảng điều khiển Bedrock:\nChuyển sang vùng us-west-2, bạn có thể thử nghiệm trên các vùng khác nhưng thường thì us-west có nhiều khả năng hơn: Truy cập Amazon Bedrock: Truy cập tab truy cập mô hình: Ở đầu màn hình, nhấp vào nút có nội dung Sửa đổi quyền truy cập mô hình Chọn Claude 3 Haiku Có thể mất một hoặc hai phút để cấp quyền truy cập, nhưng đừng lo lắng sẽ không mất nhiều thời gian hơn thế nữa.\nSau khi bạn xác nhận rằng quyền truy cập mô hình đã được cấp, hãy chuyển trở lại vùng us-east-2 nơi cụm EKS của bạn nằm. Xác nhận triển khai\nBây giờ hãy xác nhận rằng triển khai Bedrock của chúng ta đã hoàn tất. Bạn có thể theo dõi trạng thái của các pod Bedrock bằng lệnh kubectl:\n\u0026hellip;để có đầu ra như thế này:\nCó thể mất vài phút để Bedrock khởi tạo hoàn toàn và khả dụng. Chỉ tiếp tục khi bạn thấy pod chatqna-bedrock-deployment ở trạng thái Đang chạy.\nBây giờ bạn có thể sử dụng Amazon Bedrock trong môi trường của mình.\n"
},
{
	"uri": "//localhost:1313/vi/3-accessibilitytoinstances/3.1-public-instance/",
	"title": "Triển khai ChatQnA",
	"tags": [],
	"description": "",
	"content": "Giới thiệu Trong phần này, bạn sẽ triển khai bản thiết kế OPEA cho ứng dụng dựa trên RAG, ChatQnA, trên môi trường Amazon Elastic Kubernetes Service (EKS). Khám phá thực hành này sẽ giúp bạn hiểu rõ hơn về cách ứng dụng RAG hoạt động trong hệ sinh thái Kubernetes được quản lý, cho phép bạn phân tích các thành phần của ứng dụng và vai trò của chúng trong hệ thống.\nCấu hình triển khai có sẵn thông qua AWS Marketplace và ChatQnA có thể được tìm thấy trong kho lưu trữ GenAIExamples của OPEA.\nChatQnA trên GitHub\nVì bạn đã thiết lập quyền truy cập vào cụm Kubernetes của mình trong phần \u0026ldquo;Thiết lập\u0026rdquo;, giờ đây bạn sẽ tìm hiểu sâu hơn về môi trường đã triển khai để khám phá và tìm hiểu thêm về cấu trúc và chức năng của cụm.\nBước 1: Triển khai mẫu ChatQnA CloudFormation Mở AWS CloudShell và triển khai mẫu ChatQnA CloudFormation vào cụm Amazon EKS của bạn. Thao tác này sẽ khởi tạo quá trình triển khai ứng dụng RAG trong môi trường Kubernetes được quản lý của bạn. Bước 2: Khám phá Tài nguyên cụm Điều hướng đến AWS Management Console và chọn cụm EKS được chỉ định của bạn để xem lại quá trình triển khai. Mỗi cụm bao gồm các cấu hình quan trọng như phiên bản Kubernetes, thiết lập mạng và tùy chọn ghi nhật ký. Kiểm tra các thiết lập này sẽ giúp hiểu sâu hơn về cơ sở hạ tầng của ứng dụng, giúp quản lý hiệu quả và khắc phục sự cố khi cần thiết.\nXem lại Tài nguyên cụm Nhấp vào tab Tài nguyên để xem tất cả các ứng dụng hiện đang chạy trong cụm của bạn, bao gồm ChatQnA và các dịch vụ vi mô liên quan. Đảm bảo rằng tất cả các dịch vụ vi mô từ bản thiết kế OPEA ChatQnA được cài đặt đúng cách bằng cách liệt kê các pod đang hoạt động: Đầu ra sẽ hiển thị tất cả các pod ở trạng thái Đang chạy (1/1), xác nhận rằng ứng dụng đã được triển khai thành công. Tại thời điểm này, bạn đã sẵn sàng để khám phá thêm về việc triển khai và quản lý tài nguyên của mình trong cụm. "
},
{
	"uri": "//localhost:1313/vi/",
	"title": "Triển khai công cụ hỏi đáp trò chuyện trên Nền tảng mở dành cho AI doanh nghiệp (OPEA) trên AWS",
	"tags": [],
	"description": "",
	"content": "Triển khai công cụ hỏi đáp trò chuyện trên Nền tảng mở dành cho AI doanh nghiệp (OPEA) trên AWS Tổng quan Trong hội thảo thực hành này, bạn sẽ học cách triển khai một ứng dụng Trí tuệ Nhân tạo Tạo sinh (GenAI) mẫu trên AWS EKS bằng cách sử dụng OPEA và AWS CloudFormation. Bạn sẽ khám phá các phương pháp tốt nhất để xây dựng cơ sở hạ tầng GenAI trên AWS và tận dụng các thành phần OPEA để đơn giản hóa quá trình triển khai, giúp bạn tập trung vào việc phát triển và tối ưu hóa ứng dụng mà không cần thiết lập hạ tầng phức tạp từ đầu. Ngoài ra, bạn sẽ áp dụng các cơ chế kiểm soát (guardrails) để quản lý hành vi của ứng dụng và cải thiện hiệu suất bằng cách tích hợp các dịch vụ AWS như OpenSearch.\nĐối với những người hoàn thành sớm, sẽ có các bài tập tùy chọn cung cấp các phương pháp thay thế để chạy mô hình LLM theo hướng serverless trong triển khai RAG của bạn.\n"
},
{
	"uri": "//localhost:1313/vi/2-prerequiste/",
	"title": "Các bước chuẩn bị",
	"tags": [],
	"description": "",
	"content": "Trước khi chạy bất kỳ mô-đun nào, bạn cần thiết lập môi trường AWS đúng cách. Mặc dù bạn có thể triển khai các ví dụ có sẵn từ kho lưu trữ công khai, khuyến nghị sử dụng các ví dụ được dựng sẵn để có trải nghiệm mượt mà và tối ưu hơn. Những ví dụ này được cấu hình sẵn với các thiết lập phù hợp, quyền AWS (IAM roles) và Load Balancer để dễ dàng truy cập dịch vụ, giúp quá trình triển khai diễn ra suôn sẻ.\nNội dung Sử dụng Workshop Studio Sử dụng tài khoản của riêng bạn "
},
{
	"uri": "//localhost:1313/vi/3-accessibilitytoinstances/3.2-private-instance/",
	"title": "Khám phá triển khai OPEA ChatQnA",
	"tags": [],
	"description": "",
	"content": "Khám phá triển khai dịch vụ vi mô OPEA Bây giờ, chúng ta hãy cùng tìm hiểu sâu hơn về triển khai OPEA ChatQnA RAG. Là bản thiết kế dựa trên dịch vụ vi mô, bản thiết kế này được thiết kế để có khả năng mở rộng, phục hồi và linh hoạt. Trong nhiệm vụ này, bạn sẽ khám phá từng dịch vụ vi mô để hiểu vai trò của chúng trong toàn bộ hệ thống. Bằng cách kiểm tra các thành phần này, bạn sẽ hiểu sâu hơn về cách chúng tương tác và đóng góp vào chức năng của ứng dụng.\nKiến trúc này mang lại một số lợi thế chính:\nKhả năng mở rộng – Mỗi dịch vụ vi mô có thể mở rộng độc lập dựa trên nhu cầu, đảm bảo hiệu suất và sử dụng tài nguyên tối ưu.\nCách ly lỗi – Nếu một dịch vụ gặp sự cố, nó sẽ không làm gián đoạn toàn bộ hệ thống, giúp tăng cường độ tin cậy.\nBảo trì và cập nhật hiệu quả – Các dịch vụ vi mô cho phép cập nhật nhanh chóng và dễ dàng thích ứng với nhu cầu kinh doanh và nhu cầu của người dùng đang thay đổi.\nKiến trúc dịch vụ vi mô OPEA Các triển khai OPEA được xây dựng xung quanh ba thành phần chính:\nMegaservice – Hoạt động như một đơn vị điều phối cho tất cả các dịch vụ vi mô, quản lý quy trình làm việc và đảm bảo tương tác liền mạch giữa các thành phần. Điều này rất cần thiết để phối hợp một ứng dụng đầu cuối với nhiều bộ phận chuyển động. Bạn có thể tìm thêm thông tin chi tiết trong tài liệu OPEA.\nGateway – Hoạt động như điểm vào cho người dùng, định tuyến các yêu cầu đến đến các dịch vụ vi mô phù hợp trong kiến ​​trúc dịch vụ vi mô. Đảm bảo kết nối liền mạch giữa người dùng bên ngoài và các thành phần bên trong.\nMicroservice – Đây là các thành phần chức năng riêng lẻ của ứng dụng, xử lý các tác vụ như nhúng, truy xuất, xử lý LLM và tương tác cơ sở dữ liệu vector. Truy cập các dịch vụ vi mô\nTrước khi bắt đầu khám phá, hãy lưu ý rằng chỉ có dịch vụ cổng và giao diện người dùng được hiển thị bên ngoài. Trong tác vụ này, bạn sẽ trực tiếp truy cập từng dịch vụ vi mô nội bộ cho mục đích thử nghiệm, sử dụng cổng Nginx để định tuyến hiệu quả các yêu cầu đến các dịch vụ nội bộ này.\nBạn sẽ cần ghi chú tất cả các pod đã triển khai.\nkubectl get svc liệt kê tất cả các dịch vụ trong cụm Kubernetes, hiển thị tên, loại, IP cụm và cổng được hiển thị của chúng. Lệnh này cung cấp tổng quan về cách các ứng dụng được hiển thị để truy cập nội bộ hoặc bên ngoài.\nChạy lệnh sau trên CloudShell của bạn:\nBạn sẽ thấy đầu ra tương tự như sau:\nLệnh kubectl get svc được sử dụng để liệt kê các dịch vụ đang chạy trong cụm Kubernetes. Các dịch vụ hoạt động như các điểm vào cho phép giao tiếp giữa các thành phần khác nhau của ứng dụng của bạn. Mỗi dịch vụ có một tên duy nhất (ví dụ: chatqna hoặc chatqna-ui), giúp xác định vai trò của dịch vụ đó trong hệ thống.\nCác dịch vụ Kubernetes có thể được hiển thị theo nhiều cách khác nhau:\nClusterIP – Chỉ có thể truy cập trong cụm, cho phép các thành phần nội bộ giao tiếp an toàn.\nNodePort – Hiển thị dịch vụ bên ngoài thông qua một cổng cụ thể trên mỗi nút, giúp dịch vụ có thể truy cập được bên ngoài cụm. Cluster-IP là địa chỉ nội bộ được các dịch vụ khác sử dụng để tiếp cận ứng dụng. Nếu dịch vụ có thể truy cập được từ bên ngoài cụm, External-IP sẽ được hiển thị. Tuy nhiên, trong trường hợp này, các dịch vụ này hoàn toàn là nội bộ.\nCột Cổng cho biết dịch vụ lắng nghe trên cổng mạng nào. Ví dụ:\nchatqna có thể đang chạy trên cổng 8888/TCP, xử lý giao tiếp nội bộ.\nchatqna-nginx có thể được cấu hình với 80:30144/TCP, trong đó lưu lượng từ cổng 80 được chuyển tiếp đến 30144 cho mục đích định tuyến. Cuối cùng, cột Tuổi hiển thị thời gian dịch vụ đã chạy—ví dụ: 12 giờ cho tất cả các dịch vụ được liệt kê trong kịch bản này.\nBây giờ, chúng ta hãy khám phá kiến ​​trúc chi tiết.\nBước 1: Megaservice (Orchestrator) (POD:chatqna:8888) Megaservice đóng gói toàn bộ logic cho ứng dụng ChatQnA RAG. Dịch vụ siêu nhỏ này có nhiệm vụ xử lý các yêu cầu đến và thực hiện tất cả các hoạt động nội bộ cần thiết để tạo ra các phản hồi phù hợp.\nDịch vụ này không được hiển thị trực tiếp, nhưng bạn có thể truy cập trực tiếp từ LoadBalancer, dịch vụ này sẽ chuyển tiếp yêu cầu.\nTìm bộ cân bằng tải Nhấp vào chatqna-Ingress Lưu ý Tên DNS. Như đã đề cập, đây là URL công khai có thể truy cập bên ngoài. Bạn sẽ sử dụng lệnh curl để gửi yêu cầu đến các điểm cuối API, kiểm tra từng dịch vụ vi mô riêng lẻ. Mục tiêu là đặt một câu hỏi, chẳng hạn như \u0026ldquo;Doanh thu của Nike vào năm 2023 là bao nhiêu?\u0026rdquo; và xác minh rằng API phản hồi chính xác. Bước này đảm bảo rằng tất cả các dịch vụ vi mô trong hệ thống đang hoạt động như mong đợi.\nNếu mọi thứ hoạt động bình thường, bạn sẽ nhận được phản hồi xác nhận rằng quy trình làm việc Retrieval-Augmented Generation (RAG) đang hoạt động.\nTuy nhiên, bạn có thể nhận thấy rằng mô hình không thể cung cấp câu trả lời chính xác. Điều này xảy ra vì nó thiếu ngữ cảnh cần thiết và dựa vào thông tin lỗi thời. Nếu không có quyền truy cập vào dữ liệu hiện tại và có liên quan, mô hình không thể tạo ra phản hồi chính xác. Trong các bước tiếp theo, bạn sẽ cải thiện hệ thống bằng RAG, cho phép mô hình truy xuất thông tin mới nhất, có liên quan theo ngữ cảnh. Điều này sẽ đảm bảo rằng mô hình cung cấp câu trả lời chính xác và có ý nghĩa hơn.\nBây giờ, chúng ta hãy khám phá từng dịch vụ siêu nhỏ một cách chi tiết để hiểu vai trò của nó và cách nó góp phần cải thiện khả năng trả lời câu hỏi chính xác của mô hình.\nBước 2: Dịch vụ siêu nhỏ Mỗi dịch vụ siêu nhỏ tuân theo logic sau để thực hiện một tác vụ trong luồng RAG:\nTrong luồng, bạn có thể quan sát các dịch vụ siêu nhỏ và chúng ta có thể chia luồng RAG thành hai bước:\nNhắc trước: Bước này bao gồm việc chuẩn bị cơ sở kiến ​​thức (KB) bằng cách tải lên các tài liệu có liên quan và đảm bảo rằng thông tin được sắp xếp để truy xuất hiệu quả.\nNhắc nhở: Bước này tập trung vào việc truy xuất dữ liệu có liên quan từ cơ sở kiến ​​thức và sử dụng dữ liệu đó để tạo ra câu trả lời chính xác cho câu hỏi của người dùng.\nNhắc nhở trước Trong bước này, logic là bắt đầu từ một tài liệu (Nike\u0026rsquo;s revenue PDF) và thực hiện tiền xử lý cần thiết để chuẩn bị lưu trữ trong cơ sở dữ liệu. Như đã trình bày, quy trình này chủ yếu liên quan đến 3 dịch vụ siêu nhỏ: chuẩn bị dữ liệu, nhúng và lưu trữ vectơ. Hãy cùng khám phá từng dịch vụ siêu nhỏ\nNhúng dịch vụ siêu nhỏ (POD: chatqna-tei:80) Nhúng là biểu diễn số của một đối tượng—chẳng hạn như từ, cụm từ hoặc tài liệu—trong không gian vectơ liên tục. Trong xử lý ngôn ngữ tự nhiên (NLP), nhúng biến đổi các từ, câu hoặc phân đoạn văn bản thành vectơ—các tập hợp số nắm bắt ý nghĩa, mối quan hệ và ý nghĩa theo ngữ cảnh của chúng. Sự chuyển đổi này cho phép các mô hình máy học xử lý và hiểu văn bản hiệu quả hơn.\nVí dụ, nhúng từ biểu diễn các từ dưới dạng các điểm trong không gian vectơ, trong đó các từ có nghĩa tương tự như \u0026ldquo;vua\u0026rdquo; và \u0026ldquo;nữ hoàng\u0026rdquo; được đặt gần nhau hơn. Mô hình nhúng nắm bắt các mối quan hệ này thông qua phép tính vectơ.\nTrong quá trình đào tạo, nếu mô hình thường xuyên gặp \u0026ldquo;vua\u0026rdquo; khi liên kết với \u0026ldquo;đàn ông\u0026rdquo; và \u0026ldquo;nữ hoàng\u0026rdquo; khi liên kết với \u0026ldquo;phụ nữ\u0026rdquo;, mô hình sẽ học được rằng \u0026ldquo;vua\u0026rdquo; và \u0026ldquo;nữ hoàng\u0026rdquo; có mối quan hệ tương tự với \u0026ldquo;đàn ông\u0026rdquo; và \u0026ldquo;phụ nữ\u0026rdquo;. Điều này cho phép mô hình định vị các từ theo cách phản ánh các mối quan hệ có ý nghĩa, chẳng hạn như liên kết giới tính, trong ngôn ngữ.\nNhúng: Một thành phần chính của RAG Nhúng đóng vai trò quan trọng trong Thế hệ tăng cường truy xuất (RAG) bằng cách tăng cường khả năng xử lý và truy xuất thông tin có liên quan của mô hình. Chúng cung cấp một số lợi thế chính:\nGhi lại ý nghĩa – Các nhúng thể hiện mối quan hệ ngữ nghĩa giữa các từ, cho phép các mô hình RAG hiểu ngữ cảnh, sắc thái và cấu trúc ngôn ngữ sâu hơn. Điều này cải thiện khả năng tạo ra các phản hồi có liên quan và mạch lạc.\nGiảm chiều – Bằng cách chuyển đổi dữ liệu văn bản phức tạp thành các vectơ có kích thước cố định, nhúng giúp xử lý dữ liệu hiệu quả hơn và có khả năng mở rộng, cải thiện hiệu suất của hệ thống.\nNâng cao hiệu suất mô hình – Bằng cách tận dụng các điểm tương đồng về mặt ngữ nghĩa, nhúng cho phép truy xuất thông tin chính xác hơn, tinh chỉnh chất lượng phản hồi được tạo ra và giúp mô hình khái quát hóa tốt hơn trên nhiều truy vấn khác nhau.\nOPEA cung cấp nhiều tùy chọn để chạy các dịch vụ nhúng vi mô, như được nêu chi tiết trong tài liệu nhúng OPEA. Trong trường hợp này, ChatQnA sử dụng dịch vụ nhúng vi mô Hugging Face TEI, chạy mô hình nhúng BAAI/bge-large-en-v1.5 cục bộ.\nVì một số dịch vụ không được hiển thị bên ngoài, bạn sẽ sử dụng pod Nginx để tương tác với chúng thông qua curl. Để thực hiện việc này, mỗi dịch vụ sẽ được truy cập bằng tên DNS nội bộ của dịch vụ đó.\nTruy cập vào ngnix POD (sao chép toàn bộ tên pod NGNIX của bạn từ kubectl get pods và THAY THẾ chatqna-nginx-xxxxxxxx trên lệnh bên dưới) Dấu nhắc lệnh của bạn bây giờ sẽ chỉ ra rằng bạn đang ở bên trong vùng chứa, phản ánh sự thay đổi trong môi trường:\nKhi đã vào bên trong, bây giờ bạn sẽ có quyền truy cập trực tiếp vào các pod bên trong.\nNhận nhúng từ Microservice nhúng cho cụm từ \u0026ldquo;Học sâu là gì?\u0026rdquo;: Câu trả lời sẽ là biểu diễn vectơ của cụm từ \u0026ldquo;Học sâu là gì?\u0026rdquo;. Dịch vụ này trả về nhúng vectơ cho các đầu vào từ REST API.\nDịch vụ cơ sở dữ liệu vectơ (POD: chatqna-redis-vector-db:80) Dịch vụ cơ sở dữ liệu vectơ đóng vai trò quan trọng trong ứng dụng Retrieval-Augmented Generation (RAG) bằng cách lưu trữ và truy xuất các nhúng. Điều này rất cần thiết cho các ứng dụng như ChatQnA, nơi thông tin có liên quan cần được truy xuất hiệu quả dựa trên truy vấn của người dùng.\nSử dụng Redis làm Cơ sở dữ liệu vectơ Trong tác vụ này, Redis được sử dụng làm cơ sở dữ liệu vectơ. Tuy nhiên, OPEA hỗ trợ nhiều phương án thay thế, có thể tìm thấy trong kho lưu trữ vectơ OPEA.\nCơ sở dữ liệu vectơ (VDB) được thiết kế riêng để lưu trữ và quản lý các vectơ có chiều cao, biểu diễn các từ, câu hoặc hình ảnh dưới dạng số. Trong AI và học máy, các vectơ này—còn được gọi là nhúng—ghi lại ý nghĩa và mối quan hệ giữa các điểm dữ liệu, cho phép xử lý và truy xuất hiệu quả.\nDịch vụ vi mô chuẩn bị dữ liệu (POD: chatqna-data-prep:6007) Dịch vụ vi mô chuẩn bị dữ liệu (Dataprep) chịu trách nhiệm định dạng và xử lý trước dữ liệu để có thể chuyển đổi thành nhúng và lưu trữ trong cơ sở dữ liệu vector. Điều này đảm bảo dữ liệu sạch, có cấu trúc và sẵn sàng để truy xuất hiệu quả.\nChức năng chính của dịch vụ vi mô chuẩn bị dữ liệu Nhận dữ liệu thô (ví dụ: tài liệu hoặc báo cáo).\nXử lý và chia nhỏ dữ liệu thành các phân đoạn nhỏ hơn.\nGửi dữ liệu đã xử lý đến Dịch vụ vi mô nhúng để vector hóa.\nLưu trữ các nhúng kết quả trong Cơ sở dữ liệu vector.\nVì các cơ sở dữ liệu vector khác nhau có các yêu cầu định dạng dữ liệu riêng, nên Dịch vụ vi mô chuẩn bị dữ liệu đảm bảo khả năng tương thích với cơ sở dữ liệu đã chọn.\nKiểm tra các dịch vụ vi mô Để xác minh chức năng của hệ thống và giúp mô hình trả lời câu hỏi ban đầu— \u0026ldquo;Doanh thu của Nike năm 2023 là bao nhiêu?\u0026quot;—bạn sẽ cần tải lên tệp ngữ cảnh có liên quan (báo cáo doanh thu) để có thể xử lý.\nĐể thực hiện việc này, hãy tải xuống mẫu báo cáo doanh thu của Nike vào pod Nginx bằng lệnh bên dưới. (Nếu bạn không còn đăng nhập vào Nginx pod, hãy đảm bảo đăng nhập lại trước khi tiếp tục.)\nThực hiện lệnh sau để tải xuống báo cáo doanh thu Nike mẫu vào nginx pod (nếu bạn không còn đăng nhập vào NGinx pod, hãy đảm bảo sử dụng lệnh trên để đăng nhập lại):\nTải xuống tài liệu vào microservice: Cung cấp tài liệu cho cơ sở kiến ​​thức (Vectord) (Sẽ mất khoảng 30 giây): Sau khi chạy lệnh trước đó, bạn sẽ nhận được thông báo xác nhận như bên dưới. Lệnh này đã cập nhật cơ sở kiến ​​thức bằng cách tải lên tệp cục bộ để xử lý.\nAPI microservice chuẩn bị dữ liệu có thể truy xuất thông tin về danh sách các tệp được lưu trữ trong cơ sở dữ liệu vector.\nKiểm tra xem tài liệu đã được tải lên chưa: Sau khi chạy lệnh trước, bạn sẽ nhận được thông báo xác nhận.\nXin chúc mừng! Bạn đã chuẩn bị thành công cơ sở kiến ​​thức của mình. Bây giờ bạn sẽ khám phá các dịch vụ vi mô liên quan đến xử lý nhanh chóng.\nBước 3: Nhắc nhở Sau khi cơ sở kiến ​​thức được thiết lập, bạn có thể bắt đầu tương tác với ứng dụng bằng cách đặt các câu hỏi theo ngữ cảnh cụ thể. Retrieval-Augmented Generation (RAG) đảm bảo rằng các phản hồi vừa chính xác vừa dựa trên dữ liệu có liên quan.\nQuy trình bắt đầu bằng việc ứng dụng truy xuất thông tin có liên quan nhất từ ​​cơ sở kiến ​​thức để trả lời truy vấn của người dùng. Bước này đảm bảo rằng Mô hình ngôn ngữ lớn (LLM) có quyền truy cập vào ngữ cảnh chính xác và cập nhật để tạo ra phản hồi có thông tin.\nTiếp theo, thông tin đã truy xuất được kết hợp với lời nhắc nhập của người dùng và gửi đến LLM. Lời nhắc được làm giàu này nâng cao khả năng của mô hình trong việc cung cấp các câu trả lời không chỉ dựa trên kiến ​​thức đã được đào tạo trước mà còn được hỗ trợ bởi dữ liệu bên ngoài trong thế giới thực.\nCuối cùng, bạn sẽ thấy cách LLM xử lý lời nhắc được làm giàu này để tạo ra phản hồi mạch lạc và chính xác theo ngữ cảnh. Bằng cách tận dụng RAG, ứng dụng cung cấp các câu trả lời có liên quan cao, dựa trên thông tin mới nhất từ ​​cơ sở kiến ​​thức.\nCác dịch vụ vi mô liên quan đến giai đoạn này bao gồm:\nNhúng Cơ sở dữ liệu vectơ Trình thu thập Xếp hạng lại LLM Dịch vụ vi mô thu thập (POD: chatqna-retriever-usvc:7000) Dịch vụ vi mô thu thập chịu trách nhiệm định vị thông tin có liên quan nhất trong cơ sở kiến ​​thức và trả về các tài liệu khớp chặt chẽ với truy vấn của người dùng. Nó tương tác với nhiều hệ thống phụ trợ lưu trữ kiến ​​thức và cung cấp API để truy xuất dữ liệu khớp nhất.\nCác cơ sở kiến ​​thức khác nhau sử dụng các phương pháp truy xuất khác nhau:\nCơ sở dữ liệu vectơ sử dụng phương pháp khớp tương tự vectơ giữa câu hỏi của người dùng và nhúng tài liệu được lưu trữ. Cơ sở dữ liệu đồ thị tận dụng vị trí đồ thị để tìm thông tin liên quan. Cơ sở dữ liệu quan hệ dựa vào phương pháp khớp chuỗi và biểu thức chính quy để định vị văn bản có liên quan. Trong nhiệm vụ này, bạn sẽ sử dụng Redis làm cơ sở dữ liệu vectơ và truy xuất thông tin thông qua trình thu thập Redis.\nVì việc truy xuất vectơ dựa vào nhúng, trước tiên bạn cần tạo nhúng cho câu hỏi: \u0026ldquo;Doanh thu của Nike năm 2023 là bao nhiêu?\u0026rdquo;\nThao tác này sẽ cho phép trình thu thập tìm kiếm trong cơ sở kiến ​​thức để tìm tài liệu có liên quan nhất—chẳng hạn như báo cáo doanh thu của Nike mà bạn đã tải lên ở bước trước.\nĐể tạo nhúng, hãy sử dụng dịch vụ vi mô chatqna-tei. (Đảm bảo bạn đã đăng nhập vào pod Nginx trước khi tiếp tục.)\nTạo nhúng và lưu cục bộ (embed_question): Bạn sẽ nhận được thông tin chi tiết về tác vụ viết:\nKiểm tra xem nhúng của bạn đã được lưu chưa: echo $embed_question\nBạn sẽ có thể thấy các vectơ mà dịch vụ vi mô nhúng tạo ra. Bây giờ bạn có thể sử dụng dịch vụ vi mô trình thu thập để có được thông tin tương tự nhất từ ​​cơ sở kiến ​​thức của mình.\nLấy và lưu các vectơ tương tự từ embed_question ban đầu tại địa phương similar_docs: similar_docs=$(curl chatqna-retriever-usvc:7000/v1/retrieval -X POST -d \u0026ldquo;{\u0026quot;text\u0026quot;:\u0026quot;test\u0026quot;,\u0026quot;embedding\u0026quot;:${embed_question}}\u0026rdquo; -H \u0026lsquo;Content-Type: application/json\u0026rsquo;)\nBằng cách xem kết quả đầu ra trước đó, bạn có thể thấy các đoạn tương tự nhất (TOP_3) từ tài liệu Báo cáo doanh thu của Nike và câu hỏi \u0026ldquo;Doanh thu của Nike năm 2023 là bao nhiêu?\u0026rdquo;.\necho $similar_docs\nKết quả đầu ra sau đây đã được định dạng để dễ đọc hơn. Kết quả của bạn sẽ được trình bày dưới dạng văn bản thuần túy và có thể hơi khác một chút do thuật toán tìm kiếm tương tự. Tuy nhiên, bạn có thể kiểm tra lại xem các tài liệu đã truy xuất có liên quan đến truy vấn ban đầu của bạn không.\nỨng dụng sẽ sử dụng thông tin đó làm ngữ cảnh để nhắc LLM, nhưng vẫn còn một bước nữa mà bạn cần thực hiện để tinh chỉnh và kiểm tra chất lượng của các tài liệu đã truy xuất đó: trình xếp hạng lại.\nVi dịch vụ xếp hạng lại (POD: chatqna-teirerank:80) Vi dịch vụ xếp hạng lại đóng vai trò quan trọng trong tìm kiếm ngữ nghĩa, tận dụng các mô hình xếp hạng lại để tăng cường tính liên quan của các kết quả đã truy xuất. Khi được cung cấp truy vấn của người dùng và một tập hợp các tài liệu, vi dịch vụ này sẽ sắp xếp lại các tài liệu dựa trên mức độ tương đồng về mặt ngữ nghĩa của chúng với truy vấn, đảm bảo rằng các kết quả có liên quan nhất sẽ xuất hiện đầu tiên.\nXếp hạng lại đặc biệt có giá trị trong các hệ thống truy xuất văn bản, trong đó các tài liệu ban đầu được truy xuất bằng cách sử dụng:\nNhúng dày đặc, nắm bắt ý nghĩa ngữ nghĩa sâu sắc. Tìm kiếm từ vựng thưa thớt, dựa trên việc khớp dựa trên từ khóa. Mặc dù các phương pháp truy xuất này có hiệu quả, nhưng mô hình xếp hạng lại sẽ tinh chỉnh các kết quả bằng cách tối ưu hóa thứ tự của các tài liệu đã truy xuất. Bước này cải thiện đáng kể độ chính xác, đảm bảo đầu ra cuối cùng có liên quan hơn, chính xác hơn và phù hợp hơn về mặt ngữ cảnh với truy vấn của người dùng.\nOPEA có nhiều tùy chọn để xếp hạng lại. Đối với phòng thí nghiệm này, bạn sẽ sử dụng Hugging Face TEI để xếp hạng lại. Đây là dịch vụ vi mô chatqna-teirerank trong cụm của bạn.\nTrình xếp hạng lại sẽ sử dụng similar_docs từ giai đoạn trước và so sánh với câu hỏi Doanh thu Nike năm 2023 là bao nhiêu? để kiểm tra chất lượng của các tài liệu đã truy xuất.\nTrích xuất 3 đoạn văn bản đã truy xuất và lưu chúng trong một biến mới để xếp hạng lại:\nCài đặt các phụ thuộc jq để định dạng similar_docs echo -e \u0026ldquo;deb http://deb.debian.org/debian bookworm main contrib non-free\\ndeb http://security.debian.org/debian-security bookworm-security main contrib non-free\\ndeb http://deb.debian.org/debian bookworm-updates main contrib non-free\u0026rdquo; \u0026gt; /etc/apt/sources.list \u0026amp;\u0026amp; apt update \u0026amp;\u0026amp; apt install -y jq\nTrích xuất và định dạng các văn bản thành một mảng chuỗi JSON hợp lệ texts=$(echo \u0026ldquo;$similar_docs\u0026rdquo; | jq -r \u0026lsquo;[.retrieved_docs[].text | @json]\u0026rsquo;)\nGửi yêu cầu đến dịch vụ vi mô với truy vấn và các văn bản đã định dạng: curl -X POST chatqna-teirerank:80/rerank -d \u0026ldquo;{\u0026quot;query\u0026quot;:\u0026quot;Doanh thu của Nike năm 2023 là bao nhiêu?\u0026quot;, \u0026quot;texts\u0026quot;: $texts}\u0026rdquo; -H \u0026lsquo;Content-Type: application/json\u0026rsquo;\nResponse:\nĐầu ra sau đây đã được định dạng để dễ đọc hơn. Kết quả của bạn được hiển thị ở dạng văn bản thuần túy và có thể thay đổi đôi chút do thuật toán tìm kiếm tương đồng. Các tài liệu được truy xuất được xếp hạng theo mức độ tương đồng với truy vấn của bạn, với chỉ mục được xếp hạng cao nhất thể hiện sự khớp có liên quan nhất. Bạn có thể xác nhận rằng tài liệu được xếp hạng cao nhất tương ứng với tài liệu phù hợp nhất với truy vấn của bạn.\nMáy chủ phản hồi bằng một mảng JSON chứa các đối tượng có hai trường: chỉ mục và điểm. Điều này cho biết cách các đoạn trích được xếp hạng dựa trên mức độ liên quan của chúng với truy vấn: {\u0026ldquo;index\u0026rdquo;:2,\u0026ldquo;score\u0026rdquo;:0.9972289} có nghĩa là văn bản đầu tiên (chỉ mục 0) có điểm liên quan cao khoảng 0,7982. {\u0026ldquo;index\u0026rdquo;:0,\u0026ldquo;score\u0026rdquo;:0.9776342},{\u0026ldquo;index\u0026rdquo;:3,\u0026ldquo;score\u0026rdquo;:0.9296986},{\u0026ldquo;index\u0026rdquo;:1,\u0026ldquo;score\u0026rdquo;:0.84730965} cho biết các đoạn trích khác (chỉ mục 3, 1 và 2) có điểm thấp hơn nhiều.\nNhư bạn có thể thấy từ similar_doc, id=2 có thông tin bên dưới, trong đó nó CHÍNH XÁC đề cập đến doanh thu năm 2023!\nChỉ đoạn trích đầu tiên sẽ được sử dụng để nhắc LLM.\nLLM Microservice (POD: chatqna-tgi:80) Cốt lõi của ứng dụng RAG (Retrieval-Augmented Generation) là Mô hình ngôn ngữ lớn (LLM), đóng vai trò quan trọng trong việc tạo ra phản hồi. Bằng cách tận dụng RAG, hệ thống nâng cao hiệu suất của LLM, đảm bảo phản hồi chính xác, có liên quan và nhận thức được ngữ cảnh.\nCác loại LLM LLM thường được chia thành hai loại chính, mỗi loại có điểm mạnh và điểm yếu riêng:\nMô hình nguồn đóng Các mô hình độc quyền này được phát triển bởi các công ty công nghệ lớn như Amazon Web Services (AWS), OpenAI và Google. Chúng được đào tạo trên các tập dữ liệu mở rộng và được tối ưu hóa để có đầu ra chất lượng cao, đáng tin cậy. Tuy nhiên, chúng đi kèm với một số hạn chế nhất định: Tùy chỉnh hạn chế: Người dùng có quyền kiểm soát tối thiểu đối với việc tinh chỉnh. Chi phí cao hơn: Quyền truy cập thường bị giới hạn và có thể tốn kém. Mối quan ngại về chủ quyền dữ liệu: Quyền truy cập API có thể hạn chế việc sử dụng trong các ứng dụng yêu cầu quản trị dữ liệu chặt chẽ. Mô hình nguồn mở Có sẵn miễn phí để sử dụng và sửa đổi, LLM nguồn mở cung cấp tính linh hoạt và khả năng kiểm soát cao hơn. Chúng cho phép người dùng tùy chỉnh và tinh chỉnh các mô hình theo nhu cầu cụ thể. Chạy các mô hình nguồn mở cục bộ hoặc trên cơ sở hạ tầng đám mây riêng đảm bảo quyền riêng tư dữ liệu và hiệu quả chi phí tốt hơn. Tuy nhiên, chúng yêu cầu: Chuyên môn kỹ thuật: Việc triển khai và tối ưu hóa các mô hình nguồn mở có thể phức tạp. Tài nguyên tính toán: Để đạt được hiệu suất tương đương với các mô hình đóng thường đòi hỏi phần cứng mạnh mẽ. Tích hợp linh hoạt với OPEA: Kiến trúc dịch vụ vi mô này hỗ trợ cả mô hình đóng và mô hình nguồn mở, cung cấp tính linh hoạt để lựa chọn mô hình phù hợp nhất cho ứng dụng của bạn. Trong ví dụ này, mô hình TGI (Suy luận tạo văn bản) từ Hugging Face được sử dụng. Kiểm tra dịch vụ vi mô LLM: Để xác minh chức năng của nó, bạn có thể trực tiếp nhắc TGI LLM bằng một câu hỏi mẫu: \u0026ldquo;Doanh thu của Nike vào năm 2023 là bao nhiêu?\u0026rdquo;\nBài kiểm tra này sẽ chứng minh mô hình có thể truy xuất và tạo phản hồi có thông tin tốt như thế nào dựa trên cơ sở kiến ​​thức đã tải.\nTrực tiếp nhắc nhở TGI(LLM) Microservice: Mô hình sẽ cung cấp cho bạn câu trả lời cho lời nhắc như sau:\n**\u0026ldquo;generated_text\u0026rdquo;:\u0026rdquo; Doanh thu của Nike năm 2023 chưa được báo cáo vì vẫn đang trong quý 4. Năm tài chính đầy đủ trước đó—tức là năm 2022—đã mang lại 48,9 tỷ đô la doanh thu cho công ty đồ thể thao đa quốc gia của Mỹ. Họ xử lý việc thiết kế, phát triển, sản xuất và tiếp thị/bán hàng trên toàn thế giới của một danh mục sản phẩm đa dạng. Từ khi thành lập vào năm 1964 với tên gọi Blue Ribbon Sports, công ty đã được đổi tên thành Nike, Inc., vào năm 1978. Logo Jumpman (ví dụ), bao gồm Michael\n"
},
{
	"uri": "//localhost:1313/vi/3-accessibilitytoinstances/3.2-private-instance-copy-2/",
	"title": "Kiểm tra việc triển khai và xác minh quy trình làm việc RAG",
	"tags": [],
	"description": "",
	"content": "Hiểu RAG và sử dụng UI Bây giờ bạn đã xác minh tất cả các dịch vụ đang chạy, hãy cùng xem UI do triển khai cung cấp.\nĐể truy cập UI, hãy mở bất kỳ trình duyệt nào và truy cập DNS của ChatQnA Load Balancer: http://chatqna-ingress-xxxxxxx.us-east-2.elb.amazonaws.com (Sửa đổi bằng URL chatqna-ingressDNS của bạn)\nTrong UI, bạn có thể thấy chatbot tương tác với nó\nĐể xác minh UI, hãy tiếp tục và hỏi\nCâu trả lời lại đúng vì chúng ta đã lập chỉ mục cơ sở kiến ​​thức của mình ở bước trước.\nHãy thử một cái gì đó khác. Ứng dụng có thể trả lời về OPEA không:\nBạn có thể nhận thấy rằng phản hồi ban đầu của chatbot đã lỗi thời hoặc thiếu thông tin chi tiết cụ thể về OPEA. Nguyên nhân là do OPEA là một dự án tương đối mới và không được đưa vào tập dữ liệu được sử dụng để đào tạo mô hình ngôn ngữ. Vì hầu hết các mô hình ngôn ngữ đều là tĩnh—có nghĩa là chúng dựa vào dữ liệu có sẵn tại thời điểm đào tạo—nên chúng không thể tự động kết hợp các phát triển gần đây hoặc các chủ đề mới nổi như OPEA.\nTuy nhiên, RAG cung cấp giải pháp bằng cách cho phép truy xuất ngữ cảnh theo thời gian thực. Trong UI, bạn sẽ tìm thấy tùy chọn tải lên thông tin ngữ cảnh có liên quan. Khi bạn thực hiện việc này, tài liệu sẽ được gửi đến dịch vụ vi mô DataPrep, tại đó tài liệu sẽ được chuyển đổi thành nhúng và lưu trữ trong Cơ sở dữ liệu Vector.\nBằng cách tải lên tài liệu hoặc liên kết, bạn sẽ mở rộng hiệu quả cơ sở kiến ​​thức của chatbot bằng thông tin mới nhất, cải thiện tính liên quan và độ chính xác của phản hồi.\nViệc triển khai cho phép bạn tải lên tệp hoặc trang web. Đối với trường hợp này, hãy sử dụng trang web OPEA:\nNhấp vào biểu tượng tải lên để mở bảng điều khiển bên phải Nhấp vào Dán liên kết Sao chép/dán văn bản https://opea-project.github.io/latest/introduction/index.html vào hộp nhập Nhấp vào Xác nhận để bắt đầu quá trình lập chỉ mục Khi quá trình lập chỉ mục hoàn tất, bạn sẽ thấy một biểu tượng được thêm vào bên dưới hộp văn bản, có nhãn là https://opea-project.github.io/latest/introduction/index.html\nHỏi \u0026ldquo;OPEA là gì?\u0026rdquo; một lần nữa để xem câu trả lời đã cập nhật.\nLần này, chatbot phản hồi chính xác dựa trên dữ liệu mà nó đã thêm vào lời nhắc từ nguồn mới, trang web OPEA.\nKết luận\nTrong nhiệm vụ này, bạn đã khám phá cấu trúc cốt lõi của ứng dụng RAG, hiểu rõ hơn về cách từng thành phần hoạt động và tương tác trong hệ thống. Từ việc truy xuất thông tin có liên quan đến việc tạo phản hồi chính xác, mọi phần đều đóng vai trò quan trọng trong quy trình làm việc RAG của OPEA—nâng cao tính liên quan của phản hồi thông qua việc truy xuất trong khi cải thiện độ chính xác với mô hình ngôn ngữ nâng cao. Buổi thực hành này cung cấp hiểu biết rõ ràng về cách OPEA tận dụng RAG để xử lý các truy vấn phức tạp một cách hiệu quả và tinh chỉnh hiệu suất mô hình thông qua tích hợp thành phần liền mạch.\nTrong nhiệm vụ tiếp theo, bạn sẽ triển khai các rào cản cho chatbot. Các rào cản này rất cần thiết để phát hiện và giảm thiểu sự thiên vị, đảm bảo rằng các phản hồi do AI tạo ra vẫn có trách nhiệm, công bằng và phù hợp với các nguyên tắc AI có đạo đức.\n"
},
{
	"uri": "//localhost:1313/vi/3-accessibilitytoinstances/3.2-private-instance-copy/",
	"title": "Kiểm tra việc triển khai và xác minh quy trình làm việc RAG",
	"tags": [],
	"description": "",
	"content": "Hiểu RAG và sử dụng UI Bây giờ bạn đã xác minh tất cả các dịch vụ đang chạy, hãy cùng xem UI do triển khai cung cấp.\nĐể truy cập UI, hãy mở bất kỳ trình duyệt nào và truy cập DNS của ChatQnA Load Balancer: http://chatqna-ingress-xxxxxxx.us-east-2.elb.amazonaws.com (Sửa đổi bằng URL chatqna-ingressDNS của bạn)\nTrong UI, bạn có thể thấy chatbot tương tác với nó\nĐể xác minh UI, hãy tiếp tục và hỏi\nCâu trả lời lại đúng vì chúng ta đã lập chỉ mục cơ sở kiến ​​thức của mình ở bước trước.\nHãy thử một cái gì đó khác. Ứng dụng có thể trả lời về OPEA không:\nBạn có thể nhận thấy rằng phản hồi ban đầu của chatbot đã lỗi thời hoặc thiếu thông tin chi tiết cụ thể về OPEA. Nguyên nhân là do OPEA là một dự án tương đối mới và không được đưa vào tập dữ liệu được sử dụng để đào tạo mô hình ngôn ngữ. Vì hầu hết các mô hình ngôn ngữ đều là tĩnh—có nghĩa là chúng dựa vào dữ liệu có sẵn tại thời điểm đào tạo—nên chúng không thể tự động kết hợp các phát triển gần đây hoặc các chủ đề mới nổi như OPEA.\nTuy nhiên, RAG cung cấp giải pháp bằng cách cho phép truy xuất ngữ cảnh theo thời gian thực. Trong UI, bạn sẽ tìm thấy tùy chọn tải lên thông tin ngữ cảnh có liên quan. Khi bạn thực hiện việc này, tài liệu sẽ được gửi đến dịch vụ vi mô DataPrep, tại đó tài liệu sẽ được chuyển đổi thành nhúng và lưu trữ trong Cơ sở dữ liệu Vector.\nBằng cách tải lên tài liệu hoặc liên kết, bạn sẽ mở rộng hiệu quả cơ sở kiến ​​thức của chatbot bằng thông tin mới nhất, cải thiện tính liên quan và độ chính xác của phản hồi.\nViệc triển khai cho phép bạn tải lên tệp hoặc trang web. Đối với trường hợp này, hãy sử dụng trang web OPEA:\nNhấp vào biểu tượng tải lên để mở bảng điều khiển bên phải Nhấp vào Dán liên kết Sao chép/dán văn bản https://opea-project.github.io/latest/introduction/index.html vào hộp nhập Nhấp vào Xác nhận để bắt đầu quá trình lập chỉ mục Khi quá trình lập chỉ mục hoàn tất, bạn sẽ thấy một biểu tượng được thêm vào bên dưới hộp văn bản, có nhãn là https://opea-project.github.io/latest/introduction/index.html\nHỏi \u0026ldquo;OPEA là gì?\u0026rdquo; một lần nữa để xem câu trả lời đã cập nhật.\nLần này, chatbot phản hồi chính xác dựa trên dữ liệu mà nó đã thêm vào lời nhắc từ nguồn mới, trang web OPEA.\nKết luận\nTrong nhiệm vụ này, bạn đã khám phá cấu trúc cốt lõi của ứng dụng RAG, hiểu rõ hơn về cách từng thành phần hoạt động và tương tác trong hệ thống. Từ việc truy xuất thông tin có liên quan đến việc tạo phản hồi chính xác, mọi phần đều đóng vai trò quan trọng trong quy trình làm việc RAG của OPEA—nâng cao tính liên quan của phản hồi thông qua việc truy xuất trong khi cải thiện độ chính xác với mô hình ngôn ngữ nâng cao. Buổi thực hành này cung cấp hiểu biết rõ ràng về cách OPEA tận dụng RAG để xử lý các truy vấn phức tạp một cách hiệu quả và tinh chỉnh hiệu suất mô hình thông qua tích hợp thành phần liền mạch.\nTrong nhiệm vụ tiếp theo, bạn sẽ triển khai các rào cản cho chatbot. Các rào cản này rất cần thiết để phát hiện và giảm thiểu sự thiên vị, đảm bảo rằng các phản hồi do AI tạo ra vẫn có trách nhiệm, công bằng và phù hợp với các nguyên tắc AI có đạo đức.\n"
},
{
	"uri": "//localhost:1313/vi/2-prerequiste/2.2-createiamrole-copy-2/",
	"title": "Sử dụng tài khoản của riêng bạn",
	"tags": [],
	"description": "",
	"content": "Hướng dẫn thiết lập môi trường cho phòng thí nghiệm AWS Nếu bạn KHÔNG thực hiện các phòng thí nghiệm này trong một sự kiện do AWS tổ chức trên Workshop Studio, bạn cần chuẩn bị môi trường trước khi bắt đầu. Lưu ý rằng các tài nguyên này sẽ phát sinh chi phí, vì vậy hãy nhớ dọn dẹp sau khi hoàn thành.\nBước 1: Cấu hình môi trường Thiết lập tài khoản Thực hiện các bước sau để cấu hình tài khoản của bạn:\nNhấn vào Launch Stack để khởi chạy CloudFormation với các giá trị được cấu hình sẵn trong khu vực us-east-1.\nLaunch Stack\nNếu muốn chạy workshop ở khu vực khác, hãy thay đổi khu vực phù hợp. Stack sẽ thiết lập các thành phần sau: Cụm EKS (Amazon Elastic Kubernetes Service):\n-Tạo một cụm EKS mới có tên opea-eks-cluster. -Triển khai một node trong cụm sử dụng M7i.24xlarge.\nCác mẫu CloudFormation: Cung cấp các mẫu CloudFormation để triển khai các mô-đun sau: Module 1:ChatQnA Default\nModule 2:ChatQnA with Guardrails\nModule 3:ChatQnA with OpenSearch (một cơ sở dữ liệu vector mã nguồn mở)\nModule 4:ChatQnA with Remote Inference (Denvr) làm mô hình LLM\nModule 5:ChatQnA with Bedrock làm mô hình LLM\nCấu hình trước khi chạy Stack Trước khi chạy stack, trên trang Quick create stack, thực hiện các bước sau:\nHuggingFaceToken:Cung cấp mã token để tải xuống mô hình từ Hugging Face. Nếu sử dụng tính năng Guardrails, đảm bảo token có quyền truy cập vào mô hình meta-llama/Meta-Llama-Guard-2-8B.\nModelID:OPEA sử dụng Text Generation Inference toolkit. Chọn mô hình từ danh sách các mô hình được hỗ trợ trên Hugging Face và nhập Model ID.\nOpeaRoleArn:Nhập ARN hoặc tên của role mà bạn đang sử dụng.\nNếu không chắc, hãy kiểm tra thông tin người dùng ở góc trên bên phải màn hình. Nếu tên không có dấu /, sao chép toàn bộ. Nếu có dấu /, chỉ lấy phần trước dấu /. Ví dụ: Nếu thấy \u0026ldquo;USER\u0026rdquo;, nhập \u0026ldquo;USER\u0026rdquo;. Nếu thấy \u0026ldquo;ADMIN-ROLE/USER\u0026rdquo;, chỉ nhập \u0026ldquo;ADMIN-ROLE\u0026rdquo;. Lấy ARN bằng lệnh AWS CLI (nếu cần). aws sts get-caller-identity \u0026ndash;query \u0026lsquo;Arn\u0026rsquo; \u0026ndash;output text | awk -F: \u0026lsquo;{print $NF}\u0026rsquo; | (read id; if [[ $id == \u0026ldquo;user\u0026rdquo; ]]; then aws sts get-caller-identity \u0026ndash;query \u0026lsquo;Arn\u0026rsquo; \u0026ndash;output text; else role=$(echo $id | cut -d\u0026rsquo;/\u0026rsquo; -f2); aws iam get-role \u0026ndash;role-name $role \u0026ndash;query \u0026lsquo;Role.Arn\u0026rsquo; \u0026ndash;output text; fi)\nĐánh dấu vào hộp kiểm bên cạnh Tôi xác nhận rằng AWS CloudFormation có thể tạo tài nguyên IAM\nQuá trình triển khai Sau khi thiết lập các tham số, nhấp vào Tạo ngăn xếp.\nQuá trình này sẽ khởi tạo Dự án AWS CodeBuild, dự án này sẽ đưa thư viện nguồn mở opea-demo-builder vào\nBộ công cụ phát triển đám mây AWS (CDK) sẽ tạo các mẫu CloudFormation cần thiết để định cấu hình môi trường AWS của bạn cho hội thảo.\nGiám sát tiến độ triển khai Quá trình triển khai mất khoảng 25 phút. Mở Bảng điều khiển AWS CloudFormation và theo dõi tiến trình.\nĐảm bảo rằng các ngăn xếp sau đạt trạng thái CREATE_COMPLETE:\nCụm EKS Ứng dụng ChatQnA (triển khai mặc định) Một số ngăn xếp có thể vẫn còn trong REVIEW_IN_PROGRESS vì chúng sẽ được triển khai trên cụm EKS sau này.\nBước 2: Định cấu hình quyền truy cập vào cụm EKS của bạn Sau khi triển khai thành công các ngăn xếp CloudFormation, bạn cần đặt cấu hình môi trường cục bộ của mình để tương tác với cụm Amazon EKS bằng kubectl.\nCập nhật cấu hình Kubernetes của bạn (kubeconfig)\nMở AWS CloudShell Bấm vào biểu tượng CloudShell trên Bảng điều khiển quản lý AWS. Ngoài ra, hãy sử dụng AWS CLI cục bộ của bạn, đảm bảo kubectl và máy khách AWS CLI được cài đặt trên hệ thống của bạn. Cập nhật kubeconfig Chạy lệnh sau (cập nhật vùng nếu cần): Nếu thành công, bạn sẽ nhận được thông báo xác nhận cho biết file cấu hình của bạn đã được cập nhật. Xác minh kết nối với cụm EKS của bạn Đảm bảo bạn có thể tương tác với cụm bằng kubectl. Bước 3: Xác minh quyền truy cập cụm EKS Sau khi cập nhật kubeconfig, hãy kiểm tra xem bạn có thể kết nối với cụm không. Nếu bạn gặp sự cố khi truy cập nhóm thông qua bảng điều khiển AWS hoặc CloudShell, hãy làm theo các bước sau:\nKiểm tra quyền truy cập IAM của bạn trong Bảng điều khiển EKS\nĐiều hướng đến Bảng điều khiển EKS. Tìm ARN chính IAM của bạn trong Mục truy cập. Nếu ARN của bạn không được liệt kê, hãy thêm nó theo cách thủ công\nBấm vào Tạo mục truy cập. Nhập ARN người dùng IAM hoặc ARN vai trò của bạn. Đính kèm các chính sách bắt buộc Gán các quyền sau cho vai trò IAM của bạn:\nChính sách quản trị của AmazonEKS Chính sách quản trị AmazonEKSCluster Xác nhận quyền truy cập bằng nút niêm yết\nChạy lệnh sau để kiểm tra xem các nút worker có hiển thị hay không: Nếu lệnh trả về danh sách các nút thì cluster của bạn đã được cấu hình thành công. Lưu ý: Nếu không có nút nào xuất hiện, hãy đợi vài phút và thử lại vì quá trình cung cấp nút có thể vẫn đang được tiến hành. Các bước tiếp theo: Khám phá các mô-đun hội thảo Sau khi kết nối thành công với cụm EKS của mình, bạn đã sẵn sàng tiếp tục với bất kỳ mô-đun hội thảo nào. Chọn mô-đun phù hợp nhất với mục tiêu học tập của bạn và bắt đầu trải nghiệm thực tế với các giải pháp AWS, Kubernetes và hỗ trợ AI.\n"
},
{
	"uri": "//localhost:1313/vi/2-prerequiste/2.2-createiamrole-copy/",
	"title": "Sử dụng tài khoản của riêng bạn",
	"tags": [],
	"description": "",
	"content": "Hướng dẫn thiết lập môi trường cho phòng thí nghiệm AWS Nếu bạn KHÔNG thực hiện các phòng thí nghiệm này trong một sự kiện do AWS tổ chức trên Workshop Studio, bạn cần chuẩn bị môi trường trước khi bắt đầu. Lưu ý rằng các tài nguyên này sẽ phát sinh chi phí, vì vậy hãy nhớ dọn dẹp sau khi hoàn thành.\nBước 1: Cấu hình môi trường Thiết lập tài khoản Thực hiện các bước sau để cấu hình tài khoản của bạn:\nNhấn vào Launch Stack để khởi chạy CloudFormation với các giá trị được cấu hình sẵn trong khu vực us-east-1.\nLaunch Stack\nNếu muốn chạy workshop ở khu vực khác, hãy thay đổi khu vực phù hợp. Stack sẽ thiết lập các thành phần sau: Cụm EKS (Amazon Elastic Kubernetes Service):\n-Tạo một cụm EKS mới có tên opea-eks-cluster. -Triển khai một node trong cụm sử dụng M7i.24xlarge.\nCác mẫu CloudFormation: Cung cấp các mẫu CloudFormation để triển khai các mô-đun sau: Module 1:ChatQnA Default\nModule 2:ChatQnA with Guardrails\nModule 3:ChatQnA with OpenSearch (một cơ sở dữ liệu vector mã nguồn mở)\nModule 4:ChatQnA with Remote Inference (Denvr) làm mô hình LLM\nModule 5:ChatQnA with Bedrock làm mô hình LLM\nCấu hình trước khi chạy Stack Trước khi chạy stack, trên trang Quick create stack, thực hiện các bước sau:\nHuggingFaceToken:Cung cấp mã token để tải xuống mô hình từ Hugging Face. Nếu sử dụng tính năng Guardrails, đảm bảo token có quyền truy cập vào mô hình meta-llama/Meta-Llama-Guard-2-8B.\nModelID:OPEA sử dụng Text Generation Inference toolkit. Chọn mô hình từ danh sách các mô hình được hỗ trợ trên Hugging Face và nhập Model ID.\nOpeaRoleArn:Nhập ARN hoặc tên của role mà bạn đang sử dụng.\nNếu không chắc, hãy kiểm tra thông tin người dùng ở góc trên bên phải màn hình. Nếu tên không có dấu /, sao chép toàn bộ. Nếu có dấu /, chỉ lấy phần trước dấu /. Ví dụ: Nếu thấy \u0026ldquo;USER\u0026rdquo;, nhập \u0026ldquo;USER\u0026rdquo;. Nếu thấy \u0026ldquo;ADMIN-ROLE/USER\u0026rdquo;, chỉ nhập \u0026ldquo;ADMIN-ROLE\u0026rdquo;. Lấy ARN bằng lệnh AWS CLI (nếu cần). aws sts get-caller-identity \u0026ndash;query \u0026lsquo;Arn\u0026rsquo; \u0026ndash;output text | awk -F: \u0026lsquo;{print $NF}\u0026rsquo; | (read id; if [[ $id == \u0026ldquo;user\u0026rdquo; ]]; then aws sts get-caller-identity \u0026ndash;query \u0026lsquo;Arn\u0026rsquo; \u0026ndash;output text; else role=$(echo $id | cut -d\u0026rsquo;/\u0026rsquo; -f2); aws iam get-role \u0026ndash;role-name $role \u0026ndash;query \u0026lsquo;Role.Arn\u0026rsquo; \u0026ndash;output text; fi)\nĐánh dấu vào hộp kiểm bên cạnh Tôi xác nhận rằng AWS CloudFormation có thể tạo tài nguyên IAM\nQuá trình triển khai Sau khi thiết lập các tham số, nhấp vào Tạo ngăn xếp.\nQuá trình này sẽ khởi tạo Dự án AWS CodeBuild, dự án này sẽ đưa thư viện nguồn mở opea-demo-builder vào\nBộ công cụ phát triển đám mây AWS (CDK) sẽ tạo các mẫu CloudFormation cần thiết để định cấu hình môi trường AWS của bạn cho hội thảo.\nGiám sát tiến độ triển khai Quá trình triển khai mất khoảng 25 phút. Mở Bảng điều khiển AWS CloudFormation và theo dõi tiến trình.\nĐảm bảo rằng các ngăn xếp sau đạt trạng thái CREATE_COMPLETE:\nCụm EKS Ứng dụng ChatQnA (triển khai mặc định) Một số ngăn xếp có thể vẫn còn trong REVIEW_IN_PROGRESS vì chúng sẽ được triển khai trên cụm EKS sau này.\nBước 2: Định cấu hình quyền truy cập vào cụm EKS của bạn Sau khi triển khai thành công các ngăn xếp CloudFormation, bạn cần đặt cấu hình môi trường cục bộ của mình để tương tác với cụm Amazon EKS bằng kubectl.\nCập nhật cấu hình Kubernetes của bạn (kubeconfig)\nMở AWS CloudShell Bấm vào biểu tượng CloudShell trên Bảng điều khiển quản lý AWS. Ngoài ra, hãy sử dụng AWS CLI cục bộ của bạn, đảm bảo kubectl và máy khách AWS CLI được cài đặt trên hệ thống của bạn. Cập nhật kubeconfig Chạy lệnh sau (cập nhật vùng nếu cần): Nếu thành công, bạn sẽ nhận được thông báo xác nhận cho biết file cấu hình của bạn đã được cập nhật. Xác minh kết nối với cụm EKS của bạn Đảm bảo bạn có thể tương tác với cụm bằng kubectl. Bước 3: Xác minh quyền truy cập cụm EKS Sau khi cập nhật kubeconfig, hãy kiểm tra xem bạn có thể kết nối với cụm không. Nếu bạn gặp sự cố khi truy cập nhóm thông qua bảng điều khiển AWS hoặc CloudShell, hãy làm theo các bước sau:\nKiểm tra quyền truy cập IAM của bạn trong Bảng điều khiển EKS\nĐiều hướng đến Bảng điều khiển EKS. Tìm ARN chính IAM của bạn trong Mục truy cập. Nếu ARN của bạn không được liệt kê, hãy thêm nó theo cách thủ công\nBấm vào Tạo mục truy cập. Nhập ARN người dùng IAM hoặc ARN vai trò của bạn. Đính kèm các chính sách bắt buộc Gán các quyền sau cho vai trò IAM của bạn:\nChính sách quản trị của AmazonEKS Chính sách quản trị AmazonEKSCluster Xác nhận quyền truy cập bằng nút niêm yết\nChạy lệnh sau để kiểm tra xem các nút worker có hiển thị hay không: Nếu lệnh trả về danh sách các nút thì cluster của bạn đã được cấu hình thành công. Lưu ý: Nếu không có nút nào xuất hiện, hãy đợi vài phút và thử lại vì quá trình cung cấp nút có thể vẫn đang được tiến hành. Các bước tiếp theo: Khám phá các mô-đun hội thảo Sau khi kết nối thành công với cụm EKS của mình, bạn đã sẵn sàng tiếp tục với bất kỳ mô-đun hội thảo nào. Chọn mô-đun phù hợp nhất với mục tiêu học tập của bạn và bắt đầu trải nghiệm thực tế với các giải pháp AWS, Kubernetes và hỗ trợ AI.\n"
},
{
	"uri": "//localhost:1313/vi/2-prerequiste/2.2-createiamrole/",
	"title": "Sử dụng tài khoản của riêng bạn",
	"tags": [],
	"description": "",
	"content": "Hướng dẫn thiết lập môi trường cho phòng thí nghiệm AWS Nếu bạn KHÔNG thực hiện các phòng thí nghiệm này trong một sự kiện do AWS tổ chức trên Workshop Studio, bạn cần chuẩn bị môi trường trước khi bắt đầu. Lưu ý rằng các tài nguyên này sẽ phát sinh chi phí, vì vậy hãy nhớ dọn dẹp sau khi hoàn thành.\nBước 1: Cấu hình môi trường Thiết lập tài khoản Thực hiện các bước sau để cấu hình tài khoản của bạn:\nNhấn vào Launch Stack để khởi chạy CloudFormation với các giá trị được cấu hình sẵn trong khu vực us-east-1.\nLaunch Stack\nNếu muốn chạy workshop ở khu vực khác, hãy thay đổi khu vực phù hợp. Stack sẽ thiết lập các thành phần sau: Cụm EKS (Amazon Elastic Kubernetes Service):\n-Tạo một cụm EKS mới có tên opea-eks-cluster. -Triển khai một node trong cụm sử dụng M7i.24xlarge.\nCác mẫu CloudFormation: Cung cấp các mẫu CloudFormation để triển khai các mô-đun sau: Module 1:ChatQnA Default\nModule 2:ChatQnA with Guardrails\nModule 3:ChatQnA with OpenSearch (một cơ sở dữ liệu vector mã nguồn mở)\nModule 4:ChatQnA with Remote Inference (Denvr) làm mô hình LLM\nModule 5:ChatQnA with Bedrock làm mô hình LLM\nCấu hình trước khi chạy Stack Trước khi chạy stack, trên trang Quick create stack, thực hiện các bước sau:\nHuggingFaceToken:Cung cấp mã token để tải xuống mô hình từ Hugging Face. Nếu sử dụng tính năng Guardrails, đảm bảo token có quyền truy cập vào mô hình meta-llama/Meta-Llama-Guard-2-8B.\nModelID:OPEA sử dụng Text Generation Inference toolkit. Chọn mô hình từ danh sách các mô hình được hỗ trợ trên Hugging Face và nhập Model ID.\nOpeaRoleArn:Nhập ARN hoặc tên của role mà bạn đang sử dụng.\nNếu không chắc, hãy kiểm tra thông tin người dùng ở góc trên bên phải màn hình. Nếu tên không có dấu /, sao chép toàn bộ. Nếu có dấu /, chỉ lấy phần trước dấu /. Ví dụ: Nếu thấy \u0026ldquo;USER\u0026rdquo;, nhập \u0026ldquo;USER\u0026rdquo;. Nếu thấy \u0026ldquo;ADMIN-ROLE/USER\u0026rdquo;, chỉ nhập \u0026ldquo;ADMIN-ROLE\u0026rdquo;. Lấy ARN bằng lệnh AWS CLI (nếu cần). aws sts get-caller-identity \u0026ndash;query \u0026lsquo;Arn\u0026rsquo; \u0026ndash;output text | awk -F: \u0026lsquo;{print $NF}\u0026rsquo; | (read id; if [[ $id == \u0026ldquo;user\u0026rdquo; ]]; then aws sts get-caller-identity \u0026ndash;query \u0026lsquo;Arn\u0026rsquo; \u0026ndash;output text; else role=$(echo $id | cut -d\u0026rsquo;/\u0026rsquo; -f2); aws iam get-role \u0026ndash;role-name $role \u0026ndash;query \u0026lsquo;Role.Arn\u0026rsquo; \u0026ndash;output text; fi)\nĐánh dấu vào hộp kiểm bên cạnh Tôi xác nhận rằng AWS CloudFormation có thể tạo tài nguyên IAM\nQuá trình triển khai Sau khi thiết lập các tham số, nhấp vào Tạo ngăn xếp.\nQuá trình này sẽ khởi tạo Dự án AWS CodeBuild, dự án này sẽ đưa thư viện nguồn mở opea-demo-builder vào\nBộ công cụ phát triển đám mây AWS (CDK) sẽ tạo các mẫu CloudFormation cần thiết để định cấu hình môi trường AWS của bạn cho hội thảo.\nGiám sát tiến độ triển khai Quá trình triển khai mất khoảng 25 phút. Mở Bảng điều khiển AWS CloudFormation và theo dõi tiến trình.\nĐảm bảo rằng các ngăn xếp sau đạt trạng thái CREATE_COMPLETE:\nCụm EKS Ứng dụng ChatQnA (triển khai mặc định) Một số ngăn xếp có thể vẫn còn trong REVIEW_IN_PROGRESS vì chúng sẽ được triển khai trên cụm EKS sau này.\nBước 2: Định cấu hình quyền truy cập vào cụm EKS của bạn Sau khi triển khai thành công các ngăn xếp CloudFormation, bạn cần đặt cấu hình môi trường cục bộ của mình để tương tác với cụm Amazon EKS bằng kubectl.\nCập nhật cấu hình Kubernetes của bạn (kubeconfig)\nMở AWS CloudShell Bấm vào biểu tượng CloudShell trên Bảng điều khiển quản lý AWS. Ngoài ra, hãy sử dụng AWS CLI cục bộ của bạn, đảm bảo kubectl và máy khách AWS CLI được cài đặt trên hệ thống của bạn. Cập nhật kubeconfig Chạy lệnh sau (cập nhật vùng nếu cần): Nếu thành công, bạn sẽ nhận được thông báo xác nhận cho biết file cấu hình của bạn đã được cập nhật. Xác minh kết nối với cụm EKS của bạn Đảm bảo bạn có thể tương tác với cụm bằng kubectl. Bước 3: Xác minh quyền truy cập cụm EKS Sau khi cập nhật kubeconfig, hãy kiểm tra xem bạn có thể kết nối với cụm không. Nếu bạn gặp sự cố khi truy cập nhóm thông qua bảng điều khiển AWS hoặc CloudShell, hãy làm theo các bước sau:\nKiểm tra quyền truy cập IAM của bạn trong Bảng điều khiển EKS\nĐiều hướng đến Bảng điều khiển EKS. Tìm ARN chính IAM của bạn trong Mục truy cập. Nếu ARN của bạn không được liệt kê, hãy thêm nó theo cách thủ công\nBấm vào Tạo mục truy cập. Nhập ARN người dùng IAM hoặc ARN vai trò của bạn. Đính kèm các chính sách bắt buộc Gán các quyền sau cho vai trò IAM của bạn:\nChính sách quản trị của AmazonEKS Chính sách quản trị AmazonEKSCluster Xác nhận quyền truy cập bằng nút niêm yết\nChạy lệnh sau để kiểm tra xem các nút worker có hiển thị hay không: Nếu lệnh trả về danh sách các nút thì cluster của bạn đã được cấu hình thành công. Lưu ý: Nếu không có nút nào xuất hiện, hãy đợi vài phút và thử lại vì quá trình cung cấp nút có thể vẫn đang được tiến hành. Các bước tiếp theo: Khám phá các mô-đun hội thảo Sau khi kết nối thành công với cụm EKS của mình, bạn đã sẵn sàng tiếp tục với bất kỳ mô-đun hội thảo nào. Chọn mô-đun phù hợp nhất với mục tiêu học tập của bạn và bắt đầu trải nghiệm thực tế với các giải pháp AWS, Kubernetes và hỗ trợ AI.\n"
},
{
	"uri": "//localhost:1313/vi/4-s3log/4.2-creates3bucket/",
	"title": "Xác minh OPEA Chat QnA với Inferenece API",
	"tags": [],
	"description": "",
	"content": "Ứng dụng thử nghiệm\nBạn có thể kiểm tra việc triển khai bằng cách truy cập vào url DNS của bộ cân bằng tải mà mẫu hình thành đám mây đã tạo.\nTìm bộ cân bằng tải: Sao chép tên DNS của bạn cho bedrock-ingress: Dán vào tab trình duyệt mới để truy cập vào giao diện Trong giao diện người dùng, bạn có thể thấy chatbot để tương tác với nó\nKiểm tra xem mô hình có thể cung cấp cho chúng ta câu trả lời về OPEA hay không: Bạn có thể nhận thấy rằng phản hồi ban đầu của chatbot đã lỗi thời hoặc thiếu thông tin chi tiết cụ thể về OPEA. Điều này là do OPEA là một dự án tương đối mới và không được đưa vào tập dữ liệu được sử dụng để đào tạo mô hình ngôn ngữ. Vì hầu hết các LLM (Mô hình ngôn ngữ lớn) đều tĩnh nên chúng chỉ dựa vào dữ liệu đào tạo có sẵn và không thể tự động kết hợp các phát triển mới hoặc công nghệ mới nổi như OPEA.\nTải lên ngữ cảnh để cải thiện độ chính xác Để giải quyết hạn chế này, RAG (Thế hệ tăng cường truy xuất) cho phép truy xuất ngữ cảnh theo thời gian thực. Giao diện người dùng ChatQnA bao gồm một biểu tượng tải lên, cho phép bạn thêm ngữ cảnh có liên quan.\nCách thức hoạt động:\nKhi bạn tải lên một tài liệu hoặc liên kết, nó sẽ được gửi đến dịch vụ vi mô DataPrep.\nDataPrep xử lý nội dung và tạo nhúng.\nSau đó, dữ liệu đã xử lý được lưu trữ trong Cơ sở dữ liệu Vector để truy xuất.\nBằng cách tải lên các tài liệu hoặc liên kết đã cập nhật, bạn mở rộng cơ sở kiến ​​thức của chatbot, đảm bảo chatbot cung cấp các phản hồi có liên quan, chính xác và cập nhật hơn.\nViệc triển khai cho phép bạn tải lên tệp hoặc trang web. Đối với trường hợp này, hãy sử dụng trang web OPEA:\nNhấp vào biểu tượng tải lên để mở bảng điều khiển bên phải\nNhấp vào Dán liên kết\nSao chép/dán văn bản https://opea-project.github.io/latest/introduction/index.html vào hộp nhập\nNhấp vào Xác nhận để bắt đầu quá trình lập chỉ mục Khi quá trình lập chỉ mục hoàn tất, bạn sẽ thấy một biểu tượng được thêm vào bên dưới hộp văn bản, có nhãn là https://opea-project.github.io/latest/introduction/index.html\nHỏi ứng dụng sau khi cung cấp ngữ cảnh: Hỏi lại \u0026ldquo;OPEA là gì?\u0026rdquo; để xem câu trả lời đã cập nhật.\nLần này, bot trò chuyện phản hồi chính xác dựa trên dữ liệu mà nó thêm vào lời nhắc từ nguồn mới, trang web OPEA.\nKết luận\nTrong nhiệm vụ này, bạn đã triển khai thành công một chatbot hỗ trợ RAG bằng Amazon Bedrock. Bằng cách tải lên ngữ cảnh có liên quan, bạn đã cho phép mô hình cập nhật và tinh chỉnh phản hồi của mình một cách động dựa trên thông tin mới. Quy trình này chứng minh cách tích hợp RAG tăng cường khả năng thích ứng theo thời gian thực, cho phép hệ thống liên tục cải thiện độ chính xác và tính liên quan của mình trong khi tận dụng sức mạnh của Amazon Bedrock.\n"
},
{
	"uri": "//localhost:1313/vi/4-s3log/4.3-creategwes3-copy/",
	"title": "Khám phá OpenSearch (Tùy chọn)",
	"tags": [],
	"description": "",
	"content": "Khám phá RAG và tương tác với UI\nBây giờ tất cả các dịch vụ đã hoạt động, hãy cùng khám phá UI do triển khai cung cấp.\nĐể truy cập Giao diện người dùng ChatQnA Bedrock, hãy mở trình duyệt web và điều hướng đến DNS của ChatQnA Bedrock Load Balancer:\n👉 http://bedrock-ingress-xxxxxxx.us-east-2.elb.amazonaws.com (Thay thế bằng URL DNS Bedrock Ingress thực tế của bạn).\nKhi đã vào bên trong UI, bạn có thể tương tác với chatbot, kiểm tra phản hồi của chatbot và trải nghiệm cách chatbot xử lý các truy vấn bằng cách sử dụng chức năng truy xuất do RAG cung cấp.\nBây giờ khi bạn gửi lời nhắc đến chatbot, phản hồi sẽ đến từ Claude Haiku của Anthropic thông qua Amazon Bedrock.\n"
},
{
	"uri": "//localhost:1313/vi/4-s3log/4.3-creategwes3/",
	"title": "Khám phá OpenSearch (Tùy chọn)",
	"tags": [],
	"description": "",
	"content": "Khám phá RAG và tương tác với UI\nBây giờ tất cả các dịch vụ đã hoạt động, hãy cùng khám phá UI do triển khai cung cấp.\nĐể truy cập Giao diện người dùng ChatQnA Bedrock, hãy mở trình duyệt web và điều hướng đến DNS của ChatQnA Bedrock Load Balancer:\n👉 http://bedrock-ingress-xxxxxxx.us-east-2.elb.amazonaws.com (Thay thế bằng URL DNS Bedrock Ingress thực tế của bạn).\nKhi đã vào bên trong UI, bạn có thể tương tác với chatbot, kiểm tra phản hồi của chatbot và trải nghiệm cách chatbot xử lý các truy vấn bằng cách sử dụng chức năng truy xuất do RAG cung cấp.\nBây giờ khi bạn gửi lời nhắc đến chatbot, phản hồi sẽ đến từ Claude Haiku của Anthropic thông qua Amazon Bedrock.\n"
},
{
	"uri": "//localhost:1313/vi/3-accessibilitytoinstances/",
	"title": "Tìm hiểu ứng dụng ChatQnA RAG sử dụng OPEA trên EKS",
	"tags": [],
	"description": "",
	"content": "Nhiệm vụ này giới thiệu cho các nhà phát triển về các thành phần OPEA được triển khai trên cụm Amazon EKS, tận dụng cấu hình có sẵn trên AWS Marketplace. Người tham gia sẽ có quyền truy cập vào mã nguồn, cho phép họ khám phá cách các dịch vụ khác nhau vận hành và tương tác trong hệ thống.\nTrọng tâm của bài thực hành này là hiểu rõ kiến trúc Retrieval-Augmented Generation (RAG). Các nhà phát triển sẽ học cách truy vấn trang web hoặc tệp PDF để thu thập và trả lời thông tin một cách chính xác, phù hợp với ngữ cảnh. Thông qua việc tương tác với các thành phần này, họ sẽ có cơ hội trải nghiệm thực tế cách RAG kết hợp cơ chế truy xuất thông tin với mô hình sinh để nâng cao độ chính xác và tính phù hợp của dữ liệu được tạo ra bằng OPEA.\nMục tiêu học tập: Làm quen với các thành phần OPEA chạy trên Amazon EKS. Hiểu sâu về kiến trúc RAG, đặc biệt trong bối cảnh truy xuất thông tin từ tài liệu. Tương tác với ứng dụng ChatQnA để truy vấn và thu thập dữ liệu, củng cố kiến thức về khả năng tạo câu trả lời chính xác theo ngữ cảnh của RAG.\nNội dung 3.1. Triển khai ChatQnA\n3.2. Khám phá triển khai OPEA ChatQnA\n3.3. Kiểm tra việc triển khai và xác minh quy trình làm việc RAG\n"
},
{
	"uri": "//localhost:1313/vi/4-s3log/",
	"title": "Kiểm tra việc triển khai",
	"tags": [],
	"description": "",
	"content": "Nhiệm vụ này là một đóng góp quan trọng cho OPEA, chứng minh cách tích hợp AWS Bedrock dưới dạng LLM (Mô hình ngôn ngữ lớn) có thể cung cấp giải pháp thay thế không cần máy chủ cho ChatQnA. Trọng tâm là thể hiện khả năng của OPEA trong việc tích hợp liền mạch các LLM khác nhau, cung cấp kinh nghiệm thực tế trong việc tùy chỉnh các thiết lập RAG (Thế hệ tăng cường truy xuất) để hoạt động với các giải pháp gốc trên nền tảng đám mây như AWS Bedrock, đảm bảo khả năng mở rộng và thích ứng.\nMục tiêu học tập\nKhám phá tính linh hoạt của OPEA trong việc tích hợp LLM Hiểu cách kiến ​​trúc mô-đun của OPEA cho phép tích hợp liền mạch nhiều LLM khác nhau, bao gồm cả AWS Bedrock. Triển khai AWS Bedrock dưới dạng LLM trong OPEA Có được kinh nghiệm thực tế trong việc thay thế LLM mặc định bằng AWS Bedrock và sửa đổi đường ống RAG để tận dụng các mô hình của nó. Tối ưu hóa RAG Pipelines để có khả năng mở rộng và thích ứng Tìm hiểu cách tận dụng tính linh hoạt của OPEA để tích hợp và tùy chỉnh LLM, đảm bảo các giải pháp AI cấp doanh nghiệp có khả năng mở rộng và thích ứng. Điểm chính Phòng thí nghiệm này nêu bật khả năng tích hợp AWS Bedrock của OPEA, củng cố tính linh hoạt của công ty trong việc thích ứng với nhiều công nghệ khác nhau và các trường hợp sử dụng doanh nghiệp thực tế cho các giải pháp do AI thúc đẩy.\n"
},
{
	"uri": "//localhost:1313/vi/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "//localhost:1313/vi/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]