[
{
	"uri": "http://<user_name>.github.io/1-introduce/2.1-createec2/",
	"title": "Đội ngũ giáo viên",
	"tags": [],
	"description": "",
	"content": "1.Đặng Văn Minh – Gia sư Toán, Văn, Anh Thành tích học tập:\nSinh viên năm cuối Đại học Tài chính – Marketing (UFM), ngành Hệ thống thông tin quản lý, chuyên ngành Tin học quản lý.\nHọc sinh giỏi/xuất sắc 12 năm liền.\nCựu học sinh THPT Nguyễn Hữu Huân – một trong những trường top đầu.\nĐiểm tuyển sinh lớp 10 năm 2019: 39 điểm.\nThành viên lớp chọn Tiếng Anh Tăng Cường trường THCS Hoa Lư 4 năm liên tiếp.\nThành viên đội tuyển học sinh giỏi Văn cấp quận lớp 6, 7.\nChứng chỉ TOEIC 800\nĐảng viên Đảng Cộng sản Việt Nam\nKinh nghiệm giảng dạy \u0026amp; làm việc:\nHơn 1 năm kinh nghiệm gia sư, giảng dạy Toán, Văn, Anh cho học sinh cấp 2.\nThực tập sinh tại Amazon Web Services Việt Nam (AWS), có kiến thức về ứng dụng công nghệ trong giáo dục.\nPhương pháp giảng dạy:\nKết hợp Toán, Văn, Anh để phát triển tư duy toàn diện.\nRèn luyện tư duy phản biện, kỹ năng giải quyết vấn đề và phương pháp học tập hiệu quả.\nỨng dụng công nghệ vào giảng dạy để nâng cao hiệu suất học tập.\n2. Đỗ Trọng Nhân – Giáo viên Tiếng Anh nâng cao (Hiệu Trưởng) Thành tích học tập:\nSinh viên ngành Ngôn ngữ Anh – Đại học Khoa học Xã hội \u0026amp; Nhân văn.\nĐiểm tuyển sinh lớp 10 41,5 điểm (THPT Nguyễn Hữu Huân).\nIELTS 7.5.\nThành viên đội tuyển Tiếng Anh cấp quận.\nHọc sinh giỏi/xuất sắc 12 năm liền\nKinh nghiệm giảng dạy \u0026amp; làm việc:\n3 năm kinh nghiệm gia sư tiếng Anh, từ cơ bản đến nâng cao. Phương pháp giảng dạy:\nHướng dẫn luyện thi chứng chỉ quốc tế và kỹ năng giao tiếp.\nSử dụng phương pháp phản xạ giúp học sinh tăng khả năng sử dụng tiếng Anh tự nhiên.\n3.Vũ Quang Long – Gia sư Tiếng Anh (Phó Hiệu Trưởng) Thành tích học tập:\nSinh viên ngành Công nghệ đa phương tiện – Học viện Bưu chính Viễn thông.\nIELTS 6.0.\nGiải khuyến khích Tiếng Anh cấp quận lớp 6, 7.\nĐiểm tuyển sinh lớp 10 năm 2019: 38 điểm (THPT Nguyễn Hữu Huân).\nKinh nghiệm giảng dạy \u0026amp; làm việc:\nGia sư tiếng Anh cho học sinh cấp 2, tập trung vào ngữ pháp và giao tiếp cơ bản. Phương pháp giảng dạy:\nHướng dẫn học sinh cải thiện từ vựng và ngữ pháp bằng phương pháp thực hành. 4.Nguyễn Khoa Nam An – Gia sư Toán, Văn, Tiếng Anh Thành tích học tập:\nHọc sinh giỏi/xuất sắc 12 năm liền\nSinh viên ngành Thương mại điện tử – Đại học Sư phạm Kỹ thuật.\nIELTS 6.5.\nĐiểm tuyển sinh lớp 10 41 điểm (THPT Nguyễn Hữu Huân).\nKinh nghiệm giảng dạy \u0026amp; làm việc:\nThực tập sinh tại Amazon Web Services Việt Nam (AWS), có nền tảng công nghệ vững chắc. Phương pháp giảng dạy:\nHướng dẫn học sinh cách sử dụng tiếng Anh trong môi trường thực tế. 5.Nguyễn Tấn Lợi – Giáo viên Ngôn ngữ Tây Ban Nha \u0026amp; Tiếng Anh Thành tích học tập:\nSinh viên ngành Ngôn ngữ Tây Ban Nha – Đại học Khoa học Xã hội \u0026amp; Nhân văn.\nIELTS 6.5.\nĐiểm tuyển sinh lớp 10 40 điểm (THPT Nguyễn Hữu Huân).\nKinh nghiệm giảng dạy \u0026amp; làm việc:\n3 năm kinh nghiệm gia sư tiếng Anh và Tây Ban Nha. Phương pháp giảng dạy:\nKết hợp giảng dạy ngôn ngữ với văn hóa giúp học sinh tiếp cận ngôn ngữ dễ dàng hơn.\nSử dụng phương pháp thực hành giúp học sinh cải thiện nhanh chóng.\n6.Thái Thanh Kha – Gia Sư Toán, Lý, Hóa Thành tích học tập:\nCựu sinh viên Đại học Tôn Đức Thắng.\nSinh viên ngành Hệ thống thông tin quản lý – Đại học Tài chính Marketing.\nĐiểm thi đại học 25 điểm.\nKinh nghiệm giảng dạy \u0026amp; làm việc:\nGia sư Toán, Lý Hóa cho học sinh cấp 2, tập trung vào ngữ pháp và giao tiếp cơ bản. Phương pháp giảng dạy:\nKết hợp Toán, Văn, Anh để phát triển tư duy toàn diện.\nRèn luyện tư duy phản biện, kỹ năng giải quyết vấn đề và phương pháp học tập hiệu quả.\n7.Nguyễn Đức Duy – Giáo viên Tiếng Anh (Người thừa) Thành tích học tập:\nSinh viên ngành Quản trị kinh doanh – Đại học Nông Lâm.\nGiải khuyến khích Tiếng Anh cấp quận lớp 6, 7.\nKinh nghiệm giảng dạy \u0026amp; làm việc:\nHơn 2 năm kinh nghiệm gia sư tiếng Anh, đặc biệt là hỗ trợ học sinh mất gốc. Phương pháp giảng dạy:\nTập trung vào kỹ năng đọc-hiểu và luyện thi hiệu quả.\nSử dụng phương pháp thực hành giúp học sinh cải thiện giao tiếp.\n"
},
{
	"uri": "http://<user_name>.github.io/1-introduce/",
	"title": "Giới thiệu về chúng tôi",
	"tags": [],
	"description": "",
	"content": "1. Giới thiệu chung Minh Vinh Education là một tổ chức giáo dục chuyên cung cấp các chương trình đào tạo chất lượng cao, giúp học sinh phát triển toàn diện về kỹ năng, tư duy và kiến thức chuyên môn. Với đội ngũ giảng viên tận tâm, phương pháp giảng dạy tiên tiến và hệ thống học tập linh hoạt, chúng tôi cam kết mang lại giá trị tốt nhất cho học viên.\n2. Những gì chúng tôi mang đến 🌟 Sứ Mệnh\nCung cấp môi trường học tập hiện đại, sáng tạo và hiệu quả.\nHỗ trợ học viên phát triển năng lực toàn diện, ứng dụng thực tế.\nThúc đẩy sự đổi mới trong giáo dục bằng công nghệ và phương pháp giảng dạy tiên tiến.\n🚀 Tầm Nhìn\nTrở thành đơn vị hàng đầu trong lĩnh vực giáo dục tại Việt Nam và vươn ra thị trường quốc tế.\nXây dựng cộng đồng học tập chất lượng, kết nối học viên, giảng viên và doanh nghiệp.\nỨng dụng công nghệ để nâng cao trải nghiệm giáo dục.\n🏆 Giá Trị Cốt Lõi\n✅ Chất lượng – Đặt tiêu chuẩn giảng dạy cao nhất cho từng khóa học.\n✅ Sáng tạo – Luôn đổi mới phương pháp dạy và học.\n✅ Cộng đồng – Xây dựng môi trường học tập thân thiện, kết nối và hỗ trợ lẫn nhau.\n✅ Ứng dụng thực tế – Kiến thức đi đôi với thực hành, giúp học viên áp dụng ngay vào công việc.\n3. Chương Trình Đào Tạo 📌 Đối tượng: Học sinh cấp 2 (lớp 6 – lớp 9)\n📌 Mục tiêu:\n✅ Củng cố nền tảng kiến thức theo chương trình SGK\n✅ Phát triển tư duy logic (Toán), tư duy ngôn ngữ (Văn, Anh)\n✅ Rèn luyện kỹ năng làm bài thi hiệu quả\n✅ Tạo động lực học tập và phát triển kỹ năng tự học\n1️⃣ Môn Toán – Rèn luyện tư duy logic và giải quyết vấn đề\n🔹 Lớp 6 – 7: Số học, phân số, hình học cơ bản, đại số sơ cấp\n🔹 Lớp 8 – 9: Phương trình, bất phương trình, hình học không gian, đại số nâng cao\n🔹 Phương pháp: Học lý thuyết – bài tập thực hành – bài kiểm tra định kỳ\n2️⃣ Môn Văn – Phát triển kỹ năng đọc hiểu và diễn đạt\n🔹 Lớp 6 – 7: Tập làm văn (tả, kể, biểu cảm), đọc hiểu, ngữ pháp cơ bản\n🔹 Lớp 8 – 9: Văn nghị luận, đọc hiểu nâng cao, luyện thi vào lớp 10\n🔹 Phương pháp: Phân tích tác phẩm – luyện viết bài – thảo luận nhóm\n3️⃣ Môn Tiếng Anh – Giao tiếp và ngữ pháp vững chắc\n🔹 Lớp 6 – 7: Từ vựng theo chủ đề, ngữ pháp nền tảng, luyện nghe nói cơ bản\n🔹 Lớp 8 – 9: Ngữ pháp nâng cao, viết luận, luyện thi vào lớp 10\n🔹 Phương pháp: Học qua hội thoại – bài tập tình huống – trò chơi tương tác\n📅 Lịch học gợi ý:\n2 buổi/tuần/môn hoặc 3 môn kết hợp trong 1 tuần\nKết hợp học offline \u0026amp; online tùy nhu cầu học sinh\nLuyện đề kiểm tra định kỳ theo lộ trình\n🎯 Cam kết:\n✔️ Phương pháp dạy dễ hiểu, khoa học\n✔️ Học sinh tiến bộ rõ rệt qua từng giai đoạn\n✔️ Giáo viên tận tâm, hỗ trợ học sinh vượt qua khó khăn\n📞 Liên Hệ Ngay để nhận tư vấn chi tiết và bắt đầu hành trình học tập cùng chúng tôi!\n"
},
{
	"uri": "http://<user_name>.github.io/2-prerequiste/2.1-createec2/",
	"title": "Lớp 6",
	"tags": [],
	"description": "",
	"content": "If you\u0026rsquo;re participating in these labs as part of an AWS-led event in Workshop Studio, there\u0026rsquo;s no need to manually provision any resources—everything required for all modules will be pre-provisioned for you.\nWhat will be set up in your AWS account? The stack will automatically configure the following components:\nEKS Cluster: Creates a new EKS cluster named opea-eks-cluster. Deploys a node within the cluster using a M7i.24xlarge instance for high-performance computing. CloudFormation Templates: The stack also generates CloudFormation templates for each module:\nModule 1: ChatQnA Default\nModule 2: ChatQnA with Guardrails\nModule 3: ChatQnA with OpenSearch (open-source) as the vector database\nModule 4: ChatQnA with Bedrock as the LLM\nModule 5: ChatQnA with Remote Inference (Denvr) as the LLM\nWith these pre-configured resources, you can focus entirely on exploring and building your Generative AI applications without worrying about infrastructure setup.\nStep 1: Configure Access to Your EKS Cluster To interact with your EKS cluster using kubectl, you need to configure your local environment to recognize the cluster. This is done by updating the kubeconfig file, which stores authentication details and access configurations for your Kubernetes cluster.\nLog in to the AWS Management Console: Start by signing into your AWS Management Console. Open Cloud Shell or Set Up Your Local Environment: In the console, click the Cloud Shell icon to launch a preconfigured terminal. Alternatively, if you prefer to use your own AWS CLI, ensure you have both the AWS CLI Client and kubectl installed on your local machine.\nUpdate Your kubeconfig: You should receive an output confirming your conf file was updated: You are now ready to interact with the Kubernetes cluster using kubectl Step 2: Verify Cluster Access After updating your kubeconfig, verify that you can successfully connect to the cluster by listing the nodes: If the command executes successfully, you should see an output displaying the nodes associated with your cluster. You are now ready to explore the module of your choice and begin deploying workloads on your EKS cluster! "
},
{
	"uri": "http://<user_name>.github.io/3-accessibilitytoinstances/3.1-public-instance/",
	"title": "Lớp 6",
	"tags": [],
	"description": "",
	"content": "Introduction In this section, you will deploy the OPEA blueprint for a RAG-based application, ChatQnA, on an Amazon Elastic Kubernetes Service (EKS) environment. This hands-on exploration will enhance your understanding of how the RAG application functions within a managed Kubernetes ecosystem, allowing you to analyze its components and their roles in the system.\nThe deployment configuration is available through the AWS Marketplace, and ChatQnA can be found in OPEA\u0026rsquo;s GenAIExamples repository.\nChatQnA on GitHub\nSince you have already set up access to your Kubernetes cluster in the \u0026ldquo;Getting Set Up\u0026rdquo; section, you will now dive deeper into your deployed environment to explore and learn more about its structure and functionality.\nStep 1: Deploy the ChatQnA CloudFormation Template Open AWS CloudShell and deploy the ChatQnA CloudFormation template into your Amazon EKS cluster. This will initiate the deployment of the RAG application within your managed Kubernetes environment. Step 2: Explore Cluster Resources Navigate to the AWS Management Console and select your assigned EKS cluster to review its deployment. Each cluster includes critical configurations such as the Kubernetes version, networking setup, and logging options. Examining these settings will provide a deeper understanding of your application’s infrastructure, helping with efficient management and troubleshooting when necessary.\nReviewing Cluster Resources Click on the Resources tab to view all applications currently running within your cluster, including ChatQnA and its associated microservices. Ensure that all microservices from the OPEA ChatQnA blueprint are installed correctly by listing the active pods: The output should show all pods in a Running state (1/1), confirming that the application has been successfully deployed. At this point, you are ready to further explore the deployment and manage your resources within the cluster. "
},
{
	"uri": "http://<user_name>.github.io/4-s3log/4.1-updateiamrole/",
	"title": "Lớp 6",
	"tags": [],
	"description": "",
	"content": "What is Amazon Bedrock? Amazon Bedrock is a fully managed service that provides access to a diverse selection of high-performance foundation models (FMs) from industry leaders such as AI21 Labs, Anthropic, Cohere, Meta, Mistral AI, Stability AI, and Amazon itself. Through a single API, Bedrock enables developers to build generative AI applications while ensuring security, privacy, and responsible AI practices.\nDevelopers can interact with Amazon Bedrock via the AWS Software Development Kit (SDK) or the AWS Command Line Interface (CLI). Bedrock also offers native features that allow users to create RAG (Retrieval-Augmented Generation) knowledge bases, agentic workflows, and guardrails. Integrating Bedrock with OPEA expands access to a broader selection of foundation models while leveraging Bedrock’s advanced capabilities alongside OPEA.\nHow Does the Architecture Change?\nFuture integrations with OPEA will unlock the full potential of Amazon Bedrock’s capabilities, including Titan Embedding models. However, for this module, the focus is exclusively on LLMs.\nThanks to OPEA’s modular and interchangeable architecture, most components from the default ChatQnA setup (Module 1) remain unchanged. The TGI service (Hugging Face) is now replaced by a Bedrock container, which seamlessly integrates into the existing deployment.\nUpdated Architecture Component:\nchatqna-bedrock In this deployment, when a user sends a message through the ChatQnA UI, it is routed to the backend Bedrock container, which communicates with Amazon Bedrock to retrieve and return responses. This integration maintains the ChatQnA architecture while enhancing it with Amazon Bedrock’s powerful LLM capabilities, ensuring scalability, efficiency, and seamless deployment.\nWe\u0026rsquo;ve used the bedrock Kubernetes namespace to separate out the pods and services pertaining to the Bedrock deployment. When you use kubectl and other Kubernetes commands in the below examples, be sure to qualify the command with -n bedrock.\nDeploying ChatQnA Using Amazon Bedrock LLMs\nFor this lab, we\u0026rsquo;ve created a changeset with the full parallel deployment of the ChatQnA example in the same Kubernetes cluster you\u0026rsquo;ve been using. The following command will deploy pods to the cluster within the \u0026ldquo;bedrock\u0026rdquo; namespace that are identical to the original ChatQnA pods, except with Bedrock models instead of TGI.\naws cloudformation execute-change-set \u0026ndash;change-set-name bedrock-change-set \u0026ndash;stack-name OpeaBedrockStack\nActivating the Model\nThis module works with just about any text-generation LLM supported by Bedrock, but for the purposes of this lab we\u0026rsquo;ve used the Anthropic Claude Haiku 3 model. So while you\u0026rsquo;re waiting for the change set to deploy, let\u0026rsquo;s go activate our model in the Bedrock console:\nSwitch to the us-west-2 region, you could test on other regions but usually us-west has more availabity: Go to Amazon Bedrock: Go to the model access tab: At the top of the screen, click on the button that says Modify Model Access Select Claude 3 Haiku It may take a minute or two for the access to be granted, but don\u0026rsquo;t worry it won\u0026rsquo;t take much longer than that.\nOnce you\u0026rsquo;ve confirmed that model access has been granted, switch back to the us-east-2 region where your EKS cluster is located. Confirming Deployment\nNow let\u0026rsquo;s confirm that our Bedrock deployment is complete. You can onitor the state of the Bedrock pods using the kubectl command:\n\u0026hellip;to get output like this:\nIt can take several minutes for Bedrock to fully initialize and be available. Only continue when you see the chatqna-bedrock-deployment pod in the Running state.\nYou are now able to use Amazon Bedrock in your environment.\n"
},
{
	"uri": "http://<user_name>.github.io/",
	"title": "Minh Vinh Education",
	"tags": [],
	"description": "",
	"content": "Học, Học Nữa, Học Mãi "
},
{
	"uri": "http://<user_name>.github.io/2-prerequiste/",
	"title": "Bài Giảng Toán ",
	"tags": [],
	"description": "",
	"content": "Lời nói đầu Toán học là nền tảng của tư duy logic, là chìa khóa mở ra khả năng giải quyết vấn đề một cách sáng tạo và hiệu quả. Không chỉ là một môn học trên lớp, Toán còn là công cụ giúp con người hiểu sâu hơn về thế giới xung quanh, từ những phép tính đơn giản trong cuộc sống hằng ngày đến những ứng dụng phức tạp trong khoa học, kỹ thuật và kinh tế.\nỞ bậc trung học cơ sở, Toán học đóng vai trò quan trọng trong việc hình thành tư duy phân tích và suy luận của học sinh. Đây là giai đoạn giúp các em xây dựng nền móng vững chắc cho những bậc học cao hơn, đồng thời tạo cơ hội để rèn luyện sự nhạy bén trong cách tiếp cận và giải quyết vấn đề. Chương trình Toán trung học cơ sở không chỉ giúp học sinh tiếp thu những kiến thức cơ bản mà còn mở rộng đến những bài toán nâng cao, thử thách khả năng tư duy của các em.\nViệc học Toán không đơn thuần chỉ là ghi nhớ công thức và làm bài tập. Học sinh cần được hướng dẫn cách tư duy linh hoạt, biết cách phân tích bài toán, tìm ra mối liên hệ giữa các khái niệm và vận dụng chúng vào thực tế. Một phương pháp học tập hiệu quả sẽ không chỉ giúp học sinh đạt điểm cao mà còn phát triển khả năng suy nghĩ chặt chẽ, mạch lạc, ứng dụng kiến thức một cách sáng tạo vào nhiều lĩnh vực khác nhau.\nChương trình giảng dạy của trung tâm không chỉ tập trung vào việc giúp học sinh nắm vững kiến thức trong sách giáo khoa, mà còn hướng đến việc phát triển kỹ năng làm bài thi, tư duy phản biện và khả năng giải quyết các bài toán thực tế. Mỗi chuyên đề được thiết kế khoa học, phù hợp với từng cấp độ, đảm bảo học sinh không chỉ hiểu bài mà còn có thể vận dụng linh hoạt trong các tình huống khác nhau.\nBên cạnh đó, việc ôn tập và hệ thống hóa kiến thức một cách bài bản là yếu tố quan trọng giúp học sinh có sự chuẩn bị tốt nhất cho các kỳ thi quan trọng, đặc biệt là kỳ thi tuyển sinh vào lớp 10. Với phương pháp giảng dạy sáng tạo, đội ngũ giáo viên tận tâm và nguồn tài liệu phong phú, trung tâm cam kết giúp học sinh không chỉ chinh phục môn Toán một cách dễ dàng mà còn nuôi dưỡng niềm đam mê, hứng thú với bộ môn này.\nHọc Toán không phải là một hành trình đơn độc. Với sự đồng hành của thầy cô, trợ giảng và một lộ trình học tập rõ ràng, mỗi học sinh sẽ tìm thấy phương pháp học phù hợp với bản thân, phát huy tối đa tiềm năng và đạt được thành tích tốt nhất!\n🔹 Mục tiêu của chương trình:\n✔ Cung cấp kiến thức nền tảng vững chắc, giúp học sinh tự tin trong học tập và thi cử.\n✔ Phát triển tư duy logic và kỹ năng giải quyết vấn đề thông qua các bài tập đa dạng.\n✔ Hỗ trợ học sinh trong việc ôn luyện, định hướng phương pháp học tập hiệu quả và đạt điểm số cao trong các kỳ thi quan trọng.\n"
},
{
	"uri": "http://<user_name>.github.io/1-introduce/2.1-createec2-copy/",
	"title": "Đội ngũ trợ giảng",
	"tags": [],
	"description": "",
	"content": "1.Đặng Thái Vinh – Trợ giảng Toán Thành tích học tập:\n9 năm liền học sinh giỏi/xuất sắc.\n2 giải khuyến khích học sinh giỏi Toán TP. Thủ Đức lớp 6, 7.\nGiải Nhì học sinh giỏi Toán TP. Thủ Đức lớp 8.\nHuy chương Đồng cuộc thi Văn - Toán Tuổi Thơ toàn quốc.\nGiải khuyến khích Toán quốc tế SASMO.\nKinh nghiệm hỗ trợ giảng dạy:\nHỗ trợ học sinh giải bài tập khó, nâng cao tư duy Toán học.\nHướng dẫn phương pháp học tập và ôn luyện các kỳ thi Toán học sinh giỏi.\nPhương pháp trợ giảng:\nGiúp học sinh hiểu rõ bản chất bài toán thông qua cách tiếp cận logic.\nỨng dụng phương pháp tư duy hình ảnh và bài tập thực hành đa dạng để tăng khả năng tiếp thu.\n"
},
{
	"uri": "http://<user_name>.github.io/2-prerequiste/2.2-createiamrole/",
	"title": "Lớp 7",
	"tags": [],
	"description": "",
	"content": "Preparing Your AWS Environment for the Workshop If you are not participating in an AWS-led event via Workshop Studio, you must set up your environment before proceeding with these labs. Keep in mind that the resources you create will incur costs, so be sure to clean them up once you have completed the workshop.\nStep 1: Configuring Your Environment To set up your AWS account, follow these steps:\nClick Launch Stack below to initiate AWS CloudFormation with pre-configured values in the us-east-1 region.\nLaunch Stack\nIf you prefer a different AWS region, remember to change the region accordingly. The CloudFormation stack will provision the following resources:\nEKS Cluster Deployment\n-Creates a new EKS cluster named opea-eks-cluster.\n-Deploys a worker node within the cluster using an M7i.24xlarge instance.\nCloudFormation Templates for Workshop Modules The stack also includes templates for the following modules:\nModule 1: ChatQnA Default\nModule 2: ChatQnA with Guardrails\nModule 3: ChatQnA with OpenSearch (open-source) as the vector database\nModule 4: ChatQnA with Remote Inference (Denvr) as the LLM\nModule 5: ChatQnA with Bedrock as the LLM\nConfiguring Stack Parameters Before launching the stack, review and configure the following parameters:\nFor Modules 1, 2, and 3:\nHuggingFaceToken: This token is required to download models from Hugging Face. If you plan to use the Guardrails feature, ensure your token has access to the meta-llama/Meta-Llama-Guard-2-8B model.\nModelID: The OPEA system primarily uses the Text Generation Inference toolkit. Select any compatible model from Hugging Face and provide its model ID.\nOpeaRoleArn: Enter the ARN or name of the IAM role associated with your AWS account.\nIf you are unsure, check the user information displayed in the top-right corner of the AWS Management Console. If your username does not contain a forward slash (/), copy the full name. If it does contain a forward slash, only use the portion before the slash. Example:\nIf the display name is \u0026ldquo;USER\u0026rdquo;, enter \u0026ldquo;USER\u0026rdquo;. If the display name is \u0026ldquo;ADMIN-ROLE/USER\u0026rdquo;, enter \u0026ldquo;ADMIN-ROLE\u0026rdquo;. Alternatively, you can retrieve your ARN using the AWS CLI. aws sts get-caller-identity \u0026ndash;query \u0026lsquo;Arn\u0026rsquo; \u0026ndash;output text | awk -F: \u0026lsquo;{print $NF}\u0026rsquo; | (read id; if [[ $id == \u0026ldquo;user\u0026rdquo; ]]; then aws sts get-caller-identity \u0026ndash;query \u0026lsquo;Arn\u0026rsquo; \u0026ndash;output text; else role=$(echo $id | cut -d\u0026rsquo;/\u0026rsquo; -f2); aws iam get-role \u0026ndash;role-name $role \u0026ndash;query \u0026lsquo;Role.Arn\u0026rsquo; \u0026ndash;output text; fi)\nMark the checkbox next to I acknowledge that AWS CloudFormation might create IAM resources\nDeploying the CloudFormation Stack Once the parameters are set, click Create Stack.\nThis process will initiate an AWS CodeBuild Project, which pulls in the opea-demo-builder open-source library.\nThe AWS Cloud Development Kit (CDK) will generate the necessary CloudFormation templates to configure your AWS environment for the workshop.\nMonitoring Deployment Progress The deployment process takes approximately 25 minutes. Open the AWS CloudFormation Console and monitor the progress.\nEnsure that the following stacks reach the CREATE_COMPLETE status:\nThe EKS cluster The ChatQnA application (default deployment) Some stacks may remain in REVIEW_IN_PROGRESS, as they will be deployed onto the EKS cluster later.\nStep 2: Configuring Access to Your EKS Cluster Once the CloudFormation stacks are successfully deployed, you need to configure your local environment to interact with the Amazon EKS cluster using kubectl.\nUpdating Your Kubernetes Configuration (kubeconfig)\nOpen AWS CloudShell Click the CloudShell icon in the AWS Management Console. Alternatively, use your local AWS CLI, ensuring kubectl and the AWS CLI client are installed on your system. Update kubeconfig Run the following command (updating the region if necessary): If successful, you should receive a confirmation message indicating that your configuration file has been updated. Verify Connectivity to Your EKS Cluster Ensure you can interact with the cluster using kubectl. Step 3: Verifying EKS Cluster Access After updating your kubeconfig, test whether you can connect to the cluster. If you encounter issues accessing pods via the AWS console or CloudShell, follow these steps:\nCheck Your IAM Access in the EKS Console\nNavigate to the EKS Console. Look for your IAM principal ARN under Access Entries. If Your ARN Is Not Listed, Add It Manually\nClick Create Access Entry. Enter your IAM User ARN or Role ARN. Attach Required Policies Assign the following permissions to your IAM role:\nAmazonEKSAdminPolicy AmazonEKSClusterAdminPolicy Confirm Access by Listing Nodes\nRun the following command to check if worker nodes are visible: If the command returns a list of nodes, your cluster is successfully configured. Note: If no nodes appear, wait a few minutes and retry, as node provisioning may still be in progress. Next Steps: Explore the Workshop Modules Once you have successfully connected to your EKS cluster, you are ready to proceed with any of the workshop modules. Choose the module that best fits your learning objectives and begin your hands-on experience with AWS, Kubernetes, and AI-powered solutions.\n"
},
{
	"uri": "http://<user_name>.github.io/3-accessibilitytoinstances/3.2-private-instance/",
	"title": "Lớp 7",
	"tags": [],
	"description": "",
	"content": "Exploring the OPEA Microservices Deployment Now, let\u0026rsquo;s dive into the OPEA ChatQnA RAG deployment. As a microservices-based blueprint, it is designed for scalability, resilience, and flexibility. In this task, you will explore each microservice to understand its role within the overall system. By examining these components, you will gain insights into how they interact and contribute to the application\u0026rsquo;s functionality.\nThis architecture offers several key advantages:\nScalability – Each microservice can scale independently based on demand, ensuring optimal resource utilization and performance.\nFault Isolation – If one service encounters an issue, it won’t disrupt the entire system, enhancing reliability.\nEfficient Maintenance \u0026amp; Updates – Microservices allow for rapid updates and easy adaptability to evolving business needs and user demands.\nOPEA Microservices Architecture OPEA deployments are built around three key components:\nMegaservice – Acts as the orchestrator for all microservices, managing workflows and ensuring seamless interaction between components. This is essential for coordinating an end-to-end application with multiple moving parts. More details can be found in the OPEA documentation.\nGateway – Serves as the entry point for users, routing incoming requests to the appropriate microservices within the megaservice architecture. It ensures seamless connectivity between external users and internal components.\nMicroservices – These are the individual functional components of the application, handling tasks such as embeddings, retrieval, LLM processing, and vector database interactions. Accessing the Microservices\nBefore you begin exploring, note that only the gateway and UI services are exposed externally. In this task, you will directly access each internal microservice for testing purposes, using the Nginx gateway to efficiently route requests to these internal services.\nYou\u0026rsquo;ll need to take note of all pods deployed.\nkubectl get svc lists all services in a Kubernetes cluster, showing their names, types, cluster IPs, and exposed ports. It provides an overview of how applications are exposed for internal or external access.\nRun the following command on your CloudShell:\nYou will see output similar to this:\nThe kubectl get svc command is used to list the services running within a Kubernetes cluster. Services act as entry points that enable communication between different components of your application. Each service has a unique name (e.g., chatqna or chatqna-ui), which helps identify its role within the system.\nKubernetes services can be exposed in different ways:\nClusterIP – Only accessible within the cluster, allowing internal components to communicate securely.\nNodePort – Exposes the service externally through a specific port on each node, making it accessible outside the cluster. The Cluster-IP is the internal address used by other services to reach the application. If the service were accessible from outside the cluster, an External-IP would be displayed. However, in this case, these services are strictly internal.\nThe Ports column indicates which network ports the service listens on. For example:\nchatqna might be running on port 8888/TCP, handling internal communication.\nchatqna-nginx could be configured with 80:30144/TCP, where traffic from port 80 is forwarded to 30144 for routing purposes. Lastly, the Age column displays how long the service has been running—for instance, 12 hours for all listed services in this scenario.\nNow, let’s explore the architecture in detail.\nStep 1 : Megaservice (Orchestrator) (POD:chatqna:8888) The megaservice encapsulates the complete logic for the ChatQnA RAG application. This microservice is tasked with processing incoming requests and executing all the necessary internal operations to generate appropriate responses.\nThis service isn\u0026rsquo;t directly exposed, but you can access it directly from the LoadBalancer, which forwards the request.\nLook for the load balancer Click on chatqna-Ingress Note the DNS Name.As mentioned, it\u0026rsquo;s the public URL that can be accessed externally. You will use the curl command to send requests to the API endpoints, testing each microservice individually. The goal is to ask a question, such as \u0026ldquo;What was Nike\u0026rsquo;s revenue in 2023?\u0026rdquo;, and verify that the API responds correctly. This step ensures that all microservices in the system are functioning as expected.\nIf everything is working properly, you should receive a response, confirming that the Retrieval-Augmented Generation (RAG) workflow is operational.\nHowever, you may notice that the model is unable to provide an accurate answer. This happens because it lacks the necessary context and relies on outdated information. Without access to current and relevant data, the model cannot generate precise responses. In the next steps, you will enhance the system using RAG, allowing the model to retrieve up-to-date, contextually relevant information. This will ensure that it delivers more accurate and meaningful answers.\nNow, let\u0026rsquo;s explore each microservice in detail to understand its role and how it contributes to improving the model\u0026rsquo;s ability to answer questions correctly.\nStep 2 : Microservices Each microservice follows the following logic performing a task within the RAG flow:\nIn the flow, you can observe the microservices and we can divide the RAG flow into two steps:\nPreprompting: This step involves preparing the knowledge base (KB) by uploading relevant documents and ensuring that the information is organized for effective retrieval.\nPrompting: This step focuses on retrieving the relevant data from the knowledge base and using it to generate an accurate answer to the user\u0026rsquo;s question.\nPreprompting In this step, the logic is to start from a document (Nike\u0026rsquo;s revenue PDF), and do the preprocessing needed to make it ready to be stored in a database. As shown, this process primarily involves 3 microservices: data preparation, embeddings and vector store. Let\u0026rsquo;s explore each microservice\nEmbedding Microservice (POD: chatqna-tei:80) An embedding is a numerical representation of an object—such as a word, phrase, or document—within a continuous vector space. In natural language processing (NLP), embeddings transform words, sentences, or text segments into vectors—sets of numbers that capture their meaning, relationships, and contextual significance. This transformation enables machine learning models to process and understand text more effectively.\nFor example, word embeddings represent words as points in a vector space, where words with similar meanings—like \u0026ldquo;king\u0026rdquo; and \u0026ldquo;queen\u0026rdquo;—are positioned closer together. The embedding model captures these relationships through vector arithmetic.\nDuring training, if the model frequently encounters \u0026ldquo;king\u0026rdquo; in association with \u0026ldquo;man\u0026rdquo; and \u0026ldquo;queen\u0026rdquo; with \u0026ldquo;woman,\u0026rdquo; it learns that \u0026ldquo;king\u0026rdquo; and \u0026ldquo;queen\u0026rdquo; share a similar relationship to \u0026ldquo;man\u0026rdquo; and \u0026ldquo;woman.\u0026rdquo; This allows the model to position words in a way that reflects meaningful relationships, such as gender associations, in language.\nEmbeddings: A Key Component of RAG Embeddings play a crucial role in Retrieval-Augmented Generation (RAG) by enhancing the model’s ability to process and retrieve relevant information. They provide several key advantages:\nCapturing Meaning – Embeddings represent the semantic relationships between words, enabling RAG models to understand context, nuances, and deeper language structures. This improves their ability to generate relevant and coherent responses.\nDimensionality Reduction – By transforming complex textual data into fixed-size vectors, embeddings make data processing more efficient and scalable, improving the system\u0026rsquo;s performance.\nEnhancing Model Performance – By leveraging semantic similarities, embeddings enable more accurate information retrieval, refining the quality of generated responses and helping the model generalize better across various queries.\nOPEA offers multiple options for running embedding microservices, as detailed in the OPEA embedding documentation. In this case, ChatQnA uses the Hugging Face TEI microservice, which runs the embedding model BAAI/bge-large-en-v1.5 locally.\nSince some microservices are not exposed externally, you will use the Nginx pod to interact with them via curl. To do this, each microservice will be accessed using its internal DNS name.\nAccess to ngnix POD (copy your NGNIX entire pod name from kubectl get pods and REPLACE chatqna-nginx-xxxxxxxx on the below command) Your command prompt should now indicate that you are inside the container, reflecting the change in environment:\nOnce inside, you will now have direct access to the internal pods.\nGet the embedding from the Embeddings Microservice for the phrase \u0026ldquo;What was Deep Learning?\u0026rdquo;: The answer will be the vector representation of the phrase \u0026ldquo;What was Deep Learning?\u0026rdquo;. This service returns the vector embedding for the inputs from the REST API.\nVector Database Microservice (POD: chatqna-redis-vector-db:80) The Vector Database microservice plays a critical role in the Retrieval-Augmented Generation (RAG) application by storing and retrieving embeddings. This is essential for applications like ChatQnA, where relevant information needs to be efficiently retrieved based on a user\u0026rsquo;s query.\nUsing Redis as a Vector Database In this task, Redis is used as the vector database. However, OPEA supports multiple alternatives, which can be found in the OPEA vector store repository.\nA Vector Database (VDB) is specifically designed to store and manage high-dimensional vectors, which represent words, sentences, or images in numerical form. In AI and machine learning, these vectors—also known as embeddings—capture the meaning and relationships between data points, enabling efficient processing and retrieval.\nData Preparation Microservice (POD: chatqna-data-prep:6007) The Data Preparation (Dataprep) Microservice is responsible for formatting and preprocessing data so that it can be converted into embeddings and stored in the vector database. This ensures that the data is clean, structured, and ready for efficient retrieval.\nKey Functions of the Data Preparation Microservice Receives raw data (e.g., documents or reports).\nProcesses and chunks the data into smaller segments.\nSends the processed data to the Embedding Microservice for vectorization.\nStores the resulting embeddings in the Vector Database. Since different vector databases have unique data formatting requirements, the Dataprep Microservice ensures compatibility with the selected database.\nTesting the Microservices To verify the functionality of the system and help the model answer the initial question— \u0026ldquo;What was Nike\u0026rsquo;s revenue in 2023?\u0026quot;—you will need to upload a relevant context file (a revenue report) so it can be processed.\nTo do this, download a sample Nike revenue report to the Nginx pod using the command below. (If you are no longer logged into the Nginx pod, make sure to log in again before proceeding.)\nExecute the following command to download a sample Nike revenue report to the nginx pod (if you are no longer logged in to the NGinx pod, be sure to use the above command to log in again):\nDownload the document to the microservice : Feed the knowledge base (Vectord) with the document (It will take ~30 seconds): After running the previous command, you should receive a confirmation message like the one below. This command updated the knowledge base by uploading a local file for processing.\nThe data preparation microservice API can retrieve information about the list of files stored in the vector database.\nVerify if the document was uploaded: After running the previous command, you should receive the confirmation message.\nCongratulations! You\u0026rsquo;ve successfully prepared your knowledge base. Now you\u0026rsquo;ll explore the microservices involved in prompt handling.\nStep 3: Prompting Once the knowledge base is set up, you can begin interacting with the application by asking context-specific questions. Retrieval-Augmented Generation (RAG) ensures that responses are both accurate and grounded in relevant data.\nThe process begins with the application retrieving the most relevant information from the knowledge base in response to the user\u0026rsquo;s query. This step ensures that the Large Language Model (LLM) has access to up-to-date and precise context to generate an informed response.\nNext, the retrieved information is combined with the user’s input prompt and sent to the LLM. This enriched prompt enhances the model’s ability to provide answers that are not only based on its pre-trained knowledge but also supported by real-world, external data.\nFinally, you will see how the LLM processes this enriched prompt to generate a coherent and contextually accurate response. By leveraging RAG, the application delivers highly relevant answers, grounded in the latest information from the knowledge base.\nThe microservices involved in this stage include:\nEmbeddings Vector Database Retriever Re-ranking LLM Retriever Microservice (POD: chatqna-retriever-usvc:7000) The Retriever Microservice is responsible for locating the most relevant information within the knowledge base and returning documents that closely match the user’s query. It interacts with various back-end systems that store knowledge and provide APIs for retrieving the best-matching data.\nDifferent knowledge bases utilize different retrieval methods:\nVector databases use vector similarity matching between the user’s question and stored document embeddings. Graph databases leverage graph locality to find related information. Relational databases rely on string matching and regular expressions to locate relevant text. In this task, you will use Redis as the vector database and retrieve information via the Redis retriever.\nSince vector retrieval relies on embeddings, you first need to generate an embedding for the question: \u0026ldquo;What was Nike\u0026rsquo;s revenue in 2023?\u0026rdquo;\nThis will allow the retriever to search the knowledge base for the most relevant document—such as the Nike revenue report you uploaded in the previous step.\nTo create the embedding, use the chatqna-tei microservice. (Make sure you are logged into the Nginx pod before proceeding.)\nCreate the embedding and save locally (embed_question): You should get the details about the writing task:\nCheck to see if your embedding was saved: echo $embed_question\nYou should be able to see the vectors the embeddings microservice generated. You are now able to use the retriever microservice to get the most similar information from your knowledge base.\nGet and save similar vectors from the initial embed_question locally similar_docs: similar_docs=$(curl chatqna-retriever-usvc:7000/v1/retrieval -X POST -d \u0026ldquo;{\u0026quot;text\u0026quot;:\u0026quot;test\u0026quot;,\u0026quot;embedding\u0026quot;:${embed_question}}\u0026rdquo; -H \u0026lsquo;Content-Type: application/json\u0026rsquo;)\nBy looking at the previous output, you can see the most similar passages (TOP_3) from the document Nike revenue report and the question \u0026ldquo;What was the Nike revenue in 2023?\u0026rdquo;.\necho $similar_docs\nThe following output has been formatted for better readability. Your results will be presented in plain text and may vary slightly due to the similarity search algorithm. However, you can double check that the retrieved documents will be relevant to your initial query.\nThe application will use that information as context for prompting the LLM, but there is still one more step that you need to do to refine and check the quality of those retrieved documents: the reranker.\nReranker Microservice (POD: chatqna-teirerank:80) The Reranking Microservice plays a crucial role in semantic search, leveraging reranking models to enhance the relevance of retrieved results. When given a user query and a set of documents, this microservice reorders the documents based on their semantic similarity to the query, ensuring that the most relevant results appear first.\nReranking is particularly valuable in text retrieval systems, where documents are initially retrieved using either:\nDense embeddings, which capture deep semantic meaning. Sparse lexical search, which relies on keyword-based matching. While these retrieval methods are effective, the reranking model refines the results by optimizing the order of retrieved documents. This step significantly improves accuracy, ensuring the final output is more relevant, precise, and contextually aligned with the user’s query.\nOPEA has multiple options for re-rankers. For this lab, you\u0026rsquo;ll use the Hugging Face TEI for re-ranking. It is the chatqna-teirerank microservice in your cluster.\nThe reranker will use similar_docs from the previous stage and compare it with the question What was Nike Revenue in 2023? to check the quality of the retrieved documents.\nExtract the 3 retrieved text snippets and save them in a new variable to be reranked:\nInstall jq dependencies to format similar_docs echo -e \u0026ldquo;deb http://deb.debian.org/debian bookworm main contrib non-free\\ndeb http://security.debian.org/debian-security bookworm-security main contrib non-free\\ndeb http://deb.debian.org/debian bookworm-updates main contrib non-free\u0026rdquo; \u0026gt; /etc/apt/sources.list \u0026amp;\u0026amp; apt update \u0026amp;\u0026amp; apt install -y jq\nExtract and format the texts into a valid JSON array of strings texts=$(echo \u0026ldquo;$similar_docs\u0026rdquo; | jq -r \u0026lsquo;[.retrieved_docs[].text | @json]\u0026rsquo;)\nSend the request to the microservice with the query and the formatted texts: curl -X POST chatqna-teirerank:80/rerank -d \u0026ldquo;{\u0026quot;query\u0026quot;:\u0026quot;What was Nike Revenue in 2023?\u0026quot;, \u0026quot;texts\u0026quot;: $texts}\u0026rdquo; -H \u0026lsquo;Content-Type: application/json\u0026rsquo;\nResponse:\nThe following output has been formatted for better readability. Your results are displayed in plain text and may vary slightly due to the similarity search algorithm. The retrieved documents are ranked by similarity to your query, with the highest-ranked index representing the most relevant match. You can confirm that the top-ranked document corresponds to the one most closely aligned with your query.\nThe server responds with a JSON array containing objects with two fields: index and score. This indicates how the snippets are ranked based on their relevance to the query: {\u0026ldquo;index\u0026rdquo;:2,\u0026ldquo;score\u0026rdquo;:0.9972289} means the first text (index 0) has a high relevance score of approximately 0.7982. {\u0026ldquo;index\u0026rdquo;:0,\u0026ldquo;score\u0026rdquo;:0.9776342},{\u0026ldquo;index\u0026rdquo;:3,\u0026ldquo;score\u0026rdquo;:0.9296986},{\u0026ldquo;index\u0026rdquo;:1,\u0026ldquo;score\u0026rdquo;:0.84730965} indicates that the other snippets (index 3,1 and 2) have a much lower score.\nAs you can see from similar_doc the id=2 has the below information where it EXACTLY refers to the revenue for 2023!\nJust the first will be used to prompt the LLM.\nLLM Microservice (POD: chatqna-tgi:80) At the core of the RAG (Retrieval-Augmented Generation) application lies the Large Language Model (LLM), which plays a pivotal role in generating responses. By leveraging RAG, the system enhances the LLM’s performance, ensuring responses are accurate, relevant, and context-aware.\nTypes of LLMs LLMs generally fall into two main categories, each with its own strengths and trade-offs:\nClosed-Source Models These proprietary models are developed by major tech companies such as Amazon Web Services (AWS), OpenAI, and Google. They are trained on extensive datasets and optimized for high-quality, reliable outputs. However, they come with certain limitations:\nLimited Customization: Users have minimal control over fine-tuning. Higher Costs: Access is usually metered and can be expensive. Data Sovereignty Concerns: API access may restrict usage in applications requiring strict data governance. Open-Source Models Freely available for use and modification, open-source LLMs offer greater flexibility and control. They allow users to customize and fine-tune models according to specific needs. Running open-source models locally or on private cloud infrastructure ensures better data privacy and cost efficiency. However, they require:\nTechnical Expertise: Deploying and optimizing open-source models can be complex. Computational Resources: Achieving comparable performance to closed models often demands powerful hardware. Flexible Integration with OPEA: This microservice architecture supports both closed and open-source models, providing the flexibility to choose the best fit for your application. In this example, the TGI (Text Generation Inference) model from Hugging Face is used. Testing the LLM Microservice: To verify its functionality, you can directly prompt the TGI LLM with a sample question: \u0026ldquo;What was Nike\u0026rsquo;s revenue in 2023?\u0026rdquo; This test will demonstrate how well the model can retrieve and generate an informed response based on the loaded knowledge base.\nDirectly prompt the TGI(LLM) Microservice: The model will give you the answer to the prompt like the following:\n\u0026ldquo;generated_text\u0026rdquo;:\u0026rdquo; Nike revenue in 2023 has not been reported as it is still in the fourth quarter. The previous full financial year—which is 2022—brought in $48.9 billion in revenue for the American multinational sportswear company. They deal with the design, development, manufacturing, and worldwide marketing/sales of a diverse portfolio of products. From coming into being in 1964 as Blue Ribbon Sports, the firm was renamed Nike, Inc., in 1978. Jumpman logos (for example), include Michael Jordan, who is a former professional basketball player—are among the brands\u0026rsquo; numerous trademarked symbols, tied to the \u0026lsquo;Swoosh\u0026rsquo; logo.\\n\\nNike revenues are clearly affected by the football World Cup. Consequently, for the 13 weeks ending January 29 in 2022, which were characterized by the football world cup\nĐiều này trực tiếp nhắc LLM mà không cung cấp ngữ cảnh. Chúng ta có thể thấy rằng mô hình thực sự đưa ra câu trả lời sai. Để kiểm tra hiệu suất RAG tổng thể, chúng ta nên thử nghiệm với megaservice như chúng ta đã làm khi bắt đầu nhiệm vụ này, điều này sẽ liên quan đến toàn bộ luồng.\nThoát khỏi POD ngnix: Bạn có thể tìm thấy \u0026ldquo;công việc đang chờ xử lý\u0026rdquo;, vui lòng bỏ qua và thử lại. root@chatqna-nginx-deployment-XXXXXXXXXXXX:/# exit\nSử dụng URL bộ cân bằng tải mà bạn đã lưu ở trên trong lệnh bên dưới để gửi câu hỏi \u0026ldquo;Doanh thu của Nike năm 2023 là bao nhiêu?\u0026rdquo; đến ứng dụng ChatQNA.\nChạy lại curl đến bộ cân bằng tải: Xem lại kết quả của bạn và bạn sẽ thấy rằng phản hồi được truyền phát. Đây là hành vi mong đợi của dịch vụ vi mô, vì nó cung cấp câu trả lời theo từng phần nhỏ hơn thay vì tất cả cùng một lúc. Streaming cho phép dữ liệu được xử lý và hiển thị theo từng bước khi dữ liệu có sẵn. Trong ứng dụng, UI sẽ ghi lại phản hồi này và định dạng thành màn hình có thể đọc được, cho phép người dùng xem thông tin theo thời gian thực khi dữ liệu đến. "
},
{
	"uri": "http://<user_name>.github.io/4-s3log/4.2-creates3bucket/",
	"title": "Lớp 7",
	"tags": [],
	"description": "",
	"content": "Test application\nYou can check the deployment by accessing to the DNS url of the load balancer the cloud formation templated created.\nLook for the load balancer: Copy your DNS name for bedrock-ingress: Paste it on a new browser tab to access to the interface In the UI you can see the chatbot to interact with it\nCheck if the model is able to give us an answer about OPEA: You may notice that the chatbot’s initial response is outdated or lacks specific details about OPEA. This is because OPEA is a relatively new project and was not included in the dataset used to train the language model. Since most LLMs (Large Language Models) are static, they rely solely on pre-existing training data and cannot automatically incorporate new developments or emerging technologies like OPEA.\nUploading Context to Improve Accuracy To address this limitation, RAG (Retrieval-Augmented Generation) enables real-time context retrieval. The ChatQnA UI includes an upload icon, allowing you to add relevant context.\nHow It Works:\n1. When you upload a document or link, it is sent to the DataPrep microservice.\r2. DataPrep processes the content and generates embeddings.\r3. The processed data is then stored in the Vector Database for retrieval.\rBy uploading updated documents or links, you expand the chatbot’s knowledge base, ensuring it provides more relevant, accurate, and up-to-date responses.\nThe deployment allows you to upload either a file or a site. For this case, use the OPEA site:\nClick on the upload icon to open the right panel\nClick on Paste Link\nCopy/paste the text https://opea-project.github.io/latest/introduction/index.html to the entry box\nClick Confirm to start the indexing process When the indexing completes, you\u0026rsquo;ll see an icon added below the text box, labeled https://opea-project.github.io/latest/introduction/index.html\nAsk the application after the context is provided: Ask \u0026ldquo;What is OPEA?\u0026rdquo; again to see the updated answer.\nThis time, the chat bot responds correctly based on the data it added to the prompt from the new source, the OPEA web site.\nConclusion\nIn this task, you successfully deployed a RAG-powered chatbot using Amazon Bedrock. By uploading relevant context, you enabled the model to dynamically update and refine its responses based on new information. This process demonstrated how RAG integration enhances real-time adaptability, allowing the system to continuously improve its accuracy and relevance while leveraging the power of Amazon Bedrock.\n"
},
{
	"uri": "http://<user_name>.github.io/2-prerequiste/2.2-createiamrole-copy/",
	"title": "Lớp 8",
	"tags": [],
	"description": "",
	"content": "Preparing Your AWS Environment for the Workshop If you are not participating in an AWS-led event via Workshop Studio, you must set up your environment before proceeding with these labs. Keep in mind that the resources you create will incur costs, so be sure to clean them up once you have completed the workshop.\nStep 1: Configuring Your Environment To set up your AWS account, follow these steps:\nClick Launch Stack below to initiate AWS CloudFormation with pre-configured values in the us-east-1 region.\nLaunch Stack\nIf you prefer a different AWS region, remember to change the region accordingly. The CloudFormation stack will provision the following resources:\nEKS Cluster Deployment\n-Creates a new EKS cluster named opea-eks-cluster.\n-Deploys a worker node within the cluster using an M7i.24xlarge instance.\nCloudFormation Templates for Workshop Modules The stack also includes templates for the following modules:\nModule 1: ChatQnA Default\nModule 2: ChatQnA with Guardrails\nModule 3: ChatQnA with OpenSearch (open-source) as the vector database\nModule 4: ChatQnA with Remote Inference (Denvr) as the LLM\nModule 5: ChatQnA with Bedrock as the LLM\nConfiguring Stack Parameters Before launching the stack, review and configure the following parameters:\nFor Modules 1, 2, and 3:\nHuggingFaceToken: This token is required to download models from Hugging Face. If you plan to use the Guardrails feature, ensure your token has access to the meta-llama/Meta-Llama-Guard-2-8B model.\nModelID: The OPEA system primarily uses the Text Generation Inference toolkit. Select any compatible model from Hugging Face and provide its model ID.\nOpeaRoleArn: Enter the ARN or name of the IAM role associated with your AWS account.\nIf you are unsure, check the user information displayed in the top-right corner of the AWS Management Console. If your username does not contain a forward slash (/), copy the full name. If it does contain a forward slash, only use the portion before the slash. Example:\nIf the display name is \u0026ldquo;USER\u0026rdquo;, enter \u0026ldquo;USER\u0026rdquo;. If the display name is \u0026ldquo;ADMIN-ROLE/USER\u0026rdquo;, enter \u0026ldquo;ADMIN-ROLE\u0026rdquo;. Alternatively, you can retrieve your ARN using the AWS CLI. aws sts get-caller-identity \u0026ndash;query \u0026lsquo;Arn\u0026rsquo; \u0026ndash;output text | awk -F: \u0026lsquo;{print $NF}\u0026rsquo; | (read id; if [[ $id == \u0026ldquo;user\u0026rdquo; ]]; then aws sts get-caller-identity \u0026ndash;query \u0026lsquo;Arn\u0026rsquo; \u0026ndash;output text; else role=$(echo $id | cut -d\u0026rsquo;/\u0026rsquo; -f2); aws iam get-role \u0026ndash;role-name $role \u0026ndash;query \u0026lsquo;Role.Arn\u0026rsquo; \u0026ndash;output text; fi)\nMark the checkbox next to I acknowledge that AWS CloudFormation might create IAM resources\nDeploying the CloudFormation Stack Once the parameters are set, click Create Stack.\nThis process will initiate an AWS CodeBuild Project, which pulls in the opea-demo-builder open-source library.\nThe AWS Cloud Development Kit (CDK) will generate the necessary CloudFormation templates to configure your AWS environment for the workshop.\nMonitoring Deployment Progress The deployment process takes approximately 25 minutes. Open the AWS CloudFormation Console and monitor the progress.\nEnsure that the following stacks reach the CREATE_COMPLETE status:\nThe EKS cluster The ChatQnA application (default deployment) Some stacks may remain in REVIEW_IN_PROGRESS, as they will be deployed onto the EKS cluster later.\nStep 2: Configuring Access to Your EKS Cluster Once the CloudFormation stacks are successfully deployed, you need to configure your local environment to interact with the Amazon EKS cluster using kubectl.\nUpdating Your Kubernetes Configuration (kubeconfig)\nOpen AWS CloudShell Click the CloudShell icon in the AWS Management Console. Alternatively, use your local AWS CLI, ensuring kubectl and the AWS CLI client are installed on your system. Update kubeconfig Run the following command (updating the region if necessary): If successful, you should receive a confirmation message indicating that your configuration file has been updated. Verify Connectivity to Your EKS Cluster Ensure you can interact with the cluster using kubectl. Step 3: Verifying EKS Cluster Access After updating your kubeconfig, test whether you can connect to the cluster. If you encounter issues accessing pods via the AWS console or CloudShell, follow these steps:\nCheck Your IAM Access in the EKS Console\nNavigate to the EKS Console. Look for your IAM principal ARN under Access Entries. If Your ARN Is Not Listed, Add It Manually\nClick Create Access Entry. Enter your IAM User ARN or Role ARN. Attach Required Policies Assign the following permissions to your IAM role:\nAmazonEKSAdminPolicy AmazonEKSClusterAdminPolicy Confirm Access by Listing Nodes\nRun the following command to check if worker nodes are visible: If the command returns a list of nodes, your cluster is successfully configured. Note: If no nodes appear, wait a few minutes and retry, as node provisioning may still be in progress. Next Steps: Explore the Workshop Modules Once you have successfully connected to your EKS cluster, you are ready to proceed with any of the workshop modules. Choose the module that best fits your learning objectives and begin your hands-on experience with AWS, Kubernetes, and AI-powered solutions.\n"
},
{
	"uri": "http://<user_name>.github.io/3-accessibilitytoinstances/3.2-private-instance-copy/",
	"title": "Lớp 8",
	"tags": [],
	"description": "",
	"content": "Understand RAG and use the UI Now that you\u0026rsquo;ve verified all services are running, let’s take a look at the UI provided by the implementation.\nTo access the UI, open any browser and go to the DNS of the ChatQnA Load Balancer: http://chatqna-ingress-xxxxxxx.us-east-2.elb.amazonaws.com (Modify with your chatqna-ingressDNS URL)\nIn the UI you can see the chatbot interact with it\nTo verify the UI, go ahead and ask\nThe answer is correct again because we already indexed our knowledge base on the previous step.\nLet\u0026rsquo;s try something different. Will the app be able to answer about OPEA:\nYou may notice that the chatbot\u0026rsquo;s initial response is outdated or lacks specific details about OPEA. This is because OPEA is a relatively new project and was not included in the dataset used to train the language model. Since most language models are static—meaning they rely on the data available at the time of training—they cannot automatically incorporate recent developments or newly emerging topics like OPEA.\nHowever, RAG provides a solution by enabling real-time context retrieval. Within the UI, you\u0026rsquo;ll find an option to upload relevant contextual information. When you do this, the document is sent to the DataPrep microservice, where it is converted into embeddings and stored in the Vector Database.\nBy uploading a document or a link, you effectively expand the chatbot’s knowledge base with the latest information, improving the relevance and accuracy of its responses.\nThe deployment allows you to upload either a file or a site. For this case, use the OPEA site:\nClick on the upload icon to open the right panel Click on Paste Link Copy/paste the text https://opea-project.github.io/latest/introduction/index.html to the entry box Click Confirm to start the indexing process When the indexing completes, you\u0026rsquo;ll see an icon added below the text box, labeled https://opea-project.github.io/latest/introduction/index.html\nAsk \u0026ldquo;What is OPEA?\u0026rdquo; again to see the updated answer.\nThis time, the chatbot responds correctly based on the data it added to the prompt from the new source, the OPEA website.\nConclusion\nIn this task, you explored the core structure of a RAG application, gaining insight into how each component functions and interacts within the system. From retrieving relevant information to generating accurate responses, every part plays a vital role in OPEA’s RAG workflow—enhancing response relevance through retrieval while improving accuracy with advanced language modeling. This hands-on session provided a clear understanding of how OPEA leverages RAG to process complex queries efficiently and refine model performance through seamless component integration.\nIn the next task, you will implement guardrails for the chatbot. These guardrails are essential for detecting and mitigating biases, ensuring that AI-generated responses remain responsible, fair, and aligned with ethical AI principles.\n"
},
{
	"uri": "http://<user_name>.github.io/2-prerequiste/2.2-createiamrole-copy-2/",
	"title": "Lớp 9",
	"tags": [],
	"description": "",
	"content": "Preparing Your AWS Environment for the Workshop If you are not participating in an AWS-led event via Workshop Studio, you must set up your environment before proceeding with these labs. Keep in mind that the resources you create will incur costs, so be sure to clean them up once you have completed the workshop.\nStep 1: Configuring Your Environment To set up your AWS account, follow these steps:\nClick Launch Stack below to initiate AWS CloudFormation with pre-configured values in the us-east-1 region.\nLaunch Stack\nIf you prefer a different AWS region, remember to change the region accordingly. The CloudFormation stack will provision the following resources:\nEKS Cluster Deployment\n-Creates a new EKS cluster named opea-eks-cluster.\n-Deploys a worker node within the cluster using an M7i.24xlarge instance.\nCloudFormation Templates for Workshop Modules The stack also includes templates for the following modules:\nModule 1: ChatQnA Default\nModule 2: ChatQnA with Guardrails\nModule 3: ChatQnA with OpenSearch (open-source) as the vector database\nModule 4: ChatQnA with Remote Inference (Denvr) as the LLM\nModule 5: ChatQnA with Bedrock as the LLM\nConfiguring Stack Parameters Before launching the stack, review and configure the following parameters:\nFor Modules 1, 2, and 3:\nHuggingFaceToken: This token is required to download models from Hugging Face. If you plan to use the Guardrails feature, ensure your token has access to the meta-llama/Meta-Llama-Guard-2-8B model.\nModelID: The OPEA system primarily uses the Text Generation Inference toolkit. Select any compatible model from Hugging Face and provide its model ID.\nOpeaRoleArn: Enter the ARN or name of the IAM role associated with your AWS account.\nIf you are unsure, check the user information displayed in the top-right corner of the AWS Management Console. If your username does not contain a forward slash (/), copy the full name. If it does contain a forward slash, only use the portion before the slash. Example:\nIf the display name is \u0026ldquo;USER\u0026rdquo;, enter \u0026ldquo;USER\u0026rdquo;. If the display name is \u0026ldquo;ADMIN-ROLE/USER\u0026rdquo;, enter \u0026ldquo;ADMIN-ROLE\u0026rdquo;. Alternatively, you can retrieve your ARN using the AWS CLI. aws sts get-caller-identity \u0026ndash;query \u0026lsquo;Arn\u0026rsquo; \u0026ndash;output text | awk -F: \u0026lsquo;{print $NF}\u0026rsquo; | (read id; if [[ $id == \u0026ldquo;user\u0026rdquo; ]]; then aws sts get-caller-identity \u0026ndash;query \u0026lsquo;Arn\u0026rsquo; \u0026ndash;output text; else role=$(echo $id | cut -d\u0026rsquo;/\u0026rsquo; -f2); aws iam get-role \u0026ndash;role-name $role \u0026ndash;query \u0026lsquo;Role.Arn\u0026rsquo; \u0026ndash;output text; fi)\nMark the checkbox next to I acknowledge that AWS CloudFormation might create IAM resources\nDeploying the CloudFormation Stack Once the parameters are set, click Create Stack.\nThis process will initiate an AWS CodeBuild Project, which pulls in the opea-demo-builder open-source library.\nThe AWS Cloud Development Kit (CDK) will generate the necessary CloudFormation templates to configure your AWS environment for the workshop.\nMonitoring Deployment Progress The deployment process takes approximately 25 minutes. Open the AWS CloudFormation Console and monitor the progress.\nEnsure that the following stacks reach the CREATE_COMPLETE status:\nThe EKS cluster The ChatQnA application (default deployment) Some stacks may remain in REVIEW_IN_PROGRESS, as they will be deployed onto the EKS cluster later.\nStep 2: Configuring Access to Your EKS Cluster Once the CloudFormation stacks are successfully deployed, you need to configure your local environment to interact with the Amazon EKS cluster using kubectl.\nUpdating Your Kubernetes Configuration (kubeconfig)\nOpen AWS CloudShell Click the CloudShell icon in the AWS Management Console. Alternatively, use your local AWS CLI, ensuring kubectl and the AWS CLI client are installed on your system. Update kubeconfig Run the following command (updating the region if necessary): If successful, you should receive a confirmation message indicating that your configuration file has been updated. Verify Connectivity to Your EKS Cluster Ensure you can interact with the cluster using kubectl. Step 3: Verifying EKS Cluster Access After updating your kubeconfig, test whether you can connect to the cluster. If you encounter issues accessing pods via the AWS console or CloudShell, follow these steps:\nCheck Your IAM Access in the EKS Console\nNavigate to the EKS Console. Look for your IAM principal ARN under Access Entries. If Your ARN Is Not Listed, Add It Manually\nClick Create Access Entry. Enter your IAM User ARN or Role ARN. Attach Required Policies Assign the following permissions to your IAM role:\nAmazonEKSAdminPolicy AmazonEKSClusterAdminPolicy Confirm Access by Listing Nodes\nRun the following command to check if worker nodes are visible: If the command returns a list of nodes, your cluster is successfully configured. Note: If no nodes appear, wait a few minutes and retry, as node provisioning may still be in progress. Next Steps: Explore the Workshop Modules Once you have successfully connected to your EKS cluster, you are ready to proceed with any of the workshop modules. Choose the module that best fits your learning objectives and begin your hands-on experience with AWS, Kubernetes, and AI-powered solutions.\n"
},
{
	"uri": "http://<user_name>.github.io/3-accessibilitytoinstances/3.2-private-instance-copy-2/",
	"title": "Lớp 9",
	"tags": [],
	"description": "",
	"content": "Understand RAG and use the UI Now that you\u0026rsquo;ve verified all services are running, let’s take a look at the UI provided by the implementation.\nTo access the UI, open any browser and go to the DNS of the ChatQnA Load Balancer: http://chatqna-ingress-xxxxxxx.us-east-2.elb.amazonaws.com (Modify with your chatqna-ingressDNS URL)\nIn the UI you can see the chatbot interact with it\nTo verify the UI, go ahead and ask\nThe answer is correct again because we already indexed our knowledge base on the previous step.\nLet\u0026rsquo;s try something different. Will the app be able to answer about OPEA:\nYou may notice that the chatbot\u0026rsquo;s initial response is outdated or lacks specific details about OPEA. This is because OPEA is a relatively new project and was not included in the dataset used to train the language model. Since most language models are static—meaning they rely on the data available at the time of training—they cannot automatically incorporate recent developments or newly emerging topics like OPEA.\nHowever, RAG provides a solution by enabling real-time context retrieval. Within the UI, you\u0026rsquo;ll find an option to upload relevant contextual information. When you do this, the document is sent to the DataPrep microservice, where it is converted into embeddings and stored in the Vector Database.\nBy uploading a document or a link, you effectively expand the chatbot’s knowledge base with the latest information, improving the relevance and accuracy of its responses.\nThe deployment allows you to upload either a file or a site. For this case, use the OPEA site:\nClick on the upload icon to open the right panel Click on Paste Link Copy/paste the text https://opea-project.github.io/latest/introduction/index.html to the entry box Click Confirm to start the indexing process When the indexing completes, you\u0026rsquo;ll see an icon added below the text box, labeled https://opea-project.github.io/latest/introduction/index.html\nAsk \u0026ldquo;What is OPEA?\u0026rdquo; again to see the updated answer.\nThis time, the chatbot responds correctly based on the data it added to the prompt from the new source, the OPEA website.\nConclusion\nIn this task, you explored the core structure of a RAG application, gaining insight into how each component functions and interacts within the system. From retrieving relevant information to generating accurate responses, every part plays a vital role in OPEA’s RAG workflow—enhancing response relevance through retrieval while improving accuracy with advanced language modeling. This hands-on session provided a clear understanding of how OPEA leverages RAG to process complex queries efficiently and refine model performance through seamless component integration.\nIn the next task, you will implement guardrails for the chatbot. These guardrails are essential for detecting and mitigating biases, ensuring that AI-generated responses remain responsible, fair, and aligned with ethical AI principles.\n"
},
{
	"uri": "http://<user_name>.github.io/3-accessibilitytoinstances/",
	"title": "Bài Giảng Văn",
	"tags": [],
	"description": "",
	"content": "Lời nói đầu Ngữ văn không chỉ là môn học về chữ nghĩa mà còn là cánh cửa dẫn lối vào thế giới của tư duy, cảm xúc và sự sáng tạo. Văn học giúp học sinh hiểu sâu hơn về cuộc sống, con người, đồng thời rèn luyện khả năng diễn đạt, tư duy phản biện và cảm thụ cái đẹp trong ngôn từ.\nỞ bậc trung học cơ sở, môn Văn đóng vai trò quan trọng trong việc hình thành tư duy logic và khả năng biểu đạt của học sinh. Thông qua việc học các tác phẩm văn học, các em không chỉ tiếp cận với những giá trị nhân văn sâu sắc mà còn rèn luyện cách lập luận, phân tích và trình bày ý kiến một cách mạch lạc, thuyết phục.\nChương trình Ngữ văn trung học cơ sở bao gồm ba phần chính: Đọc – hiểu văn bản, Tiếng Việt và Tập làm văn.\nĐọc – hiểu văn bản giúp học sinh tiếp xúc với các tác phẩm văn học Việt Nam và thế giới, từ những bài ca dao, truyện ngắn, thơ ca cho đến các văn bản nhật dụng mang tính thực tiễn. Qua đó, học sinh rèn luyện khả năng phân tích nội dung, đánh giá nghệ thuật và hiểu được tư tưởng của tác giả. Phần Tiếng Việt giúp học sinh nắm vững các quy tắc ngữ pháp, mở rộng vốn từ, sử dụng ngôn ngữ linh hoạt và chính xác trong giao tiếp cũng như viết bài. Phần Tập làm văn rèn luyện kỹ năng viết, từ việc miêu tả, tự sự, biểu cảm đến nghị luận, giúp học sinh biết cách xây dựng lập luận chặt chẽ, diễn đạt ý tưởng rõ ràng và giàu cảm xúc. Tại trung tâm, chương trình giảng dạy môn Văn được thiết kế không chỉ để giúp học sinh đạt điểm cao mà còn truyền cảm hứng để các em yêu thích môn học này. Học sinh sẽ được hướng dẫn cách đọc hiểu nhanh, phân tích sâu, viết văn mạch lạc và sáng tạo. Các phương pháp giảng dạy đa dạng như thảo luận nhóm, phân tích tình huống, sáng tác văn học, giúp học sinh rèn luyện tư duy phản biện và khả năng viết tốt hơn.\nNgữ văn không chỉ là một môn học trong nhà trường mà còn là công cụ giúp học sinh phát triển kỹ năng sống. Biết cách sử dụng ngôn từ tinh tế, lập luận chặt chẽ và thể hiện cảm xúc chân thành chính là nền tảng để các em tự tin hơn trong giao tiếp và thành công trong tương lai.\n"
},
{
	"uri": "http://<user_name>.github.io/4-s3log/4.3-creategwes3/",
	"title": "Lớp 8",
	"tags": [],
	"description": "",
	"content": "Explore RAG and Interact with the UI\nNow that all services are up and running, let\u0026rsquo;s explore the UI provided by the implementation.\nTo access the ChatQnA Bedrock UI, open a web browser and navigate to the DNS of the ChatQnA Bedrock Load Balancer:\n👉 http://bedrock-ingress-xxxxxxx.us-east-2.elb.amazonaws.com (Replace with your actual Bedrock Ingress DNS URL).\nOnce inside the UI, you can interact with the chatbot, test its responses, and experience how it processes queries using RAG-powered retrieval.\nNow when you send a prompt to the chatbot, the response will be coming from Anthropic\u0026rsquo;s Claude Haiku through Amazon Bedrock.\n"
},
{
	"uri": "http://<user_name>.github.io/4-s3log/4.3-creategwes3-copy/",
	"title": "Lớp 9",
	"tags": [],
	"description": "",
	"content": "Explore RAG and Interact with the UI\nNow that all services are up and running, let\u0026rsquo;s explore the UI provided by the implementation.\nTo access the ChatQnA Bedrock UI, open a web browser and navigate to the DNS of the ChatQnA Bedrock Load Balancer:\n👉 http://bedrock-ingress-xxxxxxx.us-east-2.elb.amazonaws.com (Replace with your actual Bedrock Ingress DNS URL).\nOnce inside the UI, you can interact with the chatbot, test its responses, and experience how it processes queries using RAG-powered retrieval.\nNow when you send a prompt to the chatbot, the response will be coming from Anthropic\u0026rsquo;s Claude Haiku through Amazon Bedrock.\n"
},
{
	"uri": "http://<user_name>.github.io/4-s3log/",
	"title": "Bài Giảng Tiếng Anh",
	"tags": [],
	"description": "",
	"content": "Lời nói đầu Tiếng Anh không chỉ là một môn học, mà còn là chìa khóa giúp học sinh mở rộng cánh cửa đến với thế giới tri thức, công nghệ và hội nhập quốc tế. Trong thời đại toàn cầu hóa, việc thông thạo Tiếng Anh mang lại vô số cơ hội trong học tập, công việc và cuộc sống.\nỞ bậc trung học cơ sở, chương trình Tiếng Anh giúp học sinh xây dựng nền tảng vững chắc về từ vựng, ngữ pháp, kỹ năng nghe, nói, đọc, viết. Đây là giai đoạn quan trọng để các em rèn luyện khả năng sử dụng ngôn ngữ một cách tự tin và hiệu quả.\nChương trình Tiếng Anh trung học cơ sở được chia thành các kỹ năng chính:\nNgữ pháp và từ vựng: Giúp học sinh nắm vững cấu trúc câu, cách sử dụng thì, từ loại, cụm từ quan trọng, từ đó hình thành khả năng diễn đạt chính xác và linh hoạt.\nKỹ năng nghe – nói: Học sinh được tiếp xúc với các đoạn hội thoại, bài nghe thực tế, luyện tập phát âm chuẩn và nâng cao khả năng giao tiếp.\nKỹ năng đọc hiểu: Rèn luyện khả năng nắm bắt thông tin, phân tích văn bản và trau dồi vốn từ để hiểu sâu hơn các bài đọc.\nKỹ năng viết: Học sinh học cách viết từ những câu đơn giản đến đoạn văn và bài luận hoàn chỉnh, giúp thể hiện ý tưởng mạch lạc và sáng tạo.\nTại trung tâm, việc giảng dạy Tiếng Anh không chỉ dừng lại ở sách giáo khoa mà còn kết hợp với các tài liệu thực tế, phương pháp học hiện đại như học qua video, podcast, hội thoại trực tuyến, giúp học sinh tiếp cận ngôn ngữ một cách tự nhiên và hứng thú. Các bài giảng được thiết kế sinh động, lồng ghép vào các chủ đề thực tế, giúp học sinh rèn luyện phản xạ nhanh, tự tin sử dụng Tiếng Anh trong giao tiếp hằng ngày.\nNgoài ra, trung tâm còn hướng đến việc giúp học sinh đạt kết quả cao trong các kỳ thi quan trọng như thi học kỳ, thi tuyển sinh vào lớp 10, thi chứng chỉ quốc tế (KET, PET, IELTS, TOEIC), tạo tiền đề vững chắc cho việc học Tiếng Anh nâng cao ở các cấp bậc tiếp theo.\nHọc Tiếng Anh không chỉ là học một ngôn ngữ mới mà còn là học cách tư duy linh hoạt, giao tiếp hiệu quả và hòa nhập với thế giới. Với phương pháp giảng dạy khoa học, đội ngũ giáo viên tận tâm, trung tâm cam kết mang lại một môi trường học tập hiện đại, giúp học sinh không chỉ chinh phục Tiếng Anh mà còn có niềm yêu thích với ngôn ngữ này!\n"
},
{
	"uri": "http://<user_name>.github.io/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://<user_name>.github.io/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]