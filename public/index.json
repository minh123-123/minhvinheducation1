[
{
	"uri": "http://<user_name>.github.io/1-introduce/2.1-createec2/",
	"title": "ƒê·ªôi ng≈© gi√°o vi√™n",
	"tags": [],
	"description": "",
	"content": "1.ƒê·∫∑ng VƒÉn Minh ‚Äì Gia s∆∞ To√°n, VƒÉn, Anh Th√†nh t√≠ch h·ªçc t·∫≠p:\nSinh vi√™n nƒÉm cu·ªëi ƒê·∫°i h·ªçc T√†i ch√≠nh ‚Äì Marketing (UFM), ng√†nh H·ªá th·ªëng th√¥ng tin qu·∫£n l√Ω, chuy√™n ng√†nh Tin h·ªçc qu·∫£n l√Ω.\nH·ªçc sinh gi·ªèi/xu·∫•t s·∫Øc 12 nƒÉm li·ªÅn.\nC·ª±u h·ªçc sinh THPT Nguy·ªÖn H·ªØu Hu√¢n ‚Äì m·ªôt trong nh·ªØng tr∆∞·ªùng top ƒë·∫ßu.\nƒêi·ªÉm tuy·ªÉn sinh l·ªõp 10 nƒÉm 2019: 39 ƒëi·ªÉm.\nTh√†nh vi√™n l·ªõp ch·ªçn Ti·∫øng Anh TƒÉng C∆∞·ªùng tr∆∞·ªùng THCS Hoa L∆∞ 4 nƒÉm li√™n ti·∫øp.\nTh√†nh vi√™n ƒë·ªôi tuy·ªÉn h·ªçc sinh gi·ªèi VƒÉn c·∫•p qu·∫≠n l·ªõp 6, 7.\nCh·ª©ng ch·ªâ TOEIC 800\nƒê·∫£ng vi√™n ƒê·∫£ng C·ªông s·∫£n Vi·ªát Nam\nKinh nghi·ªám gi·∫£ng d·∫°y \u0026amp; l√†m vi·ªác:\nH∆°n 1 nƒÉm kinh nghi·ªám gia s∆∞, gi·∫£ng d·∫°y To√°n, VƒÉn, Anh cho h·ªçc sinh c·∫•p 2.\nTh·ª±c t·∫≠p sinh t·∫°i Amazon Web Services Vi·ªát Nam (AWS), c√≥ ki·∫øn th·ª©c v·ªÅ ·ª©ng d·ª•ng c√¥ng ngh·ªá trong gi√°o d·ª•c.\nPh∆∞∆°ng ph√°p gi·∫£ng d·∫°y:\nK·∫øt h·ª£p To√°n, VƒÉn, Anh ƒë·ªÉ ph√°t tri·ªÉn t∆∞ duy to√†n di·ªán.\nR√®n luy·ªán t∆∞ duy ph·∫£n bi·ªán, k·ªπ nƒÉng gi·∫£i quy·∫øt v·∫•n ƒë·ªÅ v√† ph∆∞∆°ng ph√°p h·ªçc t·∫≠p hi·ªáu qu·∫£.\n·ª®ng d·ª•ng c√¥ng ngh·ªá v√†o gi·∫£ng d·∫°y ƒë·ªÉ n√¢ng cao hi·ªáu su·∫•t h·ªçc t·∫≠p.\n2. ƒê·ªó Tr·ªçng Nh√¢n ‚Äì Gi√°o vi√™n Ti·∫øng Anh n√¢ng cao (Hi·ªáu Tr∆∞·ªüng) Th√†nh t√≠ch h·ªçc t·∫≠p:\nSinh vi√™n ng√†nh Ng√¥n ng·ªØ Anh ‚Äì ƒê·∫°i h·ªçc Khoa h·ªçc X√£ h·ªôi \u0026amp; Nh√¢n vƒÉn.\nƒêi·ªÉm tuy·ªÉn sinh l·ªõp 10 41,5 ƒëi·ªÉm (THPT Nguy·ªÖn H·ªØu Hu√¢n).\nIELTS 7.5.\nTh√†nh vi√™n ƒë·ªôi tuy·ªÉn Ti·∫øng Anh c·∫•p qu·∫≠n.\nH·ªçc sinh gi·ªèi/xu·∫•t s·∫Øc 12 nƒÉm li·ªÅn\nKinh nghi·ªám gi·∫£ng d·∫°y \u0026amp; l√†m vi·ªác:\n3 nƒÉm kinh nghi·ªám gia s∆∞ ti·∫øng Anh, t·ª´ c∆° b·∫£n ƒë·∫øn n√¢ng cao. Ph∆∞∆°ng ph√°p gi·∫£ng d·∫°y:\nH∆∞·ªõng d·∫´n luy·ªán thi ch·ª©ng ch·ªâ qu·ªëc t·∫ø v√† k·ªπ nƒÉng giao ti·∫øp.\nS·ª≠ d·ª•ng ph∆∞∆°ng ph√°p ph·∫£n x·∫° gi√∫p h·ªçc sinh tƒÉng kh·∫£ nƒÉng s·ª≠ d·ª•ng ti·∫øng Anh t·ª± nhi√™n.\n3.V≈© Quang Long ‚Äì Gia s∆∞ Ti·∫øng Anh (Ph√≥ Hi·ªáu Tr∆∞·ªüng) Th√†nh t√≠ch h·ªçc t·∫≠p:\nSinh vi√™n ng√†nh C√¥ng ngh·ªá ƒëa ph∆∞∆°ng ti·ªán ‚Äì H·ªçc vi·ªán B∆∞u ch√≠nh Vi·ªÖn th√¥ng.\nIELTS 6.0.\nGi·∫£i khuy·∫øn kh√≠ch Ti·∫øng Anh c·∫•p qu·∫≠n l·ªõp 6, 7.\nƒêi·ªÉm tuy·ªÉn sinh l·ªõp 10 nƒÉm 2019: 38 ƒëi·ªÉm (THPT Nguy·ªÖn H·ªØu Hu√¢n).\nKinh nghi·ªám gi·∫£ng d·∫°y \u0026amp; l√†m vi·ªác:\nGia s∆∞ ti·∫øng Anh cho h·ªçc sinh c·∫•p 2, t·∫≠p trung v√†o ng·ªØ ph√°p v√† giao ti·∫øp c∆° b·∫£n. Ph∆∞∆°ng ph√°p gi·∫£ng d·∫°y:\nH∆∞·ªõng d·∫´n h·ªçc sinh c·∫£i thi·ªán t·ª´ v·ª±ng v√† ng·ªØ ph√°p b·∫±ng ph∆∞∆°ng ph√°p th·ª±c h√†nh. 4.Nguy·ªÖn Khoa Nam An ‚Äì Gia s∆∞ To√°n, VƒÉn, Ti·∫øng Anh Th√†nh t√≠ch h·ªçc t·∫≠p:\nH·ªçc sinh gi·ªèi/xu·∫•t s·∫Øc 12 nƒÉm li·ªÅn\nSinh vi√™n ng√†nh Th∆∞∆°ng m·∫°i ƒëi·ªán t·ª≠ ‚Äì ƒê·∫°i h·ªçc S∆∞ ph·∫°m K·ªπ thu·∫≠t.\nIELTS 6.5.\nƒêi·ªÉm tuy·ªÉn sinh l·ªõp 10 41 ƒëi·ªÉm (THPT Nguy·ªÖn H·ªØu Hu√¢n).\nKinh nghi·ªám gi·∫£ng d·∫°y \u0026amp; l√†m vi·ªác:\nTh·ª±c t·∫≠p sinh t·∫°i Amazon Web Services Vi·ªát Nam (AWS), c√≥ n·ªÅn t·∫£ng c√¥ng ngh·ªá v·ªØng ch·∫Øc. Ph∆∞∆°ng ph√°p gi·∫£ng d·∫°y:\nH∆∞·ªõng d·∫´n h·ªçc sinh c√°ch s·ª≠ d·ª•ng ti·∫øng Anh trong m√¥i tr∆∞·ªùng th·ª±c t·∫ø. 5.Nguy·ªÖn T·∫•n L·ª£i ‚Äì Gi√°o vi√™n Ng√¥n ng·ªØ T√¢y Ban Nha \u0026amp; Ti·∫øng Anh Th√†nh t√≠ch h·ªçc t·∫≠p:\nSinh vi√™n ng√†nh Ng√¥n ng·ªØ T√¢y Ban Nha ‚Äì ƒê·∫°i h·ªçc Khoa h·ªçc X√£ h·ªôi \u0026amp; Nh√¢n vƒÉn.\nIELTS 6.5.\nƒêi·ªÉm tuy·ªÉn sinh l·ªõp 10 40 ƒëi·ªÉm (THPT Nguy·ªÖn H·ªØu Hu√¢n).\nKinh nghi·ªám gi·∫£ng d·∫°y \u0026amp; l√†m vi·ªác:\n3 nƒÉm kinh nghi·ªám gia s∆∞ ti·∫øng Anh v√† T√¢y Ban Nha. Ph∆∞∆°ng ph√°p gi·∫£ng d·∫°y:\nK·∫øt h·ª£p gi·∫£ng d·∫°y ng√¥n ng·ªØ v·ªõi vƒÉn h√≥a gi√∫p h·ªçc sinh ti·∫øp c·∫≠n ng√¥n ng·ªØ d·ªÖ d√†ng h∆°n.\nS·ª≠ d·ª•ng ph∆∞∆°ng ph√°p th·ª±c h√†nh gi√∫p h·ªçc sinh c·∫£i thi·ªán nhanh ch√≥ng.\n6.Th√°i Thanh Kha ‚Äì Gia S∆∞ To√°n, L√Ω, H√≥a Th√†nh t√≠ch h·ªçc t·∫≠p:\nC·ª±u sinh vi√™n ƒê·∫°i h·ªçc T√¥n ƒê·ª©c Th·∫Øng.\nSinh vi√™n ng√†nh H·ªá th·ªëng th√¥ng tin qu·∫£n l√Ω ‚Äì ƒê·∫°i h·ªçc T√†i ch√≠nh Marketing.\nƒêi·ªÉm thi ƒë·∫°i h·ªçc 25 ƒëi·ªÉm.\nKinh nghi·ªám gi·∫£ng d·∫°y \u0026amp; l√†m vi·ªác:\nGia s∆∞ To√°n, L√Ω H√≥a cho h·ªçc sinh c·∫•p 2, t·∫≠p trung v√†o ng·ªØ ph√°p v√† giao ti·∫øp c∆° b·∫£n. Ph∆∞∆°ng ph√°p gi·∫£ng d·∫°y:\nK·∫øt h·ª£p To√°n, VƒÉn, Anh ƒë·ªÉ ph√°t tri·ªÉn t∆∞ duy to√†n di·ªán.\nR√®n luy·ªán t∆∞ duy ph·∫£n bi·ªán, k·ªπ nƒÉng gi·∫£i quy·∫øt v·∫•n ƒë·ªÅ v√† ph∆∞∆°ng ph√°p h·ªçc t·∫≠p hi·ªáu qu·∫£.\n7.Nguy·ªÖn ƒê·ª©c Duy ‚Äì Gi√°o vi√™n Ti·∫øng Anh (Ng∆∞·ªùi th·ª´a) Th√†nh t√≠ch h·ªçc t·∫≠p:\nSinh vi√™n ng√†nh Qu·∫£n tr·ªã kinh doanh ‚Äì ƒê·∫°i h·ªçc N√¥ng L√¢m.\nGi·∫£i khuy·∫øn kh√≠ch Ti·∫øng Anh c·∫•p qu·∫≠n l·ªõp 6, 7.\nKinh nghi·ªám gi·∫£ng d·∫°y \u0026amp; l√†m vi·ªác:\nH∆°n 2 nƒÉm kinh nghi·ªám gia s∆∞ ti·∫øng Anh, ƒë·∫∑c bi·ªát l√† h·ªó tr·ª£ h·ªçc sinh m·∫•t g·ªëc. Ph∆∞∆°ng ph√°p gi·∫£ng d·∫°y:\nT·∫≠p trung v√†o k·ªπ nƒÉng ƒë·ªçc-hi·ªÉu v√† luy·ªán thi hi·ªáu qu·∫£.\nS·ª≠ d·ª•ng ph∆∞∆°ng ph√°p th·ª±c h√†nh gi√∫p h·ªçc sinh c·∫£i thi·ªán giao ti·∫øp.\n"
},
{
	"uri": "http://<user_name>.github.io/1-introduce/",
	"title": "Gi·ªõi thi·ªáu v·ªÅ ch√∫ng t√¥i",
	"tags": [],
	"description": "",
	"content": "1. Gi·ªõi thi·ªáu chung Minh Vinh Education l√† m·ªôt t·ªï ch·ª©c gi√°o d·ª•c chuy√™n cung c·∫•p c√°c ch∆∞∆°ng tr√¨nh ƒë√†o t·∫°o ch·∫•t l∆∞·ª£ng cao, gi√∫p h·ªçc sinh ph√°t tri·ªÉn to√†n di·ªán v·ªÅ k·ªπ nƒÉng, t∆∞ duy v√† ki·∫øn th·ª©c chuy√™n m√¥n. V·ªõi ƒë·ªôi ng≈© gi·∫£ng vi√™n t·∫≠n t√¢m, ph∆∞∆°ng ph√°p gi·∫£ng d·∫°y ti√™n ti·∫øn v√† h·ªá th·ªëng h·ªçc t·∫≠p linh ho·∫°t, ch√∫ng t√¥i cam k·∫øt mang l·∫°i gi√° tr·ªã t·ªët nh·∫•t cho h·ªçc vi√™n.\n2. Nh·ªØng g√¨ ch√∫ng t√¥i mang ƒë·∫øn üåü S·ª© M·ªánh\nCung c·∫•p m√¥i tr∆∞·ªùng h·ªçc t·∫≠p hi·ªán ƒë·∫°i, s√°ng t·∫°o v√† hi·ªáu qu·∫£.\nH·ªó tr·ª£ h·ªçc vi√™n ph√°t tri·ªÉn nƒÉng l·ª±c to√†n di·ªán, ·ª©ng d·ª•ng th·ª±c t·∫ø.\nTh√∫c ƒë·∫©y s·ª± ƒë·ªïi m·ªõi trong gi√°o d·ª•c b·∫±ng c√¥ng ngh·ªá v√† ph∆∞∆°ng ph√°p gi·∫£ng d·∫°y ti√™n ti·∫øn.\nüöÄ T·∫ßm Nh√¨n\nTr·ªü th√†nh ƒë∆°n v·ªã h√†ng ƒë·∫ßu trong lƒ©nh v·ª±c gi√°o d·ª•c t·∫°i Vi·ªát Nam v√† v∆∞∆°n ra th·ªã tr∆∞·ªùng qu·ªëc t·∫ø.\nX√¢y d·ª±ng c·ªông ƒë·ªìng h·ªçc t·∫≠p ch·∫•t l∆∞·ª£ng, k·∫øt n·ªëi h·ªçc vi√™n, gi·∫£ng vi√™n v√† doanh nghi·ªáp.\n·ª®ng d·ª•ng c√¥ng ngh·ªá ƒë·ªÉ n√¢ng cao tr·∫£i nghi·ªám gi√°o d·ª•c.\nüèÜ Gi√° Tr·ªã C·ªët L√µi\n‚úÖ Ch·∫•t l∆∞·ª£ng ‚Äì ƒê·∫∑t ti√™u chu·∫©n gi·∫£ng d·∫°y cao nh·∫•t cho t·ª´ng kh√≥a h·ªçc.\n‚úÖ S√°ng t·∫°o ‚Äì Lu√¥n ƒë·ªïi m·ªõi ph∆∞∆°ng ph√°p d·∫°y v√† h·ªçc.\n‚úÖ C·ªông ƒë·ªìng ‚Äì X√¢y d·ª±ng m√¥i tr∆∞·ªùng h·ªçc t·∫≠p th√¢n thi·ªán, k·∫øt n·ªëi v√† h·ªó tr·ª£ l·∫´n nhau.\n‚úÖ ·ª®ng d·ª•ng th·ª±c t·∫ø ‚Äì Ki·∫øn th·ª©c ƒëi ƒë√¥i v·ªõi th·ª±c h√†nh, gi√∫p h·ªçc vi√™n √°p d·ª•ng ngay v√†o c√¥ng vi·ªác.\n3. Ch∆∞∆°ng Tr√¨nh ƒê√†o T·∫°o üìå ƒê·ªëi t∆∞·ª£ng: H·ªçc sinh c·∫•p 2 (l·ªõp 6 ‚Äì l·ªõp 9)\nüìå M·ª•c ti√™u:\n‚úÖ C·ªßng c·ªë n·ªÅn t·∫£ng ki·∫øn th·ª©c theo ch∆∞∆°ng tr√¨nh SGK\n‚úÖ Ph√°t tri·ªÉn t∆∞ duy logic (To√°n), t∆∞ duy ng√¥n ng·ªØ (VƒÉn, Anh)\n‚úÖ R√®n luy·ªán k·ªπ nƒÉng l√†m b√†i thi hi·ªáu qu·∫£\n‚úÖ T·∫°o ƒë·ªông l·ª±c h·ªçc t·∫≠p v√† ph√°t tri·ªÉn k·ªπ nƒÉng t·ª± h·ªçc\n1Ô∏è‚É£ M√¥n To√°n ‚Äì R√®n luy·ªán t∆∞ duy logic v√† gi·∫£i quy·∫øt v·∫•n ƒë·ªÅ\nüîπ L·ªõp 6 ‚Äì 7: S·ªë h·ªçc, ph√¢n s·ªë, h√¨nh h·ªçc c∆° b·∫£n, ƒë·∫°i s·ªë s∆° c·∫•p\nüîπ L·ªõp 8 ‚Äì 9: Ph∆∞∆°ng tr√¨nh, b·∫•t ph∆∞∆°ng tr√¨nh, h√¨nh h·ªçc kh√¥ng gian, ƒë·∫°i s·ªë n√¢ng cao\nüîπ Ph∆∞∆°ng ph√°p: H·ªçc l√Ω thuy·∫øt ‚Äì b√†i t·∫≠p th·ª±c h√†nh ‚Äì b√†i ki·ªÉm tra ƒë·ªãnh k·ª≥\n2Ô∏è‚É£ M√¥n VƒÉn ‚Äì Ph√°t tri·ªÉn k·ªπ nƒÉng ƒë·ªçc hi·ªÉu v√† di·ªÖn ƒë·∫°t\nüîπ L·ªõp 6 ‚Äì 7: T·∫≠p l√†m vƒÉn (t·∫£, k·ªÉ, bi·ªÉu c·∫£m), ƒë·ªçc hi·ªÉu, ng·ªØ ph√°p c∆° b·∫£n\nüîπ L·ªõp 8 ‚Äì 9: VƒÉn ngh·ªã lu·∫≠n, ƒë·ªçc hi·ªÉu n√¢ng cao, luy·ªán thi v√†o l·ªõp 10\nüîπ Ph∆∞∆°ng ph√°p: Ph√¢n t√≠ch t√°c ph·∫©m ‚Äì luy·ªán vi·∫øt b√†i ‚Äì th·∫£o lu·∫≠n nh√≥m\n3Ô∏è‚É£ M√¥n Ti·∫øng Anh ‚Äì Giao ti·∫øp v√† ng·ªØ ph√°p v·ªØng ch·∫Øc\nüîπ L·ªõp 6 ‚Äì 7: T·ª´ v·ª±ng theo ch·ªß ƒë·ªÅ, ng·ªØ ph√°p n·ªÅn t·∫£ng, luy·ªán nghe n√≥i c∆° b·∫£n\nüîπ L·ªõp 8 ‚Äì 9: Ng·ªØ ph√°p n√¢ng cao, vi·∫øt lu·∫≠n, luy·ªán thi v√†o l·ªõp 10\nüîπ Ph∆∞∆°ng ph√°p: H·ªçc qua h·ªôi tho·∫°i ‚Äì b√†i t·∫≠p t√¨nh hu·ªëng ‚Äì tr√≤ ch∆°i t∆∞∆°ng t√°c\nüìÖ L·ªãch h·ªçc g·ª£i √Ω:\n2 bu·ªïi/tu·∫ßn/m√¥n ho·∫∑c 3 m√¥n k·∫øt h·ª£p trong 1 tu·∫ßn\nK·∫øt h·ª£p h·ªçc offline \u0026amp; online t√πy nhu c·∫ßu h·ªçc sinh\nLuy·ªán ƒë·ªÅ ki·ªÉm tra ƒë·ªãnh k·ª≥ theo l·ªô tr√¨nh\nüéØ Cam k·∫øt:\n‚úîÔ∏è Ph∆∞∆°ng ph√°p d·∫°y d·ªÖ hi·ªÉu, khoa h·ªçc\n‚úîÔ∏è H·ªçc sinh ti·∫øn b·ªô r√µ r·ªát qua t·ª´ng giai ƒëo·∫°n\n‚úîÔ∏è Gi√°o vi√™n t·∫≠n t√¢m, h·ªó tr·ª£ h·ªçc sinh v∆∞·ª£t qua kh√≥ khƒÉn\nüìû Li√™n H·ªá Ngay ƒë·ªÉ nh·∫≠n t∆∞ v·∫•n chi ti·∫øt v√† b·∫Øt ƒë·∫ßu h√†nh tr√¨nh h·ªçc t·∫≠p c√πng ch√∫ng t√¥i!\n"
},
{
	"uri": "http://<user_name>.github.io/2-prerequiste/2.1-createec2/",
	"title": "L·ªõp 6",
	"tags": [],
	"description": "",
	"content": "If you\u0026rsquo;re participating in these labs as part of an AWS-led event in Workshop Studio, there\u0026rsquo;s no need to manually provision any resources‚Äîeverything required for all modules will be pre-provisioned for you.\nWhat will be set up in your AWS account? The stack will automatically configure the following components:\nEKS Cluster: Creates a new EKS cluster named opea-eks-cluster. Deploys a node within the cluster using a M7i.24xlarge instance for high-performance computing. CloudFormation Templates: The stack also generates CloudFormation templates for each module:\nModule 1: ChatQnA Default\nModule 2: ChatQnA with Guardrails\nModule 3: ChatQnA with OpenSearch (open-source) as the vector database\nModule 4: ChatQnA with Bedrock as the LLM\nModule 5: ChatQnA with Remote Inference (Denvr) as the LLM\nWith these pre-configured resources, you can focus entirely on exploring and building your Generative AI applications without worrying about infrastructure setup.\nStep 1: Configure Access to Your EKS Cluster To interact with your EKS cluster using kubectl, you need to configure your local environment to recognize the cluster. This is done by updating the kubeconfig file, which stores authentication details and access configurations for your Kubernetes cluster.\nLog in to the AWS Management Console: Start by signing into your AWS Management Console. Open Cloud Shell or Set Up Your Local Environment: In the console, click the Cloud Shell icon to launch a preconfigured terminal. Alternatively, if you prefer to use your own AWS CLI, ensure you have both the AWS CLI Client and kubectl installed on your local machine.\nUpdate Your kubeconfig: You should receive an output confirming your conf file was updated: You are now ready to interact with the Kubernetes cluster using kubectl Step 2: Verify Cluster Access After updating your kubeconfig, verify that you can successfully connect to the cluster by listing the nodes: If the command executes successfully, you should see an output displaying the nodes associated with your cluster. You are now ready to explore the module of your choice and begin deploying workloads on your EKS cluster! "
},
{
	"uri": "http://<user_name>.github.io/3-accessibilitytoinstances/3.1-public-instance/",
	"title": "L·ªõp 6",
	"tags": [],
	"description": "",
	"content": "Introduction In this section, you will deploy the OPEA blueprint for a RAG-based application, ChatQnA, on an Amazon Elastic Kubernetes Service (EKS) environment. This hands-on exploration will enhance your understanding of how the RAG application functions within a managed Kubernetes ecosystem, allowing you to analyze its components and their roles in the system.\nThe deployment configuration is available through the AWS Marketplace, and ChatQnA can be found in OPEA\u0026rsquo;s GenAIExamples repository.\nChatQnA on GitHub\nSince you have already set up access to your Kubernetes cluster in the \u0026ldquo;Getting Set Up\u0026rdquo; section, you will now dive deeper into your deployed environment to explore and learn more about its structure and functionality.\nStep 1: Deploy the ChatQnA CloudFormation Template Open AWS CloudShell and deploy the ChatQnA CloudFormation template into your Amazon EKS cluster. This will initiate the deployment of the RAG application within your managed Kubernetes environment. Step 2: Explore Cluster Resources Navigate to the AWS Management Console and select your assigned EKS cluster to review its deployment. Each cluster includes critical configurations such as the Kubernetes version, networking setup, and logging options. Examining these settings will provide a deeper understanding of your application‚Äôs infrastructure, helping with efficient management and troubleshooting when necessary.\nReviewing Cluster Resources Click on the Resources tab to view all applications currently running within your cluster, including ChatQnA and its associated microservices. Ensure that all microservices from the OPEA ChatQnA blueprint are installed correctly by listing the active pods: The output should show all pods in a Running state (1/1), confirming that the application has been successfully deployed. At this point, you are ready to further explore the deployment and manage your resources within the cluster. "
},
{
	"uri": "http://<user_name>.github.io/4-s3log/4.1-updateiamrole/",
	"title": "L·ªõp 6",
	"tags": [],
	"description": "",
	"content": "What is Amazon Bedrock? Amazon Bedrock is a fully managed service that provides access to a diverse selection of high-performance foundation models (FMs) from industry leaders such as AI21 Labs, Anthropic, Cohere, Meta, Mistral AI, Stability AI, and Amazon itself. Through a single API, Bedrock enables developers to build generative AI applications while ensuring security, privacy, and responsible AI practices.\nDevelopers can interact with Amazon Bedrock via the AWS Software Development Kit (SDK) or the AWS Command Line Interface (CLI). Bedrock also offers native features that allow users to create RAG (Retrieval-Augmented Generation) knowledge bases, agentic workflows, and guardrails. Integrating Bedrock with OPEA expands access to a broader selection of foundation models while leveraging Bedrock‚Äôs advanced capabilities alongside OPEA.\nHow Does the Architecture Change?\nFuture integrations with OPEA will unlock the full potential of Amazon Bedrock‚Äôs capabilities, including Titan Embedding models. However, for this module, the focus is exclusively on LLMs.\nThanks to OPEA‚Äôs modular and interchangeable architecture, most components from the default ChatQnA setup (Module 1) remain unchanged. The TGI service (Hugging Face) is now replaced by a Bedrock container, which seamlessly integrates into the existing deployment.\nUpdated Architecture Component:\nchatqna-bedrock In this deployment, when a user sends a message through the ChatQnA UI, it is routed to the backend Bedrock container, which communicates with Amazon Bedrock to retrieve and return responses. This integration maintains the ChatQnA architecture while enhancing it with Amazon Bedrock‚Äôs powerful LLM capabilities, ensuring scalability, efficiency, and seamless deployment.\nWe\u0026rsquo;ve used the bedrock Kubernetes namespace to separate out the pods and services pertaining to the Bedrock deployment. When you use kubectl and other Kubernetes commands in the below examples, be sure to qualify the command with -n bedrock.\nDeploying ChatQnA Using Amazon Bedrock LLMs\nFor this lab, we\u0026rsquo;ve created a changeset with the full parallel deployment of the ChatQnA example in the same Kubernetes cluster you\u0026rsquo;ve been using. The following command will deploy pods to the cluster within the \u0026ldquo;bedrock\u0026rdquo; namespace that are identical to the original ChatQnA pods, except with Bedrock models instead of TGI.\naws cloudformation execute-change-set \u0026ndash;change-set-name bedrock-change-set \u0026ndash;stack-name OpeaBedrockStack\nActivating the Model\nThis module works with just about any text-generation LLM supported by Bedrock, but for the purposes of this lab we\u0026rsquo;ve used the Anthropic Claude Haiku 3 model. So while you\u0026rsquo;re waiting for the change set to deploy, let\u0026rsquo;s go activate our model in the Bedrock console:\nSwitch to the us-west-2 region, you could test on other regions but usually us-west has more availabity: Go to Amazon Bedrock: Go to the model access tab: At the top of the screen, click on the button that says Modify Model Access Select Claude 3 Haiku It may take a minute or two for the access to be granted, but don\u0026rsquo;t worry it won\u0026rsquo;t take much longer than that.\nOnce you\u0026rsquo;ve confirmed that model access has been granted, switch back to the us-east-2 region where your EKS cluster is located. Confirming Deployment\nNow let\u0026rsquo;s confirm that our Bedrock deployment is complete. You can onitor the state of the Bedrock pods using the kubectl command:\n\u0026hellip;to get output like this:\nIt can take several minutes for Bedrock to fully initialize and be available. Only continue when you see the chatqna-bedrock-deployment pod in the Running state.\nYou are now able to use Amazon Bedrock in your environment.\n"
},
{
	"uri": "http://<user_name>.github.io/",
	"title": "Minh Vinh Education",
	"tags": [],
	"description": "",
	"content": "H·ªçc, H·ªçc N·ªØa, H·ªçc M√£i "
},
{
	"uri": "http://<user_name>.github.io/2-prerequiste/",
	"title": "B√†i Gi·∫£ng To√°n ",
	"tags": [],
	"description": "",
	"content": "L·ªùi n√≥i ƒë·∫ßu To√°n h·ªçc l√† n·ªÅn t·∫£ng c·ªßa t∆∞ duy logic, l√† ch√¨a kh√≥a m·ªü ra kh·∫£ nƒÉng gi·∫£i quy·∫øt v·∫•n ƒë·ªÅ m·ªôt c√°ch s√°ng t·∫°o v√† hi·ªáu qu·∫£. Kh√¥ng ch·ªâ l√† m·ªôt m√¥n h·ªçc tr√™n l·ªõp, To√°n c√≤n l√† c√¥ng c·ª• gi√∫p con ng∆∞·ªùi hi·ªÉu s√¢u h∆°n v·ªÅ th·∫ø gi·ªõi xung quanh, t·ª´ nh·ªØng ph√©p t√≠nh ƒë∆°n gi·∫£n trong cu·ªôc s·ªëng h·∫±ng ng√†y ƒë·∫øn nh·ªØng ·ª©ng d·ª•ng ph·ª©c t·∫°p trong khoa h·ªçc, k·ªπ thu·∫≠t v√† kinh t·∫ø.\n·ªû b·∫≠c trung h·ªçc c∆° s·ªü, To√°n h·ªçc ƒë√≥ng vai tr√≤ quan tr·ªçng trong vi·ªác h√¨nh th√†nh t∆∞ duy ph√¢n t√≠ch v√† suy lu·∫≠n c·ªßa h·ªçc sinh. ƒê√¢y l√† giai ƒëo·∫°n gi√∫p c√°c em x√¢y d·ª±ng n·ªÅn m√≥ng v·ªØng ch·∫Øc cho nh·ªØng b·∫≠c h·ªçc cao h∆°n, ƒë·ªìng th·ªùi t·∫°o c∆° h·ªôi ƒë·ªÉ r√®n luy·ªán s·ª± nh·∫°y b√©n trong c√°ch ti·∫øp c·∫≠n v√† gi·∫£i quy·∫øt v·∫•n ƒë·ªÅ. Ch∆∞∆°ng tr√¨nh To√°n trung h·ªçc c∆° s·ªü kh√¥ng ch·ªâ gi√∫p h·ªçc sinh ti·∫øp thu nh·ªØng ki·∫øn th·ª©c c∆° b·∫£n m√† c√≤n m·ªü r·ªông ƒë·∫øn nh·ªØng b√†i to√°n n√¢ng cao, th·ª≠ th√°ch kh·∫£ nƒÉng t∆∞ duy c·ªßa c√°c em.\nVi·ªác h·ªçc To√°n kh√¥ng ƒë∆°n thu·∫ßn ch·ªâ l√† ghi nh·ªõ c√¥ng th·ª©c v√† l√†m b√†i t·∫≠p. H·ªçc sinh c·∫ßn ƒë∆∞·ª£c h∆∞·ªõng d·∫´n c√°ch t∆∞ duy linh ho·∫°t, bi·∫øt c√°ch ph√¢n t√≠ch b√†i to√°n, t√¨m ra m·ªëi li√™n h·ªá gi·ªØa c√°c kh√°i ni·ªám v√† v·∫≠n d·ª•ng ch√∫ng v√†o th·ª±c t·∫ø. M·ªôt ph∆∞∆°ng ph√°p h·ªçc t·∫≠p hi·ªáu qu·∫£ s·∫Ω kh√¥ng ch·ªâ gi√∫p h·ªçc sinh ƒë·∫°t ƒëi·ªÉm cao m√† c√≤n ph√°t tri·ªÉn kh·∫£ nƒÉng suy nghƒ© ch·∫∑t ch·∫Ω, m·∫°ch l·∫°c, ·ª©ng d·ª•ng ki·∫øn th·ª©c m·ªôt c√°ch s√°ng t·∫°o v√†o nhi·ªÅu lƒ©nh v·ª±c kh√°c nhau.\nCh∆∞∆°ng tr√¨nh gi·∫£ng d·∫°y c·ªßa trung t√¢m kh√¥ng ch·ªâ t·∫≠p trung v√†o vi·ªác gi√∫p h·ªçc sinh n·∫Øm v·ªØng ki·∫øn th·ª©c trong s√°ch gi√°o khoa, m√† c√≤n h∆∞·ªõng ƒë·∫øn vi·ªác ph√°t tri·ªÉn k·ªπ nƒÉng l√†m b√†i thi, t∆∞ duy ph·∫£n bi·ªán v√† kh·∫£ nƒÉng gi·∫£i quy·∫øt c√°c b√†i to√°n th·ª±c t·∫ø. M·ªói chuy√™n ƒë·ªÅ ƒë∆∞·ª£c thi·∫øt k·∫ø khoa h·ªçc, ph√π h·ª£p v·ªõi t·ª´ng c·∫•p ƒë·ªô, ƒë·∫£m b·∫£o h·ªçc sinh kh√¥ng ch·ªâ hi·ªÉu b√†i m√† c√≤n c√≥ th·ªÉ v·∫≠n d·ª•ng linh ho·∫°t trong c√°c t√¨nh hu·ªëng kh√°c nhau.\nB√™n c·∫°nh ƒë√≥, vi·ªác √¥n t·∫≠p v√† h·ªá th·ªëng h√≥a ki·∫øn th·ª©c m·ªôt c√°ch b√†i b·∫£n l√† y·∫øu t·ªë quan tr·ªçng gi√∫p h·ªçc sinh c√≥ s·ª± chu·∫©n b·ªã t·ªët nh·∫•t cho c√°c k·ª≥ thi quan tr·ªçng, ƒë·∫∑c bi·ªát l√† k·ª≥ thi tuy·ªÉn sinh v√†o l·ªõp 10. V·ªõi ph∆∞∆°ng ph√°p gi·∫£ng d·∫°y s√°ng t·∫°o, ƒë·ªôi ng≈© gi√°o vi√™n t·∫≠n t√¢m v√† ngu·ªìn t√†i li·ªáu phong ph√∫, trung t√¢m cam k·∫øt gi√∫p h·ªçc sinh kh√¥ng ch·ªâ chinh ph·ª•c m√¥n To√°n m·ªôt c√°ch d·ªÖ d√†ng m√† c√≤n nu√¥i d∆∞·ª°ng ni·ªÅm ƒëam m√™, h·ª©ng th√∫ v·ªõi b·ªô m√¥n n√†y.\nH·ªçc To√°n kh√¥ng ph·∫£i l√† m·ªôt h√†nh tr√¨nh ƒë∆°n ƒë·ªôc. V·ªõi s·ª± ƒë·ªìng h√†nh c·ªßa th·∫ßy c√¥, tr·ª£ gi·∫£ng v√† m·ªôt l·ªô tr√¨nh h·ªçc t·∫≠p r√µ r√†ng, m·ªói h·ªçc sinh s·∫Ω t√¨m th·∫•y ph∆∞∆°ng ph√°p h·ªçc ph√π h·ª£p v·ªõi b·∫£n th√¢n, ph√°t huy t·ªëi ƒëa ti·ªÅm nƒÉng v√† ƒë·∫°t ƒë∆∞·ª£c th√†nh t√≠ch t·ªët nh·∫•t!\nüîπ M·ª•c ti√™u c·ªßa ch∆∞∆°ng tr√¨nh:\n‚úî Cung c·∫•p ki·∫øn th·ª©c n·ªÅn t·∫£ng v·ªØng ch·∫Øc, gi√∫p h·ªçc sinh t·ª± tin trong h·ªçc t·∫≠p v√† thi c·ª≠.\n‚úî Ph√°t tri·ªÉn t∆∞ duy logic v√† k·ªπ nƒÉng gi·∫£i quy·∫øt v·∫•n ƒë·ªÅ th√¥ng qua c√°c b√†i t·∫≠p ƒëa d·∫°ng.\n‚úî H·ªó tr·ª£ h·ªçc sinh trong vi·ªác √¥n luy·ªán, ƒë·ªãnh h∆∞·ªõng ph∆∞∆°ng ph√°p h·ªçc t·∫≠p hi·ªáu qu·∫£ v√† ƒë·∫°t ƒëi·ªÉm s·ªë cao trong c√°c k·ª≥ thi quan tr·ªçng.\n"
},
{
	"uri": "http://<user_name>.github.io/1-introduce/2.1-createec2-copy/",
	"title": "ƒê·ªôi ng≈© tr·ª£ gi·∫£ng",
	"tags": [],
	"description": "",
	"content": "1.ƒê·∫∑ng Th√°i Vinh ‚Äì Tr·ª£ gi·∫£ng To√°n Th√†nh t√≠ch h·ªçc t·∫≠p:\n9 nƒÉm li·ªÅn h·ªçc sinh gi·ªèi/xu·∫•t s·∫Øc.\n2 gi·∫£i khuy·∫øn kh√≠ch h·ªçc sinh gi·ªèi To√°n TP. Th·ªß ƒê·ª©c l·ªõp 6, 7.\nGi·∫£i Nh√¨ h·ªçc sinh gi·ªèi To√°n TP. Th·ªß ƒê·ª©c l·ªõp 8.\nHuy ch∆∞∆°ng ƒê·ªìng cu·ªôc thi VƒÉn - To√°n Tu·ªïi Th∆° to√†n qu·ªëc.\nGi·∫£i khuy·∫øn kh√≠ch To√°n qu·ªëc t·∫ø SASMO.\nKinh nghi·ªám h·ªó tr·ª£ gi·∫£ng d·∫°y:\nH·ªó tr·ª£ h·ªçc sinh gi·∫£i b√†i t·∫≠p kh√≥, n√¢ng cao t∆∞ duy To√°n h·ªçc.\nH∆∞·ªõng d·∫´n ph∆∞∆°ng ph√°p h·ªçc t·∫≠p v√† √¥n luy·ªán c√°c k·ª≥ thi To√°n h·ªçc sinh gi·ªèi.\nPh∆∞∆°ng ph√°p tr·ª£ gi·∫£ng:\nGi√∫p h·ªçc sinh hi·ªÉu r√µ b·∫£n ch·∫•t b√†i to√°n th√¥ng qua c√°ch ti·∫øp c·∫≠n logic.\n·ª®ng d·ª•ng ph∆∞∆°ng ph√°p t∆∞ duy h√¨nh ·∫£nh v√† b√†i t·∫≠p th·ª±c h√†nh ƒëa d·∫°ng ƒë·ªÉ tƒÉng kh·∫£ nƒÉng ti·∫øp thu.\n"
},
{
	"uri": "http://<user_name>.github.io/2-prerequiste/2.2-createiamrole/",
	"title": "L·ªõp 7",
	"tags": [],
	"description": "",
	"content": "Preparing Your AWS Environment for the Workshop If you are not participating in an AWS-led event via Workshop Studio, you must set up your environment before proceeding with these labs. Keep in mind that the resources you create will incur costs, so be sure to clean them up once you have completed the workshop.\nStep 1: Configuring Your Environment To set up your AWS account, follow these steps:\nClick Launch Stack below to initiate AWS CloudFormation with pre-configured values in the us-east-1 region.\nLaunch Stack\nIf you prefer a different AWS region, remember to change the region accordingly. The CloudFormation stack will provision the following resources:\nEKS Cluster Deployment\n-Creates a new EKS cluster named opea-eks-cluster.\n-Deploys a worker node within the cluster using an M7i.24xlarge instance.\nCloudFormation Templates for Workshop Modules The stack also includes templates for the following modules:\nModule 1: ChatQnA Default\nModule 2: ChatQnA with Guardrails\nModule 3: ChatQnA with OpenSearch (open-source) as the vector database\nModule 4: ChatQnA with Remote Inference (Denvr) as the LLM\nModule 5: ChatQnA with Bedrock as the LLM\nConfiguring Stack Parameters Before launching the stack, review and configure the following parameters:\nFor Modules 1, 2, and 3:\nHuggingFaceToken: This token is required to download models from Hugging Face. If you plan to use the Guardrails feature, ensure your token has access to the meta-llama/Meta-Llama-Guard-2-8B model.\nModelID: The OPEA system primarily uses the Text Generation Inference toolkit. Select any compatible model from Hugging Face and provide its model ID.\nOpeaRoleArn: Enter the ARN or name of the IAM role associated with your AWS account.\nIf you are unsure, check the user information displayed in the top-right corner of the AWS Management Console. If your username does not contain a forward slash (/), copy the full name. If it does contain a forward slash, only use the portion before the slash. Example:\nIf the display name is \u0026ldquo;USER\u0026rdquo;, enter \u0026ldquo;USER\u0026rdquo;. If the display name is \u0026ldquo;ADMIN-ROLE/USER\u0026rdquo;, enter \u0026ldquo;ADMIN-ROLE\u0026rdquo;. Alternatively, you can retrieve your ARN using the AWS CLI. aws sts get-caller-identity \u0026ndash;query \u0026lsquo;Arn\u0026rsquo; \u0026ndash;output text | awk -F: \u0026lsquo;{print $NF}\u0026rsquo; | (read id; if [[ $id == \u0026ldquo;user\u0026rdquo; ]]; then aws sts get-caller-identity \u0026ndash;query \u0026lsquo;Arn\u0026rsquo; \u0026ndash;output text; else role=$(echo $id | cut -d\u0026rsquo;/\u0026rsquo; -f2); aws iam get-role \u0026ndash;role-name $role \u0026ndash;query \u0026lsquo;Role.Arn\u0026rsquo; \u0026ndash;output text; fi)\nMark the checkbox next to I acknowledge that AWS CloudFormation might create IAM resources\nDeploying the CloudFormation Stack Once the parameters are set, click Create Stack.\nThis process will initiate an AWS CodeBuild Project, which pulls in the opea-demo-builder open-source library.\nThe AWS Cloud Development Kit (CDK) will generate the necessary CloudFormation templates to configure your AWS environment for the workshop.\nMonitoring Deployment Progress The deployment process takes approximately 25 minutes. Open the AWS CloudFormation Console and monitor the progress.\nEnsure that the following stacks reach the CREATE_COMPLETE status:\nThe EKS cluster The ChatQnA application (default deployment) Some stacks may remain in REVIEW_IN_PROGRESS, as they will be deployed onto the EKS cluster later.\nStep 2: Configuring Access to Your EKS Cluster Once the CloudFormation stacks are successfully deployed, you need to configure your local environment to interact with the Amazon EKS cluster using kubectl.\nUpdating Your Kubernetes Configuration (kubeconfig)\nOpen AWS CloudShell Click the CloudShell icon in the AWS Management Console. Alternatively, use your local AWS CLI, ensuring kubectl and the AWS CLI client are installed on your system. Update kubeconfig Run the following command (updating the region if necessary): If successful, you should receive a confirmation message indicating that your configuration file has been updated. Verify Connectivity to Your EKS Cluster Ensure you can interact with the cluster using kubectl. Step 3: Verifying EKS Cluster Access After updating your kubeconfig, test whether you can connect to the cluster. If you encounter issues accessing pods via the AWS console or CloudShell, follow these steps:\nCheck Your IAM Access in the EKS Console\nNavigate to the EKS Console. Look for your IAM principal ARN under Access Entries. If Your ARN Is Not Listed, Add It Manually\nClick Create Access Entry. Enter your IAM User ARN or Role ARN. Attach Required Policies Assign the following permissions to your IAM role:\nAmazonEKSAdminPolicy AmazonEKSClusterAdminPolicy Confirm Access by Listing Nodes\nRun the following command to check if worker nodes are visible: If the command returns a list of nodes, your cluster is successfully configured. Note: If no nodes appear, wait a few minutes and retry, as node provisioning may still be in progress. Next Steps: Explore the Workshop Modules Once you have successfully connected to your EKS cluster, you are ready to proceed with any of the workshop modules. Choose the module that best fits your learning objectives and begin your hands-on experience with AWS, Kubernetes, and AI-powered solutions.\n"
},
{
	"uri": "http://<user_name>.github.io/3-accessibilitytoinstances/3.2-private-instance/",
	"title": "L·ªõp 7",
	"tags": [],
	"description": "",
	"content": "Exploring the OPEA Microservices Deployment Now, let\u0026rsquo;s dive into the OPEA ChatQnA RAG deployment. As a microservices-based blueprint, it is designed for scalability, resilience, and flexibility. In this task, you will explore each microservice to understand its role within the overall system. By examining these components, you will gain insights into how they interact and contribute to the application\u0026rsquo;s functionality.\nThis architecture offers several key advantages:\nScalability ‚Äì Each microservice can scale independently based on demand, ensuring optimal resource utilization and performance.\nFault Isolation ‚Äì If one service encounters an issue, it won‚Äôt disrupt the entire system, enhancing reliability.\nEfficient Maintenance \u0026amp; Updates ‚Äì Microservices allow for rapid updates and easy adaptability to evolving business needs and user demands.\nOPEA Microservices Architecture OPEA deployments are built around three key components:\nMegaservice ‚Äì Acts as the orchestrator for all microservices, managing workflows and ensuring seamless interaction between components. This is essential for coordinating an end-to-end application with multiple moving parts. More details can be found in the OPEA documentation.\nGateway ‚Äì Serves as the entry point for users, routing incoming requests to the appropriate microservices within the megaservice architecture. It ensures seamless connectivity between external users and internal components.\nMicroservices ‚Äì These are the individual functional components of the application, handling tasks such as embeddings, retrieval, LLM processing, and vector database interactions. Accessing the Microservices\nBefore you begin exploring, note that only the gateway and UI services are exposed externally. In this task, you will directly access each internal microservice for testing purposes, using the Nginx gateway to efficiently route requests to these internal services.\nYou\u0026rsquo;ll need to take note of all pods deployed.\nkubectl get svc lists all services in a Kubernetes cluster, showing their names, types, cluster IPs, and exposed ports. It provides an overview of how applications are exposed for internal or external access.\nRun the following command on your CloudShell:\nYou will see output similar to this:\nThe kubectl get svc command is used to list the services running within a Kubernetes cluster. Services act as entry points that enable communication between different components of your application. Each service has a unique name (e.g., chatqna or chatqna-ui), which helps identify its role within the system.\nKubernetes services can be exposed in different ways:\nClusterIP ‚Äì Only accessible within the cluster, allowing internal components to communicate securely.\nNodePort ‚Äì Exposes the service externally through a specific port on each node, making it accessible outside the cluster. The Cluster-IP is the internal address used by other services to reach the application. If the service were accessible from outside the cluster, an External-IP would be displayed. However, in this case, these services are strictly internal.\nThe Ports column indicates which network ports the service listens on. For example:\nchatqna might be running on port 8888/TCP, handling internal communication.\nchatqna-nginx could be configured with 80:30144/TCP, where traffic from port 80 is forwarded to 30144 for routing purposes. Lastly, the Age column displays how long the service has been running‚Äîfor instance, 12 hours for all listed services in this scenario.\nNow, let‚Äôs explore the architecture in detail.\nStep 1 : Megaservice (Orchestrator) (POD:chatqna:8888) The megaservice encapsulates the complete logic for the ChatQnA RAG application. This microservice is tasked with processing incoming requests and executing all the necessary internal operations to generate appropriate responses.\nThis service isn\u0026rsquo;t directly exposed, but you can access it directly from the LoadBalancer, which forwards the request.\nLook for the load balancer Click on chatqna-Ingress Note the DNS Name.As mentioned, it\u0026rsquo;s the public URL that can be accessed externally. You will use the curl command to send requests to the API endpoints, testing each microservice individually. The goal is to ask a question, such as \u0026ldquo;What was Nike\u0026rsquo;s revenue in 2023?\u0026rdquo;, and verify that the API responds correctly. This step ensures that all microservices in the system are functioning as expected.\nIf everything is working properly, you should receive a response, confirming that the Retrieval-Augmented Generation (RAG) workflow is operational.\nHowever, you may notice that the model is unable to provide an accurate answer. This happens because it lacks the necessary context and relies on outdated information. Without access to current and relevant data, the model cannot generate precise responses. In the next steps, you will enhance the system using RAG, allowing the model to retrieve up-to-date, contextually relevant information. This will ensure that it delivers more accurate and meaningful answers.\nNow, let\u0026rsquo;s explore each microservice in detail to understand its role and how it contributes to improving the model\u0026rsquo;s ability to answer questions correctly.\nStep 2 : Microservices Each microservice follows the following logic performing a task within the RAG flow:\nIn the flow, you can observe the microservices and we can divide the RAG flow into two steps:\nPreprompting: This step involves preparing the knowledge base (KB) by uploading relevant documents and ensuring that the information is organized for effective retrieval.\nPrompting: This step focuses on retrieving the relevant data from the knowledge base and using it to generate an accurate answer to the user\u0026rsquo;s question.\nPreprompting In this step, the logic is to start from a document (Nike\u0026rsquo;s revenue PDF), and do the preprocessing needed to make it ready to be stored in a database. As shown, this process primarily involves 3 microservices: data preparation, embeddings and vector store. Let\u0026rsquo;s explore each microservice\nEmbedding Microservice (POD: chatqna-tei:80) An embedding is a numerical representation of an object‚Äîsuch as a word, phrase, or document‚Äîwithin a continuous vector space. In natural language processing (NLP), embeddings transform words, sentences, or text segments into vectors‚Äîsets of numbers that capture their meaning, relationships, and contextual significance. This transformation enables machine learning models to process and understand text more effectively.\nFor example, word embeddings represent words as points in a vector space, where words with similar meanings‚Äîlike \u0026ldquo;king\u0026rdquo; and \u0026ldquo;queen\u0026rdquo;‚Äîare positioned closer together. The embedding model captures these relationships through vector arithmetic.\nDuring training, if the model frequently encounters \u0026ldquo;king\u0026rdquo; in association with \u0026ldquo;man\u0026rdquo; and \u0026ldquo;queen\u0026rdquo; with \u0026ldquo;woman,\u0026rdquo; it learns that \u0026ldquo;king\u0026rdquo; and \u0026ldquo;queen\u0026rdquo; share a similar relationship to \u0026ldquo;man\u0026rdquo; and \u0026ldquo;woman.\u0026rdquo; This allows the model to position words in a way that reflects meaningful relationships, such as gender associations, in language.\nEmbeddings: A Key Component of RAG Embeddings play a crucial role in Retrieval-Augmented Generation (RAG) by enhancing the model‚Äôs ability to process and retrieve relevant information. They provide several key advantages:\nCapturing Meaning ‚Äì Embeddings represent the semantic relationships between words, enabling RAG models to understand context, nuances, and deeper language structures. This improves their ability to generate relevant and coherent responses.\nDimensionality Reduction ‚Äì By transforming complex textual data into fixed-size vectors, embeddings make data processing more efficient and scalable, improving the system\u0026rsquo;s performance.\nEnhancing Model Performance ‚Äì By leveraging semantic similarities, embeddings enable more accurate information retrieval, refining the quality of generated responses and helping the model generalize better across various queries.\nOPEA offers multiple options for running embedding microservices, as detailed in the OPEA embedding documentation. In this case, ChatQnA uses the Hugging Face TEI microservice, which runs the embedding model BAAI/bge-large-en-v1.5 locally.\nSince some microservices are not exposed externally, you will use the Nginx pod to interact with them via curl. To do this, each microservice will be accessed using its internal DNS name.\nAccess to ngnix POD (copy your NGNIX entire pod name from kubectl get pods and REPLACE chatqna-nginx-xxxxxxxx on the below command) Your command prompt should now indicate that you are inside the container, reflecting the change in environment:\nOnce inside, you will now have direct access to the internal pods.\nGet the embedding from the Embeddings Microservice for the phrase \u0026ldquo;What was Deep Learning?\u0026rdquo;: The answer will be the vector representation of the phrase \u0026ldquo;What was Deep Learning?\u0026rdquo;. This service returns the vector embedding for the inputs from the REST API.\nVector Database Microservice (POD: chatqna-redis-vector-db:80) The Vector Database microservice plays a critical role in the Retrieval-Augmented Generation (RAG) application by storing and retrieving embeddings. This is essential for applications like ChatQnA, where relevant information needs to be efficiently retrieved based on a user\u0026rsquo;s query.\nUsing Redis as a Vector Database In this task, Redis is used as the vector database. However, OPEA supports multiple alternatives, which can be found in the OPEA vector store repository.\nA Vector Database (VDB) is specifically designed to store and manage high-dimensional vectors, which represent words, sentences, or images in numerical form. In AI and machine learning, these vectors‚Äîalso known as embeddings‚Äîcapture the meaning and relationships between data points, enabling efficient processing and retrieval.\nData Preparation Microservice (POD: chatqna-data-prep:6007) The Data Preparation (Dataprep) Microservice is responsible for formatting and preprocessing data so that it can be converted into embeddings and stored in the vector database. This ensures that the data is clean, structured, and ready for efficient retrieval.\nKey Functions of the Data Preparation Microservice Receives raw data (e.g., documents or reports).\nProcesses and chunks the data into smaller segments.\nSends the processed data to the Embedding Microservice for vectorization.\nStores the resulting embeddings in the Vector Database. Since different vector databases have unique data formatting requirements, the Dataprep Microservice ensures compatibility with the selected database.\nTesting the Microservices To verify the functionality of the system and help the model answer the initial question‚Äî \u0026ldquo;What was Nike\u0026rsquo;s revenue in 2023?\u0026quot;‚Äîyou will need to upload a relevant context file (a revenue report) so it can be processed.\nTo do this, download a sample Nike revenue report to the Nginx pod using the command below. (If you are no longer logged into the Nginx pod, make sure to log in again before proceeding.)\nExecute the following command to download a sample Nike revenue report to the nginx pod (if you are no longer logged in to the NGinx pod, be sure to use the above command to log in again):\nDownload the document to the microservice : Feed the knowledge base (Vectord) with the document (It will take ~30 seconds): After running the previous command, you should receive a confirmation message like the one below. This command updated the knowledge base by uploading a local file for processing.\nThe data preparation microservice API can retrieve information about the list of files stored in the vector database.\nVerify if the document was uploaded: After running the previous command, you should receive the confirmation message.\nCongratulations! You\u0026rsquo;ve successfully prepared your knowledge base. Now you\u0026rsquo;ll explore the microservices involved in prompt handling.\nStep 3: Prompting Once the knowledge base is set up, you can begin interacting with the application by asking context-specific questions. Retrieval-Augmented Generation (RAG) ensures that responses are both accurate and grounded in relevant data.\nThe process begins with the application retrieving the most relevant information from the knowledge base in response to the user\u0026rsquo;s query. This step ensures that the Large Language Model (LLM) has access to up-to-date and precise context to generate an informed response.\nNext, the retrieved information is combined with the user‚Äôs input prompt and sent to the LLM. This enriched prompt enhances the model‚Äôs ability to provide answers that are not only based on its pre-trained knowledge but also supported by real-world, external data.\nFinally, you will see how the LLM processes this enriched prompt to generate a coherent and contextually accurate response. By leveraging RAG, the application delivers highly relevant answers, grounded in the latest information from the knowledge base.\nThe microservices involved in this stage include:\nEmbeddings Vector Database Retriever Re-ranking LLM Retriever Microservice (POD: chatqna-retriever-usvc:7000) The Retriever Microservice is responsible for locating the most relevant information within the knowledge base and returning documents that closely match the user‚Äôs query. It interacts with various back-end systems that store knowledge and provide APIs for retrieving the best-matching data.\nDifferent knowledge bases utilize different retrieval methods:\nVector databases use vector similarity matching between the user‚Äôs question and stored document embeddings. Graph databases leverage graph locality to find related information. Relational databases rely on string matching and regular expressions to locate relevant text. In this task, you will use Redis as the vector database and retrieve information via the Redis retriever.\nSince vector retrieval relies on embeddings, you first need to generate an embedding for the question: \u0026ldquo;What was Nike\u0026rsquo;s revenue in 2023?\u0026rdquo;\nThis will allow the retriever to search the knowledge base for the most relevant document‚Äîsuch as the Nike revenue report you uploaded in the previous step.\nTo create the embedding, use the chatqna-tei microservice. (Make sure you are logged into the Nginx pod before proceeding.)\nCreate the embedding and save locally (embed_question): You should get the details about the writing task:\nCheck to see if your embedding was saved: echo $embed_question\nYou should be able to see the vectors the embeddings microservice generated. You are now able to use the retriever microservice to get the most similar information from your knowledge base.\nGet and save similar vectors from the initial embed_question locally similar_docs: similar_docs=$(curl chatqna-retriever-usvc:7000/v1/retrieval -X POST -d \u0026ldquo;{\u0026quot;text\u0026quot;:\u0026quot;test\u0026quot;,\u0026quot;embedding\u0026quot;:${embed_question}}\u0026rdquo; -H \u0026lsquo;Content-Type: application/json\u0026rsquo;)\nBy looking at the previous output, you can see the most similar passages (TOP_3) from the document Nike revenue report and the question \u0026ldquo;What was the Nike revenue in 2023?\u0026rdquo;.\necho $similar_docs\nThe following output has been formatted for better readability. Your results will be presented in plain text and may vary slightly due to the similarity search algorithm. However, you can double check that the retrieved documents will be relevant to your initial query.\nThe application will use that information as context for prompting the LLM, but there is still one more step that you need to do to refine and check the quality of those retrieved documents: the reranker.\nReranker Microservice (POD: chatqna-teirerank:80) The Reranking Microservice plays a crucial role in semantic search, leveraging reranking models to enhance the relevance of retrieved results. When given a user query and a set of documents, this microservice reorders the documents based on their semantic similarity to the query, ensuring that the most relevant results appear first.\nReranking is particularly valuable in text retrieval systems, where documents are initially retrieved using either:\nDense embeddings, which capture deep semantic meaning. Sparse lexical search, which relies on keyword-based matching. While these retrieval methods are effective, the reranking model refines the results by optimizing the order of retrieved documents. This step significantly improves accuracy, ensuring the final output is more relevant, precise, and contextually aligned with the user‚Äôs query.\nOPEA has multiple options for re-rankers. For this lab, you\u0026rsquo;ll use the Hugging Face TEI for re-ranking. It is the chatqna-teirerank microservice in your cluster.\nThe reranker will use similar_docs from the previous stage and compare it with the question What was Nike Revenue in 2023? to check the quality of the retrieved documents.\nExtract the 3 retrieved text snippets and save them in a new variable to be reranked:\nInstall jq dependencies to format similar_docs echo -e \u0026ldquo;deb http://deb.debian.org/debian bookworm main contrib non-free\\ndeb http://security.debian.org/debian-security bookworm-security main contrib non-free\\ndeb http://deb.debian.org/debian bookworm-updates main contrib non-free\u0026rdquo; \u0026gt; /etc/apt/sources.list \u0026amp;\u0026amp; apt update \u0026amp;\u0026amp; apt install -y jq\nExtract and format the texts into a valid JSON array of strings texts=$(echo \u0026ldquo;$similar_docs\u0026rdquo; | jq -r \u0026lsquo;[.retrieved_docs[].text | @json]\u0026rsquo;)\nSend the request to the microservice with the query and the formatted texts: curl -X POST chatqna-teirerank:80/rerank -d \u0026ldquo;{\u0026quot;query\u0026quot;:\u0026quot;What was Nike Revenue in 2023?\u0026quot;, \u0026quot;texts\u0026quot;: $texts}\u0026rdquo; -H \u0026lsquo;Content-Type: application/json\u0026rsquo;\nResponse:\nThe following output has been formatted for better readability. Your results are displayed in plain text and may vary slightly due to the similarity search algorithm. The retrieved documents are ranked by similarity to your query, with the highest-ranked index representing the most relevant match. You can confirm that the top-ranked document corresponds to the one most closely aligned with your query.\nThe server responds with a JSON array containing objects with two fields: index and score. This indicates how the snippets are ranked based on their relevance to the query: {\u0026ldquo;index\u0026rdquo;:2,\u0026ldquo;score\u0026rdquo;:0.9972289} means the first text (index 0) has a high relevance score of approximately 0.7982. {\u0026ldquo;index\u0026rdquo;:0,\u0026ldquo;score\u0026rdquo;:0.9776342},{\u0026ldquo;index\u0026rdquo;:3,\u0026ldquo;score\u0026rdquo;:0.9296986},{\u0026ldquo;index\u0026rdquo;:1,\u0026ldquo;score\u0026rdquo;:0.84730965} indicates that the other snippets (index 3,1 and 2) have a much lower score.\nAs you can see from similar_doc the id=2 has the below information where it EXACTLY refers to the revenue for 2023!\nJust the first will be used to prompt the LLM.\nLLM Microservice (POD: chatqna-tgi:80) At the core of the RAG (Retrieval-Augmented Generation) application lies the Large Language Model (LLM), which plays a pivotal role in generating responses. By leveraging RAG, the system enhances the LLM‚Äôs performance, ensuring responses are accurate, relevant, and context-aware.\nTypes of LLMs LLMs generally fall into two main categories, each with its own strengths and trade-offs:\nClosed-Source Models These proprietary models are developed by major tech companies such as Amazon Web Services (AWS), OpenAI, and Google. They are trained on extensive datasets and optimized for high-quality, reliable outputs. However, they come with certain limitations:\nLimited Customization: Users have minimal control over fine-tuning. Higher Costs: Access is usually metered and can be expensive. Data Sovereignty Concerns: API access may restrict usage in applications requiring strict data governance. Open-Source Models Freely available for use and modification, open-source LLMs offer greater flexibility and control. They allow users to customize and fine-tune models according to specific needs. Running open-source models locally or on private cloud infrastructure ensures better data privacy and cost efficiency. However, they require:\nTechnical Expertise: Deploying and optimizing open-source models can be complex. Computational Resources: Achieving comparable performance to closed models often demands powerful hardware. Flexible Integration with OPEA: This microservice architecture supports both closed and open-source models, providing the flexibility to choose the best fit for your application. In this example, the TGI (Text Generation Inference) model from Hugging Face is used. Testing the LLM Microservice: To verify its functionality, you can directly prompt the TGI LLM with a sample question: \u0026ldquo;What was Nike\u0026rsquo;s revenue in 2023?\u0026rdquo; This test will demonstrate how well the model can retrieve and generate an informed response based on the loaded knowledge base.\nDirectly prompt the TGI(LLM) Microservice: The model will give you the answer to the prompt like the following:\n\u0026ldquo;generated_text\u0026rdquo;:\u0026rdquo; Nike revenue in 2023 has not been reported as it is still in the fourth quarter. The previous full financial year‚Äîwhich is 2022‚Äîbrought in $48.9 billion in revenue for the American multinational sportswear company. They deal with the design, development, manufacturing, and worldwide marketing/sales of a diverse portfolio of products. From coming into being in 1964 as Blue Ribbon Sports, the firm was renamed Nike, Inc., in 1978. Jumpman logos (for example), include Michael Jordan, who is a former professional basketball player‚Äîare among the brands\u0026rsquo; numerous trademarked symbols, tied to the \u0026lsquo;Swoosh\u0026rsquo; logo.\\n\\nNike revenues are clearly affected by the football World Cup. Consequently, for the 13 weeks ending January 29 in 2022, which were characterized by the football world cup\nƒêi·ªÅu n√†y tr·ª±c ti·∫øp nh·∫Øc LLM m√† kh√¥ng cung c·∫•p ng·ªØ c·∫£nh. Ch√∫ng ta c√≥ th·ªÉ th·∫•y r·∫±ng m√¥ h√¨nh th·ª±c s·ª± ƒë∆∞a ra c√¢u tr·∫£ l·ªùi sai. ƒê·ªÉ ki·ªÉm tra hi·ªáu su·∫•t RAG t·ªïng th·ªÉ, ch√∫ng ta n√™n th·ª≠ nghi·ªám v·ªõi megaservice nh∆∞ ch√∫ng ta ƒë√£ l√†m khi b·∫Øt ƒë·∫ßu nhi·ªám v·ª• n√†y, ƒëi·ªÅu n√†y s·∫Ω li√™n quan ƒë·∫øn to√†n b·ªô lu·ªìng.\nTho√°t kh·ªèi POD ngnix: B·∫°n c√≥ th·ªÉ t√¨m th·∫•y \u0026ldquo;c√¥ng vi·ªác ƒëang ch·ªù x·ª≠ l√Ω\u0026rdquo;, vui l√≤ng b·ªè qua v√† th·ª≠ l·∫°i. root@chatqna-nginx-deployment-XXXXXXXXXXXX:/# exit\nS·ª≠ d·ª•ng URL b·ªô c√¢n b·∫±ng t·∫£i m√† b·∫°n ƒë√£ l∆∞u ·ªü tr√™n trong l·ªánh b√™n d∆∞·ªõi ƒë·ªÉ g·ª≠i c√¢u h·ªèi \u0026ldquo;Doanh thu c·ªßa Nike nƒÉm 2023 l√† bao nhi√™u?\u0026rdquo; ƒë·∫øn ·ª©ng d·ª•ng ChatQNA.\nCh·∫°y l·∫°i curl ƒë·∫øn b·ªô c√¢n b·∫±ng t·∫£i: Xem l·∫°i k·∫øt qu·∫£ c·ªßa b·∫°n v√† b·∫°n s·∫Ω th·∫•y r·∫±ng ph·∫£n h·ªìi ƒë∆∞·ª£c truy·ªÅn ph√°t. ƒê√¢y l√† h√†nh vi mong ƒë·ª£i c·ªßa d·ªãch v·ª• vi m√¥, v√¨ n√≥ cung c·∫•p c√¢u tr·∫£ l·ªùi theo t·ª´ng ph·∫ßn nh·ªè h∆°n thay v√¨ t·∫•t c·∫£ c√πng m·ªôt l√∫c. Streaming cho ph√©p d·ªØ li·ªáu ƒë∆∞·ª£c x·ª≠ l√Ω v√† hi·ªÉn th·ªã theo t·ª´ng b∆∞·ªõc khi d·ªØ li·ªáu c√≥ s·∫µn. Trong ·ª©ng d·ª•ng, UI s·∫Ω ghi l·∫°i ph·∫£n h·ªìi n√†y v√† ƒë·ªãnh d·∫°ng th√†nh m√†n h√¨nh c√≥ th·ªÉ ƒë·ªçc ƒë∆∞·ª£c, cho ph√©p ng∆∞·ªùi d√πng xem th√¥ng tin theo th·ªùi gian th·ª±c khi d·ªØ li·ªáu ƒë·∫øn. "
},
{
	"uri": "http://<user_name>.github.io/4-s3log/4.2-creates3bucket/",
	"title": "L·ªõp 7",
	"tags": [],
	"description": "",
	"content": "Test application\nYou can check the deployment by accessing to the DNS url of the load balancer the cloud formation templated created.\nLook for the load balancer: Copy your DNS name for bedrock-ingress: Paste it on a new browser tab to access to the interface In the UI you can see the chatbot to interact with it\nCheck if the model is able to give us an answer about OPEA: You may notice that the chatbot‚Äôs initial response is outdated or lacks specific details about OPEA. This is because OPEA is a relatively new project and was not included in the dataset used to train the language model. Since most LLMs (Large Language Models) are static, they rely solely on pre-existing training data and cannot automatically incorporate new developments or emerging technologies like OPEA.\nUploading Context to Improve Accuracy To address this limitation, RAG (Retrieval-Augmented Generation) enables real-time context retrieval. The ChatQnA UI includes an upload icon, allowing you to add relevant context.\nHow It Works:\n1. When you upload a document or link, it is sent to the DataPrep microservice.\r2. DataPrep processes the content and generates embeddings.\r3. The processed data is then stored in the Vector Database for retrieval.\rBy uploading updated documents or links, you expand the chatbot‚Äôs knowledge base, ensuring it provides more relevant, accurate, and up-to-date responses.\nThe deployment allows you to upload either a file or a site. For this case, use the OPEA site:\nClick on the upload icon to open the right panel\nClick on Paste Link\nCopy/paste the text https://opea-project.github.io/latest/introduction/index.html to the entry box\nClick Confirm to start the indexing process When the indexing completes, you\u0026rsquo;ll see an icon added below the text box, labeled https://opea-project.github.io/latest/introduction/index.html\nAsk the application after the context is provided: Ask \u0026ldquo;What is OPEA?\u0026rdquo; again to see the updated answer.\nThis time, the chat bot responds correctly based on the data it added to the prompt from the new source, the OPEA web site.\nConclusion\nIn this task, you successfully deployed a RAG-powered chatbot using Amazon Bedrock. By uploading relevant context, you enabled the model to dynamically update and refine its responses based on new information. This process demonstrated how RAG integration enhances real-time adaptability, allowing the system to continuously improve its accuracy and relevance while leveraging the power of Amazon Bedrock.\n"
},
{
	"uri": "http://<user_name>.github.io/2-prerequiste/2.2-createiamrole-copy/",
	"title": "L·ªõp 8",
	"tags": [],
	"description": "",
	"content": "Preparing Your AWS Environment for the Workshop If you are not participating in an AWS-led event via Workshop Studio, you must set up your environment before proceeding with these labs. Keep in mind that the resources you create will incur costs, so be sure to clean them up once you have completed the workshop.\nStep 1: Configuring Your Environment To set up your AWS account, follow these steps:\nClick Launch Stack below to initiate AWS CloudFormation with pre-configured values in the us-east-1 region.\nLaunch Stack\nIf you prefer a different AWS region, remember to change the region accordingly. The CloudFormation stack will provision the following resources:\nEKS Cluster Deployment\n-Creates a new EKS cluster named opea-eks-cluster.\n-Deploys a worker node within the cluster using an M7i.24xlarge instance.\nCloudFormation Templates for Workshop Modules The stack also includes templates for the following modules:\nModule 1: ChatQnA Default\nModule 2: ChatQnA with Guardrails\nModule 3: ChatQnA with OpenSearch (open-source) as the vector database\nModule 4: ChatQnA with Remote Inference (Denvr) as the LLM\nModule 5: ChatQnA with Bedrock as the LLM\nConfiguring Stack Parameters Before launching the stack, review and configure the following parameters:\nFor Modules 1, 2, and 3:\nHuggingFaceToken: This token is required to download models from Hugging Face. If you plan to use the Guardrails feature, ensure your token has access to the meta-llama/Meta-Llama-Guard-2-8B model.\nModelID: The OPEA system primarily uses the Text Generation Inference toolkit. Select any compatible model from Hugging Face and provide its model ID.\nOpeaRoleArn: Enter the ARN or name of the IAM role associated with your AWS account.\nIf you are unsure, check the user information displayed in the top-right corner of the AWS Management Console. If your username does not contain a forward slash (/), copy the full name. If it does contain a forward slash, only use the portion before the slash. Example:\nIf the display name is \u0026ldquo;USER\u0026rdquo;, enter \u0026ldquo;USER\u0026rdquo;. If the display name is \u0026ldquo;ADMIN-ROLE/USER\u0026rdquo;, enter \u0026ldquo;ADMIN-ROLE\u0026rdquo;. Alternatively, you can retrieve your ARN using the AWS CLI. aws sts get-caller-identity \u0026ndash;query \u0026lsquo;Arn\u0026rsquo; \u0026ndash;output text | awk -F: \u0026lsquo;{print $NF}\u0026rsquo; | (read id; if [[ $id == \u0026ldquo;user\u0026rdquo; ]]; then aws sts get-caller-identity \u0026ndash;query \u0026lsquo;Arn\u0026rsquo; \u0026ndash;output text; else role=$(echo $id | cut -d\u0026rsquo;/\u0026rsquo; -f2); aws iam get-role \u0026ndash;role-name $role \u0026ndash;query \u0026lsquo;Role.Arn\u0026rsquo; \u0026ndash;output text; fi)\nMark the checkbox next to I acknowledge that AWS CloudFormation might create IAM resources\nDeploying the CloudFormation Stack Once the parameters are set, click Create Stack.\nThis process will initiate an AWS CodeBuild Project, which pulls in the opea-demo-builder open-source library.\nThe AWS Cloud Development Kit (CDK) will generate the necessary CloudFormation templates to configure your AWS environment for the workshop.\nMonitoring Deployment Progress The deployment process takes approximately 25 minutes. Open the AWS CloudFormation Console and monitor the progress.\nEnsure that the following stacks reach the CREATE_COMPLETE status:\nThe EKS cluster The ChatQnA application (default deployment) Some stacks may remain in REVIEW_IN_PROGRESS, as they will be deployed onto the EKS cluster later.\nStep 2: Configuring Access to Your EKS Cluster Once the CloudFormation stacks are successfully deployed, you need to configure your local environment to interact with the Amazon EKS cluster using kubectl.\nUpdating Your Kubernetes Configuration (kubeconfig)\nOpen AWS CloudShell Click the CloudShell icon in the AWS Management Console. Alternatively, use your local AWS CLI, ensuring kubectl and the AWS CLI client are installed on your system. Update kubeconfig Run the following command (updating the region if necessary): If successful, you should receive a confirmation message indicating that your configuration file has been updated. Verify Connectivity to Your EKS Cluster Ensure you can interact with the cluster using kubectl. Step 3: Verifying EKS Cluster Access After updating your kubeconfig, test whether you can connect to the cluster. If you encounter issues accessing pods via the AWS console or CloudShell, follow these steps:\nCheck Your IAM Access in the EKS Console\nNavigate to the EKS Console. Look for your IAM principal ARN under Access Entries. If Your ARN Is Not Listed, Add It Manually\nClick Create Access Entry. Enter your IAM User ARN or Role ARN. Attach Required Policies Assign the following permissions to your IAM role:\nAmazonEKSAdminPolicy AmazonEKSClusterAdminPolicy Confirm Access by Listing Nodes\nRun the following command to check if worker nodes are visible: If the command returns a list of nodes, your cluster is successfully configured. Note: If no nodes appear, wait a few minutes and retry, as node provisioning may still be in progress. Next Steps: Explore the Workshop Modules Once you have successfully connected to your EKS cluster, you are ready to proceed with any of the workshop modules. Choose the module that best fits your learning objectives and begin your hands-on experience with AWS, Kubernetes, and AI-powered solutions.\n"
},
{
	"uri": "http://<user_name>.github.io/3-accessibilitytoinstances/3.2-private-instance-copy/",
	"title": "L·ªõp 8",
	"tags": [],
	"description": "",
	"content": "Understand RAG and use the UI Now that you\u0026rsquo;ve verified all services are running, let‚Äôs take a look at the UI provided by the implementation.\nTo access the UI, open any browser and go to the DNS of the ChatQnA Load Balancer: http://chatqna-ingress-xxxxxxx.us-east-2.elb.amazonaws.com (Modify with your chatqna-ingressDNS URL)\nIn the UI you can see the chatbot interact with it\nTo verify the UI, go ahead and ask\nThe answer is correct again because we already indexed our knowledge base on the previous step.\nLet\u0026rsquo;s try something different. Will the app be able to answer about OPEA:\nYou may notice that the chatbot\u0026rsquo;s initial response is outdated or lacks specific details about OPEA. This is because OPEA is a relatively new project and was not included in the dataset used to train the language model. Since most language models are static‚Äîmeaning they rely on the data available at the time of training‚Äîthey cannot automatically incorporate recent developments or newly emerging topics like OPEA.\nHowever, RAG provides a solution by enabling real-time context retrieval. Within the UI, you\u0026rsquo;ll find an option to upload relevant contextual information. When you do this, the document is sent to the DataPrep microservice, where it is converted into embeddings and stored in the Vector Database.\nBy uploading a document or a link, you effectively expand the chatbot‚Äôs knowledge base with the latest information, improving the relevance and accuracy of its responses.\nThe deployment allows you to upload either a file or a site. For this case, use the OPEA site:\nClick on the upload icon to open the right panel Click on Paste Link Copy/paste the text https://opea-project.github.io/latest/introduction/index.html to the entry box Click Confirm to start the indexing process When the indexing completes, you\u0026rsquo;ll see an icon added below the text box, labeled https://opea-project.github.io/latest/introduction/index.html\nAsk \u0026ldquo;What is OPEA?\u0026rdquo; again to see the updated answer.\nThis time, the chatbot responds correctly based on the data it added to the prompt from the new source, the OPEA website.\nConclusion\nIn this task, you explored the core structure of a RAG application, gaining insight into how each component functions and interacts within the system. From retrieving relevant information to generating accurate responses, every part plays a vital role in OPEA‚Äôs RAG workflow‚Äîenhancing response relevance through retrieval while improving accuracy with advanced language modeling. This hands-on session provided a clear understanding of how OPEA leverages RAG to process complex queries efficiently and refine model performance through seamless component integration.\nIn the next task, you will implement guardrails for the chatbot. These guardrails are essential for detecting and mitigating biases, ensuring that AI-generated responses remain responsible, fair, and aligned with ethical AI principles.\n"
},
{
	"uri": "http://<user_name>.github.io/2-prerequiste/2.2-createiamrole-copy-2/",
	"title": "L·ªõp 9",
	"tags": [],
	"description": "",
	"content": "Preparing Your AWS Environment for the Workshop If you are not participating in an AWS-led event via Workshop Studio, you must set up your environment before proceeding with these labs. Keep in mind that the resources you create will incur costs, so be sure to clean them up once you have completed the workshop.\nStep 1: Configuring Your Environment To set up your AWS account, follow these steps:\nClick Launch Stack below to initiate AWS CloudFormation with pre-configured values in the us-east-1 region.\nLaunch Stack\nIf you prefer a different AWS region, remember to change the region accordingly. The CloudFormation stack will provision the following resources:\nEKS Cluster Deployment\n-Creates a new EKS cluster named opea-eks-cluster.\n-Deploys a worker node within the cluster using an M7i.24xlarge instance.\nCloudFormation Templates for Workshop Modules The stack also includes templates for the following modules:\nModule 1: ChatQnA Default\nModule 2: ChatQnA with Guardrails\nModule 3: ChatQnA with OpenSearch (open-source) as the vector database\nModule 4: ChatQnA with Remote Inference (Denvr) as the LLM\nModule 5: ChatQnA with Bedrock as the LLM\nConfiguring Stack Parameters Before launching the stack, review and configure the following parameters:\nFor Modules 1, 2, and 3:\nHuggingFaceToken: This token is required to download models from Hugging Face. If you plan to use the Guardrails feature, ensure your token has access to the meta-llama/Meta-Llama-Guard-2-8B model.\nModelID: The OPEA system primarily uses the Text Generation Inference toolkit. Select any compatible model from Hugging Face and provide its model ID.\nOpeaRoleArn: Enter the ARN or name of the IAM role associated with your AWS account.\nIf you are unsure, check the user information displayed in the top-right corner of the AWS Management Console. If your username does not contain a forward slash (/), copy the full name. If it does contain a forward slash, only use the portion before the slash. Example:\nIf the display name is \u0026ldquo;USER\u0026rdquo;, enter \u0026ldquo;USER\u0026rdquo;. If the display name is \u0026ldquo;ADMIN-ROLE/USER\u0026rdquo;, enter \u0026ldquo;ADMIN-ROLE\u0026rdquo;. Alternatively, you can retrieve your ARN using the AWS CLI. aws sts get-caller-identity \u0026ndash;query \u0026lsquo;Arn\u0026rsquo; \u0026ndash;output text | awk -F: \u0026lsquo;{print $NF}\u0026rsquo; | (read id; if [[ $id == \u0026ldquo;user\u0026rdquo; ]]; then aws sts get-caller-identity \u0026ndash;query \u0026lsquo;Arn\u0026rsquo; \u0026ndash;output text; else role=$(echo $id | cut -d\u0026rsquo;/\u0026rsquo; -f2); aws iam get-role \u0026ndash;role-name $role \u0026ndash;query \u0026lsquo;Role.Arn\u0026rsquo; \u0026ndash;output text; fi)\nMark the checkbox next to I acknowledge that AWS CloudFormation might create IAM resources\nDeploying the CloudFormation Stack Once the parameters are set, click Create Stack.\nThis process will initiate an AWS CodeBuild Project, which pulls in the opea-demo-builder open-source library.\nThe AWS Cloud Development Kit (CDK) will generate the necessary CloudFormation templates to configure your AWS environment for the workshop.\nMonitoring Deployment Progress The deployment process takes approximately 25 minutes. Open the AWS CloudFormation Console and monitor the progress.\nEnsure that the following stacks reach the CREATE_COMPLETE status:\nThe EKS cluster The ChatQnA application (default deployment) Some stacks may remain in REVIEW_IN_PROGRESS, as they will be deployed onto the EKS cluster later.\nStep 2: Configuring Access to Your EKS Cluster Once the CloudFormation stacks are successfully deployed, you need to configure your local environment to interact with the Amazon EKS cluster using kubectl.\nUpdating Your Kubernetes Configuration (kubeconfig)\nOpen AWS CloudShell Click the CloudShell icon in the AWS Management Console. Alternatively, use your local AWS CLI, ensuring kubectl and the AWS CLI client are installed on your system. Update kubeconfig Run the following command (updating the region if necessary): If successful, you should receive a confirmation message indicating that your configuration file has been updated. Verify Connectivity to Your EKS Cluster Ensure you can interact with the cluster using kubectl. Step 3: Verifying EKS Cluster Access After updating your kubeconfig, test whether you can connect to the cluster. If you encounter issues accessing pods via the AWS console or CloudShell, follow these steps:\nCheck Your IAM Access in the EKS Console\nNavigate to the EKS Console. Look for your IAM principal ARN under Access Entries. If Your ARN Is Not Listed, Add It Manually\nClick Create Access Entry. Enter your IAM User ARN or Role ARN. Attach Required Policies Assign the following permissions to your IAM role:\nAmazonEKSAdminPolicy AmazonEKSClusterAdminPolicy Confirm Access by Listing Nodes\nRun the following command to check if worker nodes are visible: If the command returns a list of nodes, your cluster is successfully configured. Note: If no nodes appear, wait a few minutes and retry, as node provisioning may still be in progress. Next Steps: Explore the Workshop Modules Once you have successfully connected to your EKS cluster, you are ready to proceed with any of the workshop modules. Choose the module that best fits your learning objectives and begin your hands-on experience with AWS, Kubernetes, and AI-powered solutions.\n"
},
{
	"uri": "http://<user_name>.github.io/3-accessibilitytoinstances/3.2-private-instance-copy-2/",
	"title": "L·ªõp 9",
	"tags": [],
	"description": "",
	"content": "Understand RAG and use the UI Now that you\u0026rsquo;ve verified all services are running, let‚Äôs take a look at the UI provided by the implementation.\nTo access the UI, open any browser and go to the DNS of the ChatQnA Load Balancer: http://chatqna-ingress-xxxxxxx.us-east-2.elb.amazonaws.com (Modify with your chatqna-ingressDNS URL)\nIn the UI you can see the chatbot interact with it\nTo verify the UI, go ahead and ask\nThe answer is correct again because we already indexed our knowledge base on the previous step.\nLet\u0026rsquo;s try something different. Will the app be able to answer about OPEA:\nYou may notice that the chatbot\u0026rsquo;s initial response is outdated or lacks specific details about OPEA. This is because OPEA is a relatively new project and was not included in the dataset used to train the language model. Since most language models are static‚Äîmeaning they rely on the data available at the time of training‚Äîthey cannot automatically incorporate recent developments or newly emerging topics like OPEA.\nHowever, RAG provides a solution by enabling real-time context retrieval. Within the UI, you\u0026rsquo;ll find an option to upload relevant contextual information. When you do this, the document is sent to the DataPrep microservice, where it is converted into embeddings and stored in the Vector Database.\nBy uploading a document or a link, you effectively expand the chatbot‚Äôs knowledge base with the latest information, improving the relevance and accuracy of its responses.\nThe deployment allows you to upload either a file or a site. For this case, use the OPEA site:\nClick on the upload icon to open the right panel Click on Paste Link Copy/paste the text https://opea-project.github.io/latest/introduction/index.html to the entry box Click Confirm to start the indexing process When the indexing completes, you\u0026rsquo;ll see an icon added below the text box, labeled https://opea-project.github.io/latest/introduction/index.html\nAsk \u0026ldquo;What is OPEA?\u0026rdquo; again to see the updated answer.\nThis time, the chatbot responds correctly based on the data it added to the prompt from the new source, the OPEA website.\nConclusion\nIn this task, you explored the core structure of a RAG application, gaining insight into how each component functions and interacts within the system. From retrieving relevant information to generating accurate responses, every part plays a vital role in OPEA‚Äôs RAG workflow‚Äîenhancing response relevance through retrieval while improving accuracy with advanced language modeling. This hands-on session provided a clear understanding of how OPEA leverages RAG to process complex queries efficiently and refine model performance through seamless component integration.\nIn the next task, you will implement guardrails for the chatbot. These guardrails are essential for detecting and mitigating biases, ensuring that AI-generated responses remain responsible, fair, and aligned with ethical AI principles.\n"
},
{
	"uri": "http://<user_name>.github.io/3-accessibilitytoinstances/",
	"title": "B√†i Gi·∫£ng VƒÉn",
	"tags": [],
	"description": "",
	"content": "L·ªùi n√≥i ƒë·∫ßu Ng·ªØ vƒÉn kh√¥ng ch·ªâ l√† m√¥n h·ªçc v·ªÅ ch·ªØ nghƒ©a m√† c√≤n l√† c√°nh c·ª≠a d·∫´n l·ªëi v√†o th·∫ø gi·ªõi c·ªßa t∆∞ duy, c·∫£m x√∫c v√† s·ª± s√°ng t·∫°o. VƒÉn h·ªçc gi√∫p h·ªçc sinh hi·ªÉu s√¢u h∆°n v·ªÅ cu·ªôc s·ªëng, con ng∆∞·ªùi, ƒë·ªìng th·ªùi r√®n luy·ªán kh·∫£ nƒÉng di·ªÖn ƒë·∫°t, t∆∞ duy ph·∫£n bi·ªán v√† c·∫£m th·ª• c√°i ƒë·∫πp trong ng√¥n t·ª´.\n·ªû b·∫≠c trung h·ªçc c∆° s·ªü, m√¥n VƒÉn ƒë√≥ng vai tr√≤ quan tr·ªçng trong vi·ªác h√¨nh th√†nh t∆∞ duy logic v√† kh·∫£ nƒÉng bi·ªÉu ƒë·∫°t c·ªßa h·ªçc sinh. Th√¥ng qua vi·ªác h·ªçc c√°c t√°c ph·∫©m vƒÉn h·ªçc, c√°c em kh√¥ng ch·ªâ ti·∫øp c·∫≠n v·ªõi nh·ªØng gi√° tr·ªã nh√¢n vƒÉn s√¢u s·∫Øc m√† c√≤n r√®n luy·ªán c√°ch l·∫≠p lu·∫≠n, ph√¢n t√≠ch v√† tr√¨nh b√†y √Ω ki·∫øn m·ªôt c√°ch m·∫°ch l·∫°c, thuy·∫øt ph·ª•c.\nCh∆∞∆°ng tr√¨nh Ng·ªØ vƒÉn trung h·ªçc c∆° s·ªü bao g·ªìm ba ph·∫ßn ch√≠nh: ƒê·ªçc ‚Äì hi·ªÉu vƒÉn b·∫£n, Ti·∫øng Vi·ªát v√† T·∫≠p l√†m vƒÉn.\nƒê·ªçc ‚Äì hi·ªÉu vƒÉn b·∫£n gi√∫p h·ªçc sinh ti·∫øp x√∫c v·ªõi c√°c t√°c ph·∫©m vƒÉn h·ªçc Vi·ªát Nam v√† th·∫ø gi·ªõi, t·ª´ nh·ªØng b√†i ca dao, truy·ªán ng·∫Øn, th∆° ca cho ƒë·∫øn c√°c vƒÉn b·∫£n nh·∫≠t d·ª•ng mang t√≠nh th·ª±c ti·ªÖn. Qua ƒë√≥, h·ªçc sinh r√®n luy·ªán kh·∫£ nƒÉng ph√¢n t√≠ch n·ªôi dung, ƒë√°nh gi√° ngh·ªá thu·∫≠t v√† hi·ªÉu ƒë∆∞·ª£c t∆∞ t∆∞·ªüng c·ªßa t√°c gi·∫£. Ph·∫ßn Ti·∫øng Vi·ªát gi√∫p h·ªçc sinh n·∫Øm v·ªØng c√°c quy t·∫Øc ng·ªØ ph√°p, m·ªü r·ªông v·ªën t·ª´, s·ª≠ d·ª•ng ng√¥n ng·ªØ linh ho·∫°t v√† ch√≠nh x√°c trong giao ti·∫øp c≈©ng nh∆∞ vi·∫øt b√†i. Ph·∫ßn T·∫≠p l√†m vƒÉn r√®n luy·ªán k·ªπ nƒÉng vi·∫øt, t·ª´ vi·ªác mi√™u t·∫£, t·ª± s·ª±, bi·ªÉu c·∫£m ƒë·∫øn ngh·ªã lu·∫≠n, gi√∫p h·ªçc sinh bi·∫øt c√°ch x√¢y d·ª±ng l·∫≠p lu·∫≠n ch·∫∑t ch·∫Ω, di·ªÖn ƒë·∫°t √Ω t∆∞·ªüng r√µ r√†ng v√† gi√†u c·∫£m x√∫c. T·∫°i trung t√¢m, ch∆∞∆°ng tr√¨nh gi·∫£ng d·∫°y m√¥n VƒÉn ƒë∆∞·ª£c thi·∫øt k·∫ø kh√¥ng ch·ªâ ƒë·ªÉ gi√∫p h·ªçc sinh ƒë·∫°t ƒëi·ªÉm cao m√† c√≤n truy·ªÅn c·∫£m h·ª©ng ƒë·ªÉ c√°c em y√™u th√≠ch m√¥n h·ªçc n√†y. H·ªçc sinh s·∫Ω ƒë∆∞·ª£c h∆∞·ªõng d·∫´n c√°ch ƒë·ªçc hi·ªÉu nhanh, ph√¢n t√≠ch s√¢u, vi·∫øt vƒÉn m·∫°ch l·∫°c v√† s√°ng t·∫°o. C√°c ph∆∞∆°ng ph√°p gi·∫£ng d·∫°y ƒëa d·∫°ng nh∆∞ th·∫£o lu·∫≠n nh√≥m, ph√¢n t√≠ch t√¨nh hu·ªëng, s√°ng t√°c vƒÉn h·ªçc, gi√∫p h·ªçc sinh r√®n luy·ªán t∆∞ duy ph·∫£n bi·ªán v√† kh·∫£ nƒÉng vi·∫øt t·ªët h∆°n.\nNg·ªØ vƒÉn kh√¥ng ch·ªâ l√† m·ªôt m√¥n h·ªçc trong nh√† tr∆∞·ªùng m√† c√≤n l√† c√¥ng c·ª• gi√∫p h·ªçc sinh ph√°t tri·ªÉn k·ªπ nƒÉng s·ªëng. Bi·∫øt c√°ch s·ª≠ d·ª•ng ng√¥n t·ª´ tinh t·∫ø, l·∫≠p lu·∫≠n ch·∫∑t ch·∫Ω v√† th·ªÉ hi·ªán c·∫£m x√∫c ch√¢n th√†nh ch√≠nh l√† n·ªÅn t·∫£ng ƒë·ªÉ c√°c em t·ª± tin h∆°n trong giao ti·∫øp v√† th√†nh c√¥ng trong t∆∞∆°ng lai.\n"
},
{
	"uri": "http://<user_name>.github.io/4-s3log/4.3-creategwes3/",
	"title": "L·ªõp 8",
	"tags": [],
	"description": "",
	"content": "Explore RAG and Interact with the UI\nNow that all services are up and running, let\u0026rsquo;s explore the UI provided by the implementation.\nTo access the ChatQnA Bedrock UI, open a web browser and navigate to the DNS of the ChatQnA Bedrock Load Balancer:\nüëâ http://bedrock-ingress-xxxxxxx.us-east-2.elb.amazonaws.com (Replace with your actual Bedrock Ingress DNS URL).\nOnce inside the UI, you can interact with the chatbot, test its responses, and experience how it processes queries using RAG-powered retrieval.\nNow when you send a prompt to the chatbot, the response will be coming from Anthropic\u0026rsquo;s Claude Haiku through Amazon Bedrock.\n"
},
{
	"uri": "http://<user_name>.github.io/4-s3log/4.3-creategwes3-copy/",
	"title": "L·ªõp 9",
	"tags": [],
	"description": "",
	"content": "Explore RAG and Interact with the UI\nNow that all services are up and running, let\u0026rsquo;s explore the UI provided by the implementation.\nTo access the ChatQnA Bedrock UI, open a web browser and navigate to the DNS of the ChatQnA Bedrock Load Balancer:\nüëâ http://bedrock-ingress-xxxxxxx.us-east-2.elb.amazonaws.com (Replace with your actual Bedrock Ingress DNS URL).\nOnce inside the UI, you can interact with the chatbot, test its responses, and experience how it processes queries using RAG-powered retrieval.\nNow when you send a prompt to the chatbot, the response will be coming from Anthropic\u0026rsquo;s Claude Haiku through Amazon Bedrock.\n"
},
{
	"uri": "http://<user_name>.github.io/4-s3log/",
	"title": "B√†i Gi·∫£ng Ti·∫øng Anh",
	"tags": [],
	"description": "",
	"content": "L·ªùi n√≥i ƒë·∫ßu Ti·∫øng Anh kh√¥ng ch·ªâ l√† m·ªôt m√¥n h·ªçc, m√† c√≤n l√† ch√¨a kh√≥a gi√∫p h·ªçc sinh m·ªü r·ªông c√°nh c·ª≠a ƒë·∫øn v·ªõi th·∫ø gi·ªõi tri th·ª©c, c√¥ng ngh·ªá v√† h·ªôi nh·∫≠p qu·ªëc t·∫ø. Trong th·ªùi ƒë·∫°i to√†n c·∫ßu h√≥a, vi·ªác th√¥ng th·∫°o Ti·∫øng Anh mang l·∫°i v√¥ s·ªë c∆° h·ªôi trong h·ªçc t·∫≠p, c√¥ng vi·ªác v√† cu·ªôc s·ªëng.\n·ªû b·∫≠c trung h·ªçc c∆° s·ªü, ch∆∞∆°ng tr√¨nh Ti·∫øng Anh gi√∫p h·ªçc sinh x√¢y d·ª±ng n·ªÅn t·∫£ng v·ªØng ch·∫Øc v·ªÅ t·ª´ v·ª±ng, ng·ªØ ph√°p, k·ªπ nƒÉng nghe, n√≥i, ƒë·ªçc, vi·∫øt. ƒê√¢y l√† giai ƒëo·∫°n quan tr·ªçng ƒë·ªÉ c√°c em r√®n luy·ªán kh·∫£ nƒÉng s·ª≠ d·ª•ng ng√¥n ng·ªØ m·ªôt c√°ch t·ª± tin v√† hi·ªáu qu·∫£.\nCh∆∞∆°ng tr√¨nh Ti·∫øng Anh trung h·ªçc c∆° s·ªü ƒë∆∞·ª£c chia th√†nh c√°c k·ªπ nƒÉng ch√≠nh:\nNg·ªØ ph√°p v√† t·ª´ v·ª±ng: Gi√∫p h·ªçc sinh n·∫Øm v·ªØng c·∫•u tr√∫c c√¢u, c√°ch s·ª≠ d·ª•ng th√¨, t·ª´ lo·∫°i, c·ª•m t·ª´ quan tr·ªçng, t·ª´ ƒë√≥ h√¨nh th√†nh kh·∫£ nƒÉng di·ªÖn ƒë·∫°t ch√≠nh x√°c v√† linh ho·∫°t.\nK·ªπ nƒÉng nghe ‚Äì n√≥i: H·ªçc sinh ƒë∆∞·ª£c ti·∫øp x√∫c v·ªõi c√°c ƒëo·∫°n h·ªôi tho·∫°i, b√†i nghe th·ª±c t·∫ø, luy·ªán t·∫≠p ph√°t √¢m chu·∫©n v√† n√¢ng cao kh·∫£ nƒÉng giao ti·∫øp.\nK·ªπ nƒÉng ƒë·ªçc hi·ªÉu: R√®n luy·ªán kh·∫£ nƒÉng n·∫Øm b·∫Øt th√¥ng tin, ph√¢n t√≠ch vƒÉn b·∫£n v√† trau d·ªìi v·ªën t·ª´ ƒë·ªÉ hi·ªÉu s√¢u h∆°n c√°c b√†i ƒë·ªçc.\nK·ªπ nƒÉng vi·∫øt: H·ªçc sinh h·ªçc c√°ch vi·∫øt t·ª´ nh·ªØng c√¢u ƒë∆°n gi·∫£n ƒë·∫øn ƒëo·∫°n vƒÉn v√† b√†i lu·∫≠n ho√†n ch·ªânh, gi√∫p th·ªÉ hi·ªán √Ω t∆∞·ªüng m·∫°ch l·∫°c v√† s√°ng t·∫°o.\nT·∫°i trung t√¢m, vi·ªác gi·∫£ng d·∫°y Ti·∫øng Anh kh√¥ng ch·ªâ d·ª´ng l·∫°i ·ªü s√°ch gi√°o khoa m√† c√≤n k·∫øt h·ª£p v·ªõi c√°c t√†i li·ªáu th·ª±c t·∫ø, ph∆∞∆°ng ph√°p h·ªçc hi·ªán ƒë·∫°i nh∆∞ h·ªçc qua video, podcast, h·ªôi tho·∫°i tr·ª±c tuy·∫øn, gi√∫p h·ªçc sinh ti·∫øp c·∫≠n ng√¥n ng·ªØ m·ªôt c√°ch t·ª± nhi√™n v√† h·ª©ng th√∫. C√°c b√†i gi·∫£ng ƒë∆∞·ª£c thi·∫øt k·∫ø sinh ƒë·ªông, l·ªìng gh√©p v√†o c√°c ch·ªß ƒë·ªÅ th·ª±c t·∫ø, gi√∫p h·ªçc sinh r√®n luy·ªán ph·∫£n x·∫° nhanh, t·ª± tin s·ª≠ d·ª•ng Ti·∫øng Anh trong giao ti·∫øp h·∫±ng ng√†y.\nNgo√†i ra, trung t√¢m c√≤n h∆∞·ªõng ƒë·∫øn vi·ªác gi√∫p h·ªçc sinh ƒë·∫°t k·∫øt qu·∫£ cao trong c√°c k·ª≥ thi quan tr·ªçng nh∆∞ thi h·ªçc k·ª≥, thi tuy·ªÉn sinh v√†o l·ªõp 10, thi ch·ª©ng ch·ªâ qu·ªëc t·∫ø (KET, PET, IELTS, TOEIC), t·∫°o ti·ªÅn ƒë·ªÅ v·ªØng ch·∫Øc cho vi·ªác h·ªçc Ti·∫øng Anh n√¢ng cao ·ªü c√°c c·∫•p b·∫≠c ti·∫øp theo.\nH·ªçc Ti·∫øng Anh kh√¥ng ch·ªâ l√† h·ªçc m·ªôt ng√¥n ng·ªØ m·ªõi m√† c√≤n l√† h·ªçc c√°ch t∆∞ duy linh ho·∫°t, giao ti·∫øp hi·ªáu qu·∫£ v√† h√≤a nh·∫≠p v·ªõi th·∫ø gi·ªõi. V·ªõi ph∆∞∆°ng ph√°p gi·∫£ng d·∫°y khoa h·ªçc, ƒë·ªôi ng≈© gi√°o vi√™n t·∫≠n t√¢m, trung t√¢m cam k·∫øt mang l·∫°i m·ªôt m√¥i tr∆∞·ªùng h·ªçc t·∫≠p hi·ªán ƒë·∫°i, gi√∫p h·ªçc sinh kh√¥ng ch·ªâ chinh ph·ª•c Ti·∫øng Anh m√† c√≤n c√≥ ni·ªÅm y√™u th√≠ch v·ªõi ng√¥n ng·ªØ n√†y!\n"
},
{
	"uri": "http://<user_name>.github.io/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://<user_name>.github.io/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]